[
    {
        "text": "Deep Residual Learning for Image Recognition",
        "aspect": "Deep Residual Learning",
        "sentiment": 0
    },
    {
        "text": "Deeper neural networks are more difficult to train .",
        "aspect": "Deeper neural networks",
        "sentiment": 0
    },
    {
        "text": "We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously .",
        "aspect": "residual learning framework",
        "sentiment": 1
    },
    {
        "text": "We provide comprehensive empirical evidence showing that these residual networks are easier to optimize , and can gain accuracy from considerably increased depth .",
        "aspect": "residual networks",
        "sentiment": 0
    },
    {
        "text": "On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8\u00d7 deeper than VGG nets [ 41 ] but still having lower complexity .",
        "aspect": "residual nets",
        "sentiment": 1
    },
    {
        "text": "On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8\u00d7 deeper than VGG nets [ 41 ] but still having lower complexity .",
        "aspect": "VGG nets",
        "sentiment": 0
    },
    {
        "text": "An ensemble of these residual nets achieves 3.57 % error on the ImageNet test set .",
        "aspect": "residual nets",
        "sentiment": 0
    },
    {
        "text": "Deep convolutional neural networks have led to a series of breakthroughs for image classification .",
        "aspect": "Deep convolutional neural networks",
        "sentiment": 0
    },
    {
        "text": "Deep networks naturally integrate low / mid / highlevel features and classifiers in an end - to - end multilayer fashion , and the \" levels \" of features can be enriched by the number of stacked layers ( depth ) .",
        "aspect": "Deep networks",
        "sentiment": 0
    },
    {
        "text": "Deep networks naturally integrate low / mid / highlevel features and classifiers in an end - to - end multilayer fashion , and the \" levels \" of features can be enriched by the number of stacked layers ( depth ) .",
        "aspect": "classifiers",
        "sentiment": 0
    },
    {
        "text": "greatly benefited from very deep models .",
        "aspect": "deep models",
        "sentiment": 0
    },
    {
        "text": "This problem , however , has been largely addressed by normalized initialization and intermediate normalization layers , which enable networks with tens of layers to start converging for stochastic gradient descent ( SGD ) with backpropagation .",
        "aspect": "normalized initialization",
        "sentiment": 0
    },
    {
        "text": "This problem , however , has been largely addressed by normalized initialization and intermediate normalization layers , which enable networks with tens of layers to start converging for stochastic gradient descent ( SGD ) with backpropagation .",
        "aspect": "intermediate normalization layers",
        "sentiment": 0
    },
    {
        "text": "This problem , however , has been largely addressed by normalized initialization and intermediate normalization layers , which enable networks with tens of layers to start converging for stochastic gradient descent ( SGD ) with backpropagation .",
        "aspect": "stochastic gradient descent",
        "sentiment": 0
    },
    {
        "text": "This problem , however , has been largely addressed by normalized initialization and intermediate normalization layers , which enable networks with tens of layers to start converging for stochastic gradient descent ( SGD ) with backpropagation .",
        "aspect": "SGD",
        "sentiment": 0
    },
    {
        "text": "This problem , however , has been largely addressed by normalized initialization and intermediate normalization layers , which enable networks with tens of layers to start converging for stochastic gradient descent ( SGD ) with backpropagation .",
        "aspect": "backpropagation",
        "sentiment": 0
    },
    {
        "text": "When deeper networks are able to start converging , a degradation problem has been exposed : with the network depth increasing , accuracy gets saturated ( which might be unsurprising ) and then degrades rapidly .",
        "aspect": "deeper networks",
        "sentiment": 0
    },
    {
        "text": "Unexpectedly , such degradation is not caused by overfitting , and adding more layers to a suitably deep model leads to higher training error , as reported in and thoroughly verified by our experiments .",
        "aspect": "deep model",
        "sentiment": 0
    },
    {
        "text": "Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it .",
        "aspect": "shallower architecture",
        "sentiment": 0
    },
    {
        "text": "There exists a solution by construction to the deeper model : the added layers are identity mapping , and the other layers are copied from the learned shallower model .",
        "aspect": "deeper model",
        "sentiment": 0
    },
    {
        "text": "There exists a solution by construction to the deeper model : the added layers are identity mapping , and the other layers are copied from the learned shallower model .",
        "aspect": "shallower model",
        "sentiment": 0
    },
    {
        "text": "The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart .",
        "aspect": "deeper model",
        "sentiment": 0
    },
    {
        "text": "The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart .",
        "aspect": "shallower counterpart",
        "sentiment": 0
    },
    {
        "text": "In this paper , we address the degradation problem by introducing a deep residual learning framework .",
        "aspect": "deep residual learning framework",
        "sentiment": 1
    },
    {
        "text": "Instead of hoping each few stacked layers directly fit a desired underlying mapping , we explicitly let these layers fit a residual mapping .",
        "aspect": "residual mapping",
        "sentiment": 1
    },
    {
        "text": "Formally , denoting the desired underlying mapping as H(x ) , we let the stacked nonlinear layers fit another mapping of F(x ) : = H(x ) \u2212 x.",
        "aspect": "stacked nonlinear layers",
        "sentiment": 0
    },
    {
        "text": "To the extreme , if an identity mapping were optimal , it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers .",
        "aspect": "identity mapping",
        "sentiment": 0
    },
    {
        "text": "To the extreme , if an identity mapping were optimal , it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers .",
        "aspect": "identity mapping",
        "sentiment": 0
    },
    {
        "text": "To the extreme , if an identity mapping were optimal , it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers .",
        "aspect": "stack of nonlinear layers",
        "sentiment": 0
    },
    {
        "text": "The formulation of F(x ) + x can be realized by feedforward neural networks with \" shortcut connections \" ( Fig . ) .",
        "aspect": "feedforward neural networks",
        "sentiment": 0
    },
    {
        "text": "In our case , the shortcut connections simply perform identity mapping , and their outputs are added to the outputs of the stacked layers ( Fig . ) .",
        "aspect": "identity mapping",
        "sentiment": 1
    },
    {
        "text": "Identity shortcut connections add neither extra parameter nor computational complexity .",
        "aspect": "Identity shortcut connections",
        "sentiment": 0
    },
    {
        "text": "The entire network can still be trained end - to - end by SGD with backpropagation , and can be easily implemented using common libraries ( e.g. , Caffe ) without modifying the solvers .",
        "aspect": "SGD",
        "sentiment": 1
    },
    {
        "text": "The entire network can still be trained end - to - end by SGD with backpropagation , and can be easily implemented using common libraries ( e.g. , Caffe ) without modifying the solvers .",
        "aspect": "backpropagation",
        "sentiment": 1
    },
    {
        "text": "We show that : 1 ) Our extremely deep residual nets are easy to optimize , but the counterpart \" plain \" nets ( that simply stack layers ) exhibit higher training error when the depth increases ; 2 ) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth , producing results substantially better than previous networks .",
        "aspect": "deep residual nets",
        "sentiment": 1
    },
    {
        "text": "We show that : 1 ) Our extremely deep residual nets are easy to optimize , but the counterpart \" plain \" nets ( that simply stack layers ) exhibit higher training error when the depth increases ; 2 ) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth , producing results substantially better than previous networks .",
        "aspect": "deep residual nets",
        "sentiment": 1
    },
    {
        "text": "On the ImageNet classification dataset , we obtain excellent results by extremely deep residual nets .",
        "aspect": "deep residual nets",
        "sentiment": 1
    },
    {
        "text": "Our 152layer residual net is the deepest network ever presented on ImageNet , while still having lower complexity than VGG nets .",
        "aspect": "deepest network",
        "sentiment": 1
    },
    {
        "text": "Our 152layer residual net is the deepest network ever presented on ImageNet , while still having lower complexity than VGG nets .",
        "aspect": "VGG nets",
        "sentiment": 3
    },
    {
        "text": "Our 152layer residual net is the deepest network ever presented on ImageNet , while still having lower complexity than VGG nets .",
        "aspect": "152layer residual net",
        "sentiment": 1
    },
    {
        "text": "The extremely deep representations also have excellent generalization performance on other recognition tasks , and lead us to further win the 1st places on : ImageNet detection , ImageNet localization , COCO detection , and COCO segmentation in ILSVRC & COCO 2015 competitions .",
        "aspect": "deep representations",
        "sentiment": 0
    },
    {
        "text": "This strong evidence shows that the residual learning principle is generic , and we expect that it is applicable in other vision and non - vision problems .",
        "aspect": "residual learning principle",
        "sentiment": 0
    },
    {
        "text": "Residual Representations .",
        "aspect": "Residual Representations",
        "sentiment": 0
    },
    {
        "text": "In image recognition , VLAD is a representation that encodes by the residual vectors with respect to a dictionary , and Fisher Vector can be formulated as a probabilistic version of VLAD .",
        "aspect": "VLAD",
        "sentiment": 0
    },
    {
        "text": "In image recognition , VLAD is a representation that encodes by the residual vectors with respect to a dictionary , and Fisher Vector can be formulated as a probabilistic version of VLAD .",
        "aspect": "Fisher Vector",
        "sentiment": 0
    },
    {
        "text": "In image recognition , VLAD is a representation that encodes by the residual vectors with respect to a dictionary , and Fisher Vector can be formulated as a probabilistic version of VLAD .",
        "aspect": "VLAD",
        "sentiment": 0
    },
    {
        "text": "Both of them are powerful shallow representations for image retrieval and classification .",
        "aspect": "shallow representations",
        "sentiment": 0
    },
    {
        "text": "In low - level vision and computer graphics , for solving Partial Differential Equations ( PDEs ) , the widely used Multigrid method reformulates the system as subproblems at multiple scales , where each subproblem is responsible for the residual solution between a coarser and a finer scale .",
        "aspect": "Multigrid method",
        "sentiment": 0
    },
    {
        "text": "An alternative to Multigrid is hierarchical basis preconditioning , which relies on variables that represent residual vectors between two scales .",
        "aspect": "Multigrid",
        "sentiment": 0
    },
    {
        "text": "An alternative to Multigrid is hierarchical basis preconditioning , which relies on variables that represent residual vectors between two scales .",
        "aspect": "hierarchical basis preconditioning",
        "sentiment": 0
    },
    {
        "text": "An early practice of training multi - layer perceptrons ( MLPs ) is to add a linear layer connected from the network input to the output .",
        "aspect": "multi - layer perceptrons",
        "sentiment": 0
    },
    {
        "text": "An early practice of training multi - layer perceptrons ( MLPs ) is to add a linear layer connected from the network input to the output .",
        "aspect": "MLPs",
        "sentiment": 0
    },
    {
        "text": "An early practice of training multi - layer perceptrons ( MLPs ) is to add a linear layer connected from the network input to the output .",
        "aspect": "linear layer",
        "sentiment": 0
    },
    {
        "text": "In , a few intermediate layers are directly connected to auxiliary classifiers for addressing vanishing / exploding gradients .",
        "aspect": "auxiliary classifiers",
        "sentiment": 0
    },
    {
        "text": "The papers of propose methods for centering layer responses , gradients , and propagated errors , implemented by shortcut connections .",
        "aspect": "shortcut connections",
        "sentiment": 0
    },
    {
        "text": "Concurrent with our work , \" highway networks \" present shortcut connections with gating functions .",
        "aspect": "gating functions",
        "sentiment": 0
    },
    {
        "text": "These gates are data - dependent and have parameters , in contrast to our identity shortcuts that are parameter - free .",
        "aspect": "identity shortcuts",
        "sentiment": 0
    },
    {
        "text": "When a gated shortcut is \" closed \" ( approaching zero ) , the layers in highway networks represent non - residual functions .",
        "aspect": "highway networks",
        "sentiment": 0
    },
    {
        "text": "In addition , high - way networks have not demonstrated accuracy gains with extremely increased depth ( e.g. , over 100 layers ) .",
        "aspect": "high - way networks",
        "sentiment": 0
    },
    {
        "text": "SEMANTIC IMAGE SEGMENTATION WITH DEEP CON - VOLUTIONAL NETS AND FULLY CONNECTED CRFS",
        "aspect": "DEEP CON - VOLUTIONAL NETS",
        "sentiment": 0
    },
    {
        "text": "SEMANTIC IMAGE SEGMENTATION WITH DEEP CON - VOLUTIONAL NETS AND FULLY CONNECTED CRFS",
        "aspect": "FULLY CONNECTED CRFS",
        "sentiment": 0
    },
    {
        "text": "Deep Convolutional Neural Networks ( DCNNs ) have recently shown state of the art performance in high level vision tasks , such as image classification and object detection .",
        "aspect": "Deep Convolutional Neural Networks",
        "sentiment": 0
    },
    {
        "text": "Deep Convolutional Neural Networks ( DCNNs ) have recently shown state of the art performance in high level vision tasks , such as image classification and object detection .",
        "aspect": "DCNNs",
        "sentiment": 0
    },
    {
        "text": "This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel - level classification ( also called \" semantic image segmentation \" ) .",
        "aspect": "DCNNs",
        "sentiment": 1
    },
    {
        "text": "This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel - level classification ( also called \" semantic image segmentation \" ) .",
        "aspect": "probabilistic graphical models",
        "sentiment": 1
    },
    {
        "text": "We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation .",
        "aspect": "DCNNs",
        "sentiment": 0
    },
    {
        "text": "This is due to the very invariance properties that make DCNNs good for high level tasks .",
        "aspect": "DCNNs",
        "sentiment": 0
    },
    {
        "text": "We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field ( CRF ) .",
        "aspect": "deep networks",
        "sentiment": 0
    },
    {
        "text": "We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field ( CRF ) .",
        "aspect": "DCNN layer",
        "sentiment": 1
    },
    {
        "text": "We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field ( CRF ) .",
        "aspect": "fully connected Conditional Random Field",
        "sentiment": 1
    },
    {
        "text": "We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field ( CRF ) .",
        "aspect": "CRF",
        "sentiment": 1
    },
    {
        "text": "Qualitatively , our \" DeepLab \" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods .",
        "aspect": "DeepLab",
        "sentiment": 2
    },
    {
        "text": "We show how these results can be obtained efficiently : Careful network re - purposing and a novel application of the ' hole ' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU .",
        "aspect": "hole ' algorithm",
        "sentiment": 1
    },
    {
        "text": "Deep Convolutional Neural Networks ( DCNNs ) had been the method of choice for document recognition since , but have only recently become the mainstream of high - level vision research .",
        "aspect": "Deep Convolutional Neural Networks",
        "sentiment": 0
    },
    {
        "text": "Deep Convolutional Neural Networks ( DCNNs ) had been the method of choice for document recognition since , but have only recently become the mainstream of high - level vision research .",
        "aspect": "DCNNs",
        "sentiment": 0
    },
    {
        "text": "Over the past two years DCNNs have pushed the performance of computer vision systems to soaring heights on a broad array of high - level problems , including image classification , object detection , fine - grained categorization , among others .",
        "aspect": "DCNNs",
        "sentiment": 0
    },
    {
        "text": "A common theme in these works is that DCNNs trained in an end - to - end manner deliver strikingly better results than systems relying on carefully engineered representations , such as SIFT or HOG features .",
        "aspect": "DCNNs",
        "sentiment": 0
    },
    {
        "text": "This success can be partially attributed to the built - in invariance of DCNNs to local image transformations , which underpins their ability to learn hierarchical abstractions of data .",
        "aspect": "DCNNs",
        "sentiment": 0
    },
    {
        "text": "There are two technical hurdles in the application of DCNNs to image labeling tasks : signal downsampling , and spatial ' insensitivity ' ( invariance ) .",
        "aspect": "DCNNs",
        "sentiment": 0
    },
    {
        "text": "The first problem relates to the reduction of signal resolution incurred by the repeated combination of max - pooling and downsampling ( ' striding ' ) performed at every layer of standard DCNNs .",
        "aspect": "max - pooling",
        "sentiment": 0
    },
    {
        "text": "The first problem relates to the reduction of signal resolution incurred by the repeated combination of max - pooling and downsampling ( ' striding ' ) performed at every layer of standard DCNNs .",
        "aspect": "downsampling",
        "sentiment": 0
    },
    {
        "text": "The first problem relates to the reduction of signal resolution incurred by the repeated combination of max - pooling and downsampling ( ' striding ' ) performed at every layer of standard DCNNs .",
        "aspect": "DCNNs",
        "sentiment": 0
    },
    {
        "text": "Instead , as in , we employ the ' atrous ' ( with holes ) algorithm originally developed for efficiently computing the undecimated discrete wavelet transform .",
        "aspect": "atrous ' ( with holes ) algorithm",
        "sentiment": 1
    },
    {
        "text": "Instead , as in , we employ the ' atrous ' ( with holes ) algorithm originally developed for efficiently computing the undecimated discrete wavelet transform .",
        "aspect": "undecimated discrete wavelet transform",
        "sentiment": 1
    },
    {
        "text": "The second problem relates to the fact that obtaining object - centric decisions from a classifier requires invariance to spatial transformations , inherently limiting the spatial accuracy of the DCNN model .",
        "aspect": "classifier",
        "sentiment": 0
    },
    {
        "text": "The second problem relates to the fact that obtaining object - centric decisions from a classifier requires invariance to spatial transformations , inherently limiting the spatial accuracy of the DCNN model .",
        "aspect": "DCNN model",
        "sentiment": 0
    },
    {
        "text": "We boost our model 's ability to capture fine details by employing a fully - connected Conditional Random Field ( CRF ) .",
        "aspect": "fully - connected Conditional Random Field",
        "sentiment": 1
    },
    {
        "text": "We boost our model 's ability to capture fine details by employing a fully - connected Conditional Random Field ( CRF ) .",
        "aspect": "CRF",
        "sentiment": 1
    },
    {
        "text": "Conditional Random Fields have been broadly used in semantic segmentation to combine class scores computed by multi - way classifiers with the low - level information captured by the local interactions of pixels and edges or superpixels .",
        "aspect": "Conditional Random Fields",
        "sentiment": 0
    },
    {
        "text": "Conditional Random Fields have been broadly used in semantic segmentation to combine class scores computed by multi - way classifiers with the low - level information captured by the local interactions of pixels and edges or superpixels .",
        "aspect": "multi - way classifiers",
        "sentiment": 0
    },
    {
        "text": "Even though works of increased sophistication have been proposed to model the hierarchical dependency and/or high - order dependencies of segments , we use the fully connected pairwise CRF proposed by for its efficient computation , and ability to capture fine edge details while also catering for long range dependencies .",
        "aspect": "fully connected pairwise CRF",
        "sentiment": 1
    },
    {
        "text": "That model was shown in to largely improve the performance of a boosting - based pixel - level classifier , and in our work we demonstrate that it leads to state - of - the - art results when coupled with a DCNN - based pixel - level classifier .",
        "aspect": "boosting - based pixel - level classifier",
        "sentiment": 0
    },
    {
        "text": "That model was shown in to largely improve the performance of a boosting - based pixel - level classifier , and in our work we demonstrate that it leads to state - of - the - art results when coupled with a DCNN - based pixel - level classifier .",
        "aspect": "DCNN - based pixel - level classifier",
        "sentiment": 0
    },
    {
        "text": "The three main advantages of our \" DeepLab \" system are ( i ) speed : by virtue of the ' atrous ' algorithm , our dense DCNN operates at 8 fps , while Mean Field Inference for the fully - connected CRF requires 0.5 second , ( ii ) accuracy : we obtain state - of - the - art results on the PASCAL semantic segmentation challenge , outperforming the second - best approach of by a margin of 7.2 % and ( iii ) simplicity : our system is composed of a cascade of two fairly well - established modules , DCNNs and CRFs .",
        "aspect": "atrous ' algorithm",
        "sentiment": 1
    },
    {
        "text": "The three main advantages of our \" DeepLab \" system are ( i ) speed : by virtue of the ' atrous ' algorithm , our dense DCNN operates at 8 fps , while Mean Field Inference for the fully - connected CRF requires 0.5 second , ( ii ) accuracy : we obtain state - of - the - art results on the PASCAL semantic segmentation challenge , outperforming the second - best approach of by a margin of 7.2 % and ( iii ) simplicity : our system is composed of a cascade of two fairly well - established modules , DCNNs and CRFs .",
        "aspect": "dense DCNN",
        "sentiment": 0
    },
    {
        "text": "The three main advantages of our \" DeepLab \" system are ( i ) speed : by virtue of the ' atrous ' algorithm , our dense DCNN operates at 8 fps , while Mean Field Inference for the fully - connected CRF requires 0.5 second , ( ii ) accuracy : we obtain state - of - the - art results on the PASCAL semantic segmentation challenge , outperforming the second - best approach of by a margin of 7.2 % and ( iii ) simplicity : our system is composed of a cascade of two fairly well - established modules , DCNNs and CRFs .",
        "aspect": "Mean Field Inference",
        "sentiment": 3
    },
    {
        "text": "The three main advantages of our \" DeepLab \" system are ( i ) speed : by virtue of the ' atrous ' algorithm , our dense DCNN operates at 8 fps , while Mean Field Inference for the fully - connected CRF requires 0.5 second , ( ii ) accuracy : we obtain state - of - the - art results on the PASCAL semantic segmentation challenge , outperforming the second - best approach of by a margin of 7.2 % and ( iii ) simplicity : our system is composed of a cascade of two fairly well - established modules , DCNNs and CRFs .",
        "aspect": "cascade",
        "sentiment": 1
    },
    {
        "text": "The three main advantages of our \" DeepLab \" system are ( i ) speed : by virtue of the ' atrous ' algorithm , our dense DCNN operates at 8 fps , while Mean Field Inference for the fully - connected CRF requires 0.5 second , ( ii ) accuracy : we obtain state - of - the - art results on the PASCAL semantic segmentation challenge , outperforming the second - best approach of by a margin of 7.2 % and ( iii ) simplicity : our system is composed of a cascade of two fairly well - established modules , DCNNs and CRFs .",
        "aspect": "DCNNs",
        "sentiment": 1
    },
    {
        "text": "The three main advantages of our \" DeepLab \" system are ( i ) speed : by virtue of the ' atrous ' algorithm , our dense DCNN operates at 8 fps , while Mean Field Inference for the fully - connected CRF requires 0.5 second , ( ii ) accuracy : we obtain state - of - the - art results on the PASCAL semantic segmentation challenge , outperforming the second - best approach of by a margin of 7.2 % and ( iii ) simplicity : our system is composed of a cascade of two fairly well - established modules , DCNNs and CRFs .",
        "aspect": "CRFs",
        "sentiment": 1
    },
    {
        "text": "The three main advantages of our \" DeepLab \" system are ( i ) speed : by virtue of the ' atrous ' algorithm , our dense DCNN operates at 8 fps , while Mean Field Inference for the fully - connected CRF requires 0.5 second , ( ii ) accuracy : we obtain state - of - the - art results on the PASCAL semantic segmentation challenge , outperforming the second - best approach of by a margin of 7.2 % and ( iii ) simplicity : our system is composed of a cascade of two fairly well - established modules , DCNNs and CRFs .",
        "aspect": "DeepLab",
        "sentiment": 2
    },
    {
        "text": "This is in contrast to the two - stage approaches that are now most common in semantic segmentation with DCNNs : such techniques typically use a cascade of bottom - up image segmentation and DCNN - based region classification , which makes the system commit to potential errors of the front - end segmentation system .",
        "aspect": "stage approaches",
        "sentiment": 0
    },
    {
        "text": "This is in contrast to the two - stage approaches that are now most common in semantic segmentation with DCNNs : such techniques typically use a cascade of bottom - up image segmentation and DCNN - based region classification , which makes the system commit to potential errors of the front - end segmentation system .",
        "aspect": "DCNNs",
        "sentiment": 0
    },
    {
        "text": "This is in contrast to the two - stage approaches that are now most common in semantic segmentation with DCNNs : such techniques typically use a cascade of bottom - up image segmentation and DCNN - based region classification , which makes the system commit to potential errors of the front - end segmentation system .",
        "aspect": "cascade of bottom - up image segmentation",
        "sentiment": 0
    },
    {
        "text": "This is in contrast to the two - stage approaches that are now most common in semantic segmentation with DCNNs : such techniques typically use a cascade of bottom - up image segmentation and DCNN - based region classification , which makes the system commit to potential errors of the front - end segmentation system .",
        "aspect": "DCNN - based region classification",
        "sentiment": 0
    },
    {
        "text": "This is in contrast to the two - stage approaches that are now most common in semantic segmentation with DCNNs : such techniques typically use a cascade of bottom - up image segmentation and DCNN - based region classification , which makes the system commit to potential errors of the front - end segmentation system .",
        "aspect": "front - end segmentation system",
        "sentiment": 0
    },
    {
        "text": "For instance , the bounding box proposals and masked regions delivered by are used in and as inputs to a DCNN to introduce shape information into the classification process .",
        "aspect": "DCNN",
        "sentiment": 0
    },
    {
        "text": "Similarly , the authors of rely on a superpixel representation .",
        "aspect": "superpixel representation",
        "sentiment": 0
    },
    {
        "text": "A celebrated non - DCNN precursor to these works is the second order pooling method of which also assigns labels to the regions proposals delivered by .",
        "aspect": "non - DCNN precursor",
        "sentiment": 0
    },
    {
        "text": "A celebrated non - DCNN precursor to these works is the second order pooling method of which also assigns labels to the regions proposals delivered by .",
        "aspect": "second order pooling method",
        "sentiment": 0
    },
    {
        "text": "Understanding the perils of committing to a single segmentation , the authors of build on to explore a diverse set of CRF - based segmentation proposals , computed also by .",
        "aspect": "CRF - based segmentation proposals",
        "sentiment": 0
    },
    {
        "text": "These segmentation proposals are then re - ranked according to a DCNN trained in particular for this reranking task .",
        "aspect": "DCNN",
        "sentiment": 0
    },
    {
        "text": "Even though this approach explicitly tries to handle the temperamental nature of a front - end segmentation algorithm , there is still no explicit ex - ploitation of the DCNN scores in the CRF - based segmentation algorithm : the DCNN is only applied post - hoc , while it would make sense to directly try to use its results during segmentation .",
        "aspect": "front - end segmentation algorithm",
        "sentiment": 0
    },
    {
        "text": "Even though this approach explicitly tries to handle the temperamental nature of a front - end segmentation algorithm , there is still no explicit ex - ploitation of the DCNN scores in the CRF - based segmentation algorithm : the DCNN is only applied post - hoc , while it would make sense to directly try to use its results during segmentation .",
        "aspect": "CRF",
        "sentiment": 0
    },
    {
        "text": "Even though this approach explicitly tries to handle the temperamental nature of a front - end segmentation algorithm , there is still no explicit ex - ploitation of the DCNN scores in the CRF - based segmentation algorithm : the DCNN is only applied post - hoc , while it would make sense to directly try to use its results during segmentation .",
        "aspect": "DCNN",
        "sentiment": 1
    },
    {
        "text": "Moving towards works that lie closer to our approach , several other researchers have considered the use of convolutionally computed DCNN features for dense image labeling .",
        "aspect": "convolutionally computed DCNN features",
        "sentiment": 0
    },
    {
        "text": "Among the first have been who apply DCNNs at multiple image resolutions and then employ a segmentation tree to smooth the prediction results ; more recently , propose to concatenate the computed inter - mediate feature maps within the DCNNs for pixel classification , and propose to pool the inter - mediate feature maps by region proposals .",
        "aspect": "DCNNs",
        "sentiment": 0
    },
    {
        "text": "Among the first have been who apply DCNNs at multiple image resolutions and then employ a segmentation tree to smooth the prediction results ; more recently , propose to concatenate the computed inter - mediate feature maps within the DCNNs for pixel classification , and propose to pool the inter - mediate feature maps by region proposals .",
        "aspect": "segmentation tree",
        "sentiment": 0
    },
    {
        "text": "Among the first have been who apply DCNNs at multiple image resolutions and then employ a segmentation tree to smooth the prediction results ; more recently , propose to concatenate the computed inter - mediate feature maps within the DCNNs for pixel classification , and propose to pool the inter - mediate feature maps by region proposals .",
        "aspect": "DCNNs",
        "sentiment": 0
    },
    {
        "text": "Among the first have been who apply DCNNs at multiple image resolutions and then employ a segmentation tree to smooth the prediction results ; more recently , propose to concatenate the computed inter - mediate feature maps within the DCNNs for pixel classification , and propose to pool the inter - mediate feature maps by region proposals .",
        "aspect": "region proposals",
        "sentiment": 0
    },
    {
        "text": "Even though these works still employ segmentation algorithms that are decoupled from the DCNN classifier 's results , we believe it is advantageous that segmentation is only used at a later stage , avoiding the commitment to premature decisions .",
        "aspect": "segmentation algorithms",
        "sentiment": 0
    },
    {
        "text": "Even though these works still employ segmentation algorithms that are decoupled from the DCNN classifier 's results , we believe it is advantageous that segmentation is only used at a later stage , avoiding the commitment to premature decisions .",
        "aspect": "DCNN classifier",
        "sentiment": 0
    },
    {
        "text": "More recently , the segmentation - free techniques of directly apply DCNNs to the whole image in a sliding window fashion , replacing the last fully connected layers of a DCNN by convolutional layers .",
        "aspect": "segmentation - free techniques",
        "sentiment": 0
    },
    {
        "text": "More recently , the segmentation - free techniques of directly apply DCNNs to the whole image in a sliding window fashion , replacing the last fully connected layers of a DCNN by convolutional layers .",
        "aspect": "DCNNs",
        "sentiment": 0
    },
    {
        "text": "More recently , the segmentation - free techniques of directly apply DCNNs to the whole image in a sliding window fashion , replacing the last fully connected layers of a DCNN by convolutional layers .",
        "aspect": "sliding window fashion",
        "sentiment": 0
    },
    {
        "text": "More recently , the segmentation - free techniques of directly apply DCNNs to the whole image in a sliding window fashion , replacing the last fully connected layers of a DCNN by convolutional layers .",
        "aspect": "fully connected layers",
        "sentiment": 0
    },
    {
        "text": "More recently , the segmentation - free techniques of directly apply DCNNs to the whole image in a sliding window fashion , replacing the last fully connected layers of a DCNN by convolutional layers .",
        "aspect": "DCNN",
        "sentiment": 0
    },
    {
        "text": "More recently , the segmentation - free techniques of directly apply DCNNs to the whole image in a sliding window fashion , replacing the last fully connected layers of a DCNN by convolutional layers .",
        "aspect": "convolutional layers",
        "sentiment": 0
    },
    {
        "text": "In order to deal with the spatial localization issues outlined in the beginning of the introduction , upsample and concatenate the scores from inter - mediate feature maps , while refine the prediction result from coarse to fine by propagating the coarse results to another DCNN .",
        "aspect": "DCNN",
        "sentiment": 1
    },
    {
        "text": "The main difference between our model and other state - of - the - art models is the combination of pixel - level CRFs and DCNN - based ' unary terms ' .",
        "aspect": "pixel - level CRFs",
        "sentiment": 1
    },
    {
        "text": "Focusing on the closest works in this direction , use CRFs as a proposal mechanism for a DCNN - based reranking system , while treat superpixels as nodes for a local pairwise CRF and use graph - cuts for discrete inference ; as such their results can be limited by errors in superpixel computations , while ignoring long - range superpixel dependencies .",
        "aspect": "CRFs",
        "sentiment": 0
    },
    {
        "text": "Focusing on the closest works in this direction , use CRFs as a proposal mechanism for a DCNN - based reranking system , while treat superpixels as nodes for a local pairwise CRF and use graph - cuts for discrete inference ; as such their results can be limited by errors in superpixel computations , while ignoring long - range superpixel dependencies .",
        "aspect": "proposal mechanism",
        "sentiment": 0
    },
    {
        "text": "Focusing on the closest works in this direction , use CRFs as a proposal mechanism for a DCNN - based reranking system , while treat superpixels as nodes for a local pairwise CRF and use graph - cuts for discrete inference ; as such their results can be limited by errors in superpixel computations , while ignoring long - range superpixel dependencies .",
        "aspect": "DCNN - based reranking system",
        "sentiment": 0
    },
    {
        "text": "Focusing on the closest works in this direction , use CRFs as a proposal mechanism for a DCNN - based reranking system , while treat superpixels as nodes for a local pairwise CRF and use graph - cuts for discrete inference ; as such their results can be limited by errors in superpixel computations , while ignoring long - range superpixel dependencies .",
        "aspect": "local pairwise CRF",
        "sentiment": 0
    },
    {
        "text": "Focusing on the closest works in this direction , use CRFs as a proposal mechanism for a DCNN - based reranking system , while treat superpixels as nodes for a local pairwise CRF and use graph - cuts for discrete inference ; as such their results can be limited by errors in superpixel computations , while ignoring long - range superpixel dependencies .",
        "aspect": "graph - cuts",
        "sentiment": 0
    },
    {
        "text": "Our approach instead treats every pixel as a CRF node , exploits long - range dependencies , and uses CRF inference to directly optimize a DCNN - driven cost function .",
        "aspect": "CRF inference",
        "sentiment": 1
    },
    {
        "text": "Our approach instead treats every pixel as a CRF node , exploits long - range dependencies , and uses CRF inference to directly optimize a DCNN - driven cost function .",
        "aspect": "DCNN - driven cost function",
        "sentiment": 1
    },
    {
        "text": "We note that mean field had been extensively studied for traditional image segmentation / edge detection tasks , e.g. , , but recently showed that the inference can be very efficient for fully connected CRF and particularly effective in the context of semantic segmentation .",
        "aspect": "mean field",
        "sentiment": 0
    },
    {
        "text": "We note that mean field had been extensively studied for traditional image segmentation / edge detection tasks , e.g. , , but recently showed that the inference can be very efficient for fully connected CRF and particularly effective in the context of semantic segmentation .",
        "aspect": "fully connected CRF",
        "sentiment": 0
    },
    {
        "text": "After the first version of our manuscript was made publicly available , it came to our attention that two other groups have independently and concurrently pursued a very similar direction , combining DCNNs and densely connected CRFs .",
        "aspect": "DCNNs",
        "sentiment": 1
    },
    {
        "text": "After the first version of our manuscript was made publicly available , it came to our attention that two other groups have independently and concurrently pursued a very similar direction , combining DCNNs and densely connected CRFs .",
        "aspect": "densely connected CRFs",
        "sentiment": 1
    },
    {
        "text": "We have updated our proposed \" DeepLab \" system with much improved methods and results in our latest work .",
        "aspect": "DeepLab \" system",
        "sentiment": 2
    },
    {
        "text": "Our work combines ideas from deep convolutional neural networks and fully - connected conditional random fields , yielding a novel method able to produce semantically accurate predictions and detailed segmentation maps , while being computationally efficient .",
        "aspect": "deep convolutional neural networks",
        "sentiment": 1
    },
    {
        "text": "Our work combines ideas from deep convolutional neural networks and fully - connected conditional random fields , yielding a novel method able to produce semantically accurate predictions and detailed segmentation maps , while being computationally efficient .",
        "aspect": "fully - connected conditional random fields",
        "sentiment": 1
    },
    {
        "text": "There are multiple aspects in our model that we intend to refine , such as fully integrating its two main components ( CNN and CRF ) and train the whole system in an end - to - end fashion , similar to ; ; .",
        "aspect": "CNN and CRF",
        "sentiment": 1
    },
    {
        "text": "Recently , we have pursued model training with weakly supervised annotations , in the form of bounding boxes or image - level labels .",
        "aspect": "model training",
        "sentiment": 0
    },
    {
        "text": "At a higher level , our work lies in the intersection of convolutional neural networks and probabilistic graphical models .",
        "aspect": "convolutional neural networks",
        "sentiment": 1
    },
    {
        "text": "At a higher level , our work lies in the intersection of convolutional neural networks and probabilistic graphical models .",
        "aspect": "probabilistic graphical models",
        "sentiment": 1
    },
    {
        "text": "Highly expressive directed latent variable models , such as sigmoid belief networks , are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well .",
        "aspect": "directed latent variable models",
        "sentiment": 0
    },
    {
        "text": "Highly expressive directed latent variable models , such as sigmoid belief networks , are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well .",
        "aspect": "sigmoid belief networks",
        "sentiment": 0
    },
    {
        "text": "Highly expressive directed latent variable models , such as sigmoid belief networks , are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well .",
        "aspect": "approximate inference methods",
        "sentiment": 0
    },
    {
        "text": "We propose a fast non - iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior .",
        "aspect": "non - iterative approximate inference method",
        "sentiment": 1
    },
    {
        "text": "We propose a fast non - iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior .",
        "aspect": "feedforward network",
        "sentiment": 1
    },
    {
        "text": "The model and this inference network are trained jointly by maximizing a variational lower bound on the log - likelihood .",
        "aspect": "inference network",
        "sentiment": 0
    },
    {
        "text": "The model and this inference network are trained jointly by maximizing a variational lower bound on the log - likelihood .",
        "aspect": "variational lower bound",
        "sentiment": 0
    },
    {
        "text": "Although the naive estimator of the inference network gradient is too high - variance to be useful , we make it practical by applying several straightforward modelindependent variance reduction techniques .",
        "aspect": "naive estimator of the inference network gradient",
        "sentiment": 0
    },
    {
        "text": "Although the naive estimator of the inference network gradient is too high - variance to be useful , we make it practical by applying several straightforward modelindependent variance reduction techniques .",
        "aspect": "modelindependent variance reduction techniques",
        "sentiment": 0
    },
    {
        "text": "Applying our approach to training sigmoid belief networks and deep autoregressive networks , we show that it outperforms the wake - sleep algorithm on MNIST and achieves state - of - the - art results on the Reuters RCV1 document dataset .",
        "aspect": "sigmoid belief networks",
        "sentiment": 1
    },
    {
        "text": "Applying our approach to training sigmoid belief networks and deep autoregressive networks , we show that it outperforms the wake - sleep algorithm on MNIST and achieves state - of - the - art results on the Reuters RCV1 document dataset .",
        "aspect": "deep autoregressive networks",
        "sentiment": 1
    },
    {
        "text": "Applying our approach to training sigmoid belief networks and deep autoregressive networks , we show that it outperforms the wake - sleep algorithm on MNIST and achieves state - of - the - art results on the Reuters RCV1 document dataset .",
        "aspect": "wake - sleep algorithm",
        "sentiment": 3
    },
    {
        "text": "Compared to powerful globally - normalized latent variable models , such as deep belief networks and deep Boltzmann machines , which can now be trained on fairly large datasets , their purely directed counterparts have been left behind due to the lack of efficient learning algorithms .",
        "aspect": "globally - normalized latent variable models",
        "sentiment": 0
    },
    {
        "text": "Compared to powerful globally - normalized latent variable models , such as deep belief networks and deep Boltzmann machines , which can now be trained on fairly large datasets , their purely directed counterparts have been left behind due to the lack of efficient learning algorithms .",
        "aspect": "deep belief networks",
        "sentiment": 0
    },
    {
        "text": "Compared to powerful globally - normalized latent variable models , such as deep belief networks and deep Boltzmann machines , which can now be trained on fairly large datasets , their purely directed counterparts have been left behind due to the lack of efficient learning algorithms .",
        "aspect": "deep Boltzmann machines",
        "sentiment": 0
    },
    {
        "text": "Compared to powerful globally - normalized latent variable models , such as deep belief networks and deep Boltzmann machines , which can now be trained on fairly large datasets , their purely directed counterparts have been left behind due to the lack of efficient learning algorithms .",
        "aspect": "learning algorithms",
        "sentiment": 0
    },
    {
        "text": "Although the generality of Markov Chain Monte Carlo ( MCMC ) methods makes them straightforward to apply to models of this type , they tend to suffer from slow mixing and are usually too computationally expensive to be practical in all but the simplest models .",
        "aspect": "Markov Chain Monte Carlo ( MCMC ) methods",
        "sentiment": 0
    },
    {
        "text": "Such methods are also difficult to scale to large datasets because they need to store the current state of the latent variables for all the training observations between parameter updates .",
        "aspect": "Such methods",
        "sentiment": 0
    },
    {
        "text": "Variational methods provide an optimization - based alternative to the sampling - based Monte Carlo methods , and tend to be more efficient .",
        "aspect": "Variational methods",
        "sentiment": 0
    },
    {
        "text": "Variational methods provide an optimization - based alternative to the sampling - based Monte Carlo methods , and tend to be more efficient .",
        "aspect": "optimization - based alternative",
        "sentiment": 0
    },
    {
        "text": "Variational methods provide an optimization - based alternative to the sampling - based Monte Carlo methods , and tend to be more efficient .",
        "aspect": "sampling - based Monte Carlo methods",
        "sentiment": 0
    },
    {
        "text": "They involve approximating the exact posterior using a distribution from a more tractable family , often a fully factored one , by maximizing a variational lower bound on the loglikelihood w.r.t . the parameters of the distribution .",
        "aspect": "fully factored one",
        "sentiment": 0
    },
    {
        "text": "For a small class of models , using such variational posteriors allows the expectations that specify the parameter updates to be computed analytically .",
        "aspect": "variational posteriors",
        "sentiment": 0
    },
    {
        "text": "However , for highly expressive models such as the ones we are interested in , these expectations are intractable even with the simplest variational posteriors .",
        "aspect": "variational posteriors",
        "sentiment": 0
    },
    {
        "text": "This difficulty is usually dealt with by lower bounding the intractable expectations with tractable one by introducing more variational parameters , as was done for sigmoid belief nets by .",
        "aspect": "sigmoid belief nets",
        "sentiment": 0
    },
    {
        "text": "In general , variational methods tend to be more model - dependent than sampling - based methods , often requiring non - trivial model - specific derivations .",
        "aspect": "variational methods",
        "sentiment": 0
    },
    {
        "text": "In general , variational methods tend to be more model - dependent than sampling - based methods , often requiring non - trivial model - specific derivations .",
        "aspect": "sampling - based methods",
        "sentiment": 0
    },
    {
        "text": "We propose a new approach to training directed graphical models that combines the advantages of the samplingbased and variational methods .",
        "aspect": "directed graphical models",
        "sentiment": 1
    },
    {
        "text": "We propose a new approach to training directed graphical models that combines the advantages of the samplingbased and variational methods .",
        "aspect": "samplingbased and variational methods",
        "sentiment": 1
    },
    {
        "text": "Its central idea is using a feedforward network to implement efficient exact sampling from the variational posterior for the given observation .",
        "aspect": "feedforward network",
        "sentiment": 1
    },
    {
        "text": "We train this inference network jointly with the model by maximizing the variational lower bound on the log - likelihood , estimating all the required gradients using samples from the inference network .",
        "aspect": "inference network",
        "sentiment": 1
    },
    {
        "text": "We train this inference network jointly with the model by maximizing the variational lower bound on the log - likelihood , estimating all the required gradients using samples from the inference network .",
        "aspect": "inference network",
        "sentiment": 1
    },
    {
        "text": "Although naive estimate of the gradient for the inference network parameters is unusable due to its high variance , we make the approach practical by applying several straightforward and general variance reduc - tion techniques .",
        "aspect": "variance reduc - tion techniques",
        "sentiment": 1
    },
    {
        "text": "The resulting training procedure for the inference network can be seen as an instance of the RE - INFORCE algorithm .",
        "aspect": "inference network",
        "sentiment": 0
    },
    {
        "text": "The resulting training procedure for the inference network can be seen as an instance of the RE - INFORCE algorithm .",
        "aspect": "RE - INFORCE algorithm",
        "sentiment": 0
    },
    {
        "text": "Due to our use of stochastic feedforward networks for performing inference we call our approach Neural Variational Inference and Learning ( NVIL ) .",
        "aspect": "stochastic feedforward networks",
        "sentiment": 1
    },
    {
        "text": "Due to our use of stochastic feedforward networks for performing inference we call our approach Neural Variational Inference and Learning ( NVIL ) .",
        "aspect": "Neural Variational Inference and Learning",
        "sentiment": 2
    },
    {
        "text": "Due to our use of stochastic feedforward networks for performing inference we call our approach Neural Variational Inference and Learning ( NVIL ) .",
        "aspect": "NVIL",
        "sentiment": 2
    },
    {
        "text": "Compared to MCMC methods , where many iterations over the latent variables are required to generate a sample from the exact posterior and successive samples tend to be highly correlated , NVIL does not suffer from mixing issues as each forward pass through the inference network generates an independent exact sample from the variational posterior .",
        "aspect": "MCMC methods",
        "sentiment": 3
    },
    {
        "text": "Compared to MCMC methods , where many iterations over the latent variables are required to generate a sample from the exact posterior and successive samples tend to be highly correlated , NVIL does not suffer from mixing issues as each forward pass through the inference network generates an independent exact sample from the variational posterior .",
        "aspect": "NVIL",
        "sentiment": 2
    },
    {
        "text": "Compared to MCMC methods , where many iterations over the latent variables are required to generate a sample from the exact posterior and successive samples tend to be highly correlated , NVIL does not suffer from mixing issues as each forward pass through the inference network generates an independent exact sample from the variational posterior .",
        "aspect": "inference network",
        "sentiment": 0
    },
    {
        "text": "In addition to being much faster than MCMC , our approach has the additional advantage of not needing to store the latent variables for each observation and thus is not only more memory efficient but also applicable to the pure online learning setting , where each training case is seen once before being discarded .",
        "aspect": "MCMC",
        "sentiment": 0
    },
    {
        "text": "In addition to being much faster than MCMC , our approach has the additional advantage of not needing to store the latent variables for each observation and thus is not only more memory efficient but also applicable to the pure online learning setting , where each training case is seen once before being discarded .",
        "aspect": "our approach",
        "sentiment": 0
    },
    {
        "text": "In contrast to other work on scaling up variational inference , NVIL can handle both discrete and continuous latent variables ( unlike ; ) as well variational posteriors with complex dependency structures ( unlike ) .",
        "aspect": "NVIL",
        "sentiment": 0
    },
    {
        "text": "Moreover , the variance reduction methods we employ are simple and model - independent , unlike the more sophisticated model - specific control variates of .",
        "aspect": "variance reduction methods",
        "sentiment": 0
    },
    {
        "text": "Though the idea of training an inference model by following the gradient of the variational bound has been considered before , it was dismissed as infeasible .",
        "aspect": "inference model",
        "sentiment": 0
    },
    {
        "text": "Our primary contribution is to show how to reduce the variance of the naive gradient estimator to make it practical without narrowing its range of applicability .",
        "aspect": "naive gradient estimator",
        "sentiment": 1
    },
    {
        "text": "We also show that the resulting method trains sigmoid belief networks better than the wake - sleep algorithm , which is the only algorithm we are aware of that is capable of training the same range of models efficiently .",
        "aspect": "sigmoid belief networks",
        "sentiment": 0
    },
    {
        "text": "We also show that the resulting method trains sigmoid belief networks better than the wake - sleep algorithm , which is the only algorithm we are aware of that is capable of training the same range of models efficiently .",
        "aspect": "wake - sleep algorithm",
        "sentiment": 0
    },
    {
        "text": "Finally , we demonstrate the effectiveness and scalability of NVIL by using it to achieve stateof - the - art results on the Reuters RCV1 document dataset .",
        "aspect": "NVIL",
        "sentiment": 0
    },
    {
        "text": "We developed , NVIL , a new training method for intractable directed latent variable models which is general and easy to apply to new models .",
        "aspect": "NVIL",
        "sentiment": 2
    },
    {
        "text": "We developed , NVIL , a new training method for intractable directed latent variable models which is general and easy to apply to new models .",
        "aspect": "training method",
        "sentiment": 1
    },
    {
        "text": "We showed that NVIL consistently outperforms the wake - sleep algorithm at training sigmoidbelief - network - like models .",
        "aspect": "NVIL",
        "sentiment": 0
    },
    {
        "text": "We showed that NVIL consistently outperforms the wake - sleep algorithm at training sigmoidbelief - network - like models .",
        "aspect": "wake - sleep algorithm",
        "sentiment": 3
    },
    {
        "text": "We showed that NVIL consistently outperforms the wake - sleep algorithm at training sigmoidbelief - network - like models .",
        "aspect": "sigmoidbelief - network - like models",
        "sentiment": 0
    },
    {
        "text": "Applying NVIL to models with continuous latent variables is another promising direction since binary latent variables are not always appropriate .",
        "aspect": "NVIL",
        "sentiment": 0
    },
    {
        "text": "We expect NVIL to be also applicable to training conditional latent variable models for modelling the distribution of observations given some context , which would require making the inference network take both the context and the observation as input .",
        "aspect": "NVIL",
        "sentiment": 0
    },
    {
        "text": "We expect NVIL to be also applicable to training conditional latent variable models for modelling the distribution of observations given some context , which would require making the inference network take both the context and the observation as input .",
        "aspect": "conditional latent variable models",
        "sentiment": 0
    },
    {
        "text": "We expect NVIL to be also applicable to training conditional latent variable models for modelling the distribution of observations given some context , which would require making the inference network take both the context and the observation as input .",
        "aspect": "inference network",
        "sentiment": 0
    },
    {
        "text": "This would make it an alternative to the importance - sampling training method of for conditional models with structured high - dimensional outputs .",
        "aspect": "importance - sampling training method",
        "sentiment": 0
    },
    {
        "text": "This would make it an alternative to the importance - sampling training method of for conditional models with structured high - dimensional outputs .",
        "aspect": "conditional models",
        "sentiment": 0
    },
    {
        "text": "We hope that the generality and flexibility of our approach will make it easier to apply powerful directed latent variable models to real - world problems .",
        "aspect": "directed latent variable models",
        "sentiment": 0
    },
    {
        "text": "Context R - CNN : Long Term Temporal Context for Per - Camera Object Detection",
        "aspect": "Context R - CNN",
        "sentiment": 0
    },
    {
        "text": "In static monitoring cameras , useful contextual information can stretch far beyond the few seconds typical video understanding models might see : subjects may exhibit similar behavior over multiple days , and background objects remain static .",
        "aspect": "video understanding models",
        "sentiment": 0
    },
    {
        "text": "In this paper we propose a method that leverages temporal context from the unlabeled frames of a novel camera to improve performance at that camera .",
        "aspect": "leverages temporal context",
        "sentiment": 1
    },
    {
        "text": "Specifically , we propose an attention - based approach that allows our model , Context R - CNN , to index into a long term memory bank constructed on a per - camera basis and aggregate contextual features from other frames to boost object detection performance on the current frame .",
        "aspect": "attention - based approach",
        "sentiment": 1
    },
    {
        "text": "Specifically , we propose an attention - based approach that allows our model , Context R - CNN , to index into a long term memory bank constructed on a per - camera basis and aggregate contextual features from other frames to boost object detection performance on the current frame .",
        "aspect": "Context R - CNN",
        "sentiment": 2
    },
    {
        "text": "Specifically , we propose an attention - based approach that allows our model , Context R - CNN , to index into a long term memory bank constructed on a per - camera basis and aggregate contextual features from other frames to boost object detection performance on the current frame .",
        "aspect": "long term memory bank",
        "sentiment": 1
    },
    {
        "text": "missed by the single - frame model and corrected by our method are in green , and detections that are correct in both models are in blue .",
        "aspect": "single - frame model",
        "sentiment": 0
    },
    {
        "text": "Note that in camera traps , the intra - image context is very powerful due to the group behavior of animal species .",
        "aspect": "image context",
        "sentiment": 0
    },
    {
        "text": "More specifically , we propose a detection architecture , Context R - CNN , that learns to differentiably index into a long - term memory bank while performing detection within a static camera .",
        "aspect": "detection architecture",
        "sentiment": 1
    },
    {
        "text": "More specifically , we propose a detection architecture , Context R - CNN , that learns to differentiably index into a long - term memory bank while performing detection within a static camera .",
        "aspect": "Context R - CNN",
        "sentiment": 2
    },
    {
        "text": "At a high level , our approach can be framed as a non - parametric estimation method ( like nearest neighbors ) sitting on top of a high - powered parametric function ( Faster R - CNN ) .",
        "aspect": "non - parametric estimation method",
        "sentiment": 1
    },
    {
        "text": "At a high level , our approach can be framed as a non - parametric estimation method ( like nearest neighbors ) sitting on top of a high - powered parametric function ( Faster R - CNN ) .",
        "aspect": "high - powered parametric function",
        "sentiment": 1
    },
    {
        "text": "At a high level , our approach can be framed as a non - parametric estimation method ( like nearest neighbors ) sitting on top of a high - powered parametric function ( Faster R - CNN ) .",
        "aspect": "Faster R - CNN",
        "sentiment": 1
    },
    {
        "text": "When train and test locations are quite different , one might not expect a parametric method to generalize well , whereas Context R - CNN is able to leverage an unlabeled ' neighborhood ' of test examples for improved generalization .",
        "aspect": "parametric method",
        "sentiment": 0
    },
    {
        "text": "When train and test locations are quite different , one might not expect a parametric method to generalize well , whereas Context R - CNN is able to leverage an unlabeled ' neighborhood ' of test examples for improved generalization .",
        "aspect": "Context R - CNN",
        "sentiment": 2
    },
    {
        "text": "\u2022 Camera traps are remote static monitoring cameras used by biologists to study animal species occurrence , populations , and behavior .",
        "aspect": "Camera traps",
        "sentiment": 0
    },
    {
        "text": "\u2022 Camera traps are remote static monitoring cameras used by biologists to study animal species occurrence , populations , and behavior .",
        "aspect": "remote static monitoring cameras",
        "sentiment": 0
    },
    {
        "text": "\u2022 We propose Context R - CNN , which leverages temporal context for improving object detection regardless of frame rate or sampling irregularity .",
        "aspect": "Context R - CNN",
        "sentiment": 2
    },
    {
        "text": "\u2022 We propose Context R - CNN , which leverages temporal context for improving object detection regardless of frame rate or sampling irregularity .",
        "aspect": "temporal context",
        "sentiment": 1
    },
    {
        "text": "\u2022 We demonstrate major improvements over strong single - frame baselines ; on a commonly - used camera trap dataset we improve mAP at 0.5 IoU by 17.9 % .",
        "aspect": "single - frame baselines",
        "sentiment": 0
    },
    {
        "text": "\u2022 We show that Context R - CNN is able to leverage up to a month of temporal context which is significantly more than prior approaches .",
        "aspect": "Context R - CNN",
        "sentiment": 0
    },
    {
        "text": "These detection architectures include anchor - based models , both single stage ( e.g. , SSD , RetinaNet , Yolo ) and two - stage ( e.g. , Fast / Faster R - CNN , R - FCN ) , as well as more recent anchor - free models ( e.g. , CornerNet , Cen - terNet , FCOS ) .",
        "aspect": "detection architectures",
        "sentiment": 0
    },
    {
        "text": "These detection architectures include anchor - based models , both single stage ( e.g. , SSD , RetinaNet , Yolo ) and two - stage ( e.g. , Fast / Faster R - CNN , R - FCN ) , as well as more recent anchor - free models ( e.g. , CornerNet , Cen - terNet , FCOS ) .",
        "aspect": "anchor - based models",
        "sentiment": 0
    },
    {
        "text": "These detection architectures include anchor - based models , both single stage ( e.g. , SSD , RetinaNet , Yolo ) and two - stage ( e.g. , Fast / Faster R - CNN , R - FCN ) , as well as more recent anchor - free models ( e.g. , CornerNet , Cen - terNet , FCOS ) .",
        "aspect": "SSD",
        "sentiment": 0
    },
    {
        "text": "These detection architectures include anchor - based models , both single stage ( e.g. , SSD , RetinaNet , Yolo ) and two - stage ( e.g. , Fast / Faster R - CNN , R - FCN ) , as well as more recent anchor - free models ( e.g. , CornerNet , Cen - terNet , FCOS ) .",
        "aspect": "RetinaNet",
        "sentiment": 0
    },
    {
        "text": "These detection architectures include anchor - based models , both single stage ( e.g. , SSD , RetinaNet , Yolo ) and two - stage ( e.g. , Fast / Faster R - CNN , R - FCN ) , as well as more recent anchor - free models ( e.g. , CornerNet , Cen - terNet , FCOS ) .",
        "aspect": "Yolo",
        "sentiment": 0
    },
    {
        "text": "These detection architectures include anchor - based models , both single stage ( e.g. , SSD , RetinaNet , Yolo ) and two - stage ( e.g. , Fast / Faster R - CNN , R - FCN ) , as well as more recent anchor - free models ( e.g. , CornerNet , Cen - terNet , FCOS ) .",
        "aspect": "Fast / Faster R - CNN",
        "sentiment": 0
    },
    {
        "text": "These detection architectures include anchor - based models , both single stage ( e.g. , SSD , RetinaNet , Yolo ) and two - stage ( e.g. , Fast / Faster R - CNN , R - FCN ) , as well as more recent anchor - free models ( e.g. , CornerNet , Cen - terNet , FCOS ) .",
        "aspect": "R - FCN",
        "sentiment": 0
    },
    {
        "text": "These detection architectures include anchor - based models , both single stage ( e.g. , SSD , RetinaNet , Yolo ) and two - stage ( e.g. , Fast / Faster R - CNN , R - FCN ) , as well as more recent anchor - free models ( e.g. , CornerNet , Cen - terNet , FCOS ) .",
        "aspect": "anchor - free models",
        "sentiment": 0
    },
    {
        "text": "These detection architectures include anchor - based models , both single stage ( e.g. , SSD , RetinaNet , Yolo ) and two - stage ( e.g. , Fast / Faster R - CNN , R - FCN ) , as well as more recent anchor - free models ( e.g. , CornerNet , Cen - terNet , FCOS ) .",
        "aspect": "CornerNet",
        "sentiment": 0
    },
    {
        "text": "These detection architectures include anchor - based models , both single stage ( e.g. , SSD , RetinaNet , Yolo ) and two - stage ( e.g. , Fast / Faster R - CNN , R - FCN ) , as well as more recent anchor - free models ( e.g. , CornerNet , Cen - terNet , FCOS ) .",
        "aspect": "Cen - terNet",
        "sentiment": 0
    },
    {
        "text": "These detection architectures include anchor - based models , both single stage ( e.g. , SSD , RetinaNet , Yolo ) and two - stage ( e.g. , Fast / Faster R - CNN , R - FCN ) , as well as more recent anchor - free models ( e.g. , CornerNet , Cen - terNet , FCOS ) .",
        "aspect": "FCOS",
        "sentiment": 0
    },
    {
        "text": "Object detection methods have shown great improvements on COCO - or Imagenet - style images , but these gains do not always generalize to challenging real - world data ( See Figure ) .",
        "aspect": "Object detection methods",
        "sentiment": 0
    },
    {
        "text": "Single frame architectures then form the basis for video detection and spatio - temporal action localization architectures , which build upon single frame models by incorporating contextual cues from other frames in order to deal with more specific challenges that arise in video data including motion blur , occlusion , and rare poses .",
        "aspect": "Single frame architectures",
        "sentiment": 0
    },
    {
        "text": "Single frame architectures then form the basis for video detection and spatio - temporal action localization architectures , which build upon single frame models by incorporating contextual cues from other frames in order to deal with more specific challenges that arise in video data including motion blur , occlusion , and rare poses .",
        "aspect": "spatio - temporal action localization architectures",
        "sentiment": 0
    },
    {
        "text": "Single frame architectures then form the basis for video detection and spatio - temporal action localization architectures , which build upon single frame models by incorporating contextual cues from other frames in order to deal with more specific challenges that arise in video data including motion blur , occlusion , and rare poses .",
        "aspect": "single frame models",
        "sentiment": 0
    },
    {
        "text": "Leading methods have used pixel level flow ( or flow - like concepts ) to aggregate features or used correlation to densely relate features at the current timestep to an adjacent timestep .",
        "aspect": "Leading methods",
        "sentiment": 0
    },
    {
        "text": "Other papers have explored the use of 3d convolutions ( e.g. , I3D , S3D ) or recurrent networks to extract better temporal features .",
        "aspect": "3d convolutions",
        "sentiment": 0
    },
    {
        "text": "Other papers have explored the use of 3d convolutions ( e.g. , I3D , S3D ) or recurrent networks to extract better temporal features .",
        "aspect": "I3D",
        "sentiment": 0
    },
    {
        "text": "Other papers have explored the use of 3d convolutions ( e.g. , I3D , S3D ) or recurrent networks to extract better temporal features .",
        "aspect": "S3D",
        "sentiment": 0
    },
    {
        "text": "Other papers have explored the use of 3d convolutions ( e.g. , I3D , S3D ) or recurrent networks to extract better temporal features .",
        "aspect": "recurrent networks",
        "sentiment": 0
    },
    {
        "text": "Finally , many works apply video specific postprocessing to smooth predictions along time , including tubelet smoothing or SeqNMS .",
        "aspect": "video specific postprocessing",
        "sentiment": 0
    },
    {
        "text": "Finally , many works apply video specific postprocessing to smooth predictions along time , including tubelet smoothing or SeqNMS .",
        "aspect": "tubelet smoothing",
        "sentiment": 0
    },
    {
        "text": "Finally , many works apply video specific postprocessing to smooth predictions along time , including tubelet smoothing or SeqNMS .",
        "aspect": "SeqNMS",
        "sentiment": 0
    },
    {
        "text": "Object - level attention - based temporal aggregation methods .",
        "aspect": "Object - level attention - based temporal aggregation methods",
        "sentiment": 0
    },
    {
        "text": "The majority of the above video detection approaches are not well suited to our target setting of sparse , irregular frame rates .",
        "aspect": "video detection approaches",
        "sentiment": 0
    },
    {
        "text": "For example , flow based methods , 3d convolutions and LSTMs typically assume a dense , regular temporal sampling .",
        "aspect": "flow based methods",
        "sentiment": 0
    },
    {
        "text": "For example , flow based methods , 3d convolutions and LSTMs typically assume a dense , regular temporal sampling .",
        "aspect": "3d convolutions",
        "sentiment": 0
    },
    {
        "text": "For example , flow based methods , 3d convolutions and LSTMs typically assume a dense , regular temporal sampling .",
        "aspect": "LSTMs",
        "sentiment": 0
    },
    {
        "text": "For example , flow based methods , 3d convolutions and LSTMs typically assume a dense , regular temporal sampling .",
        "aspect": "dense , regular temporal sampling",
        "sentiment": 0
    },
    {
        "text": "And while models like LSTMs can theoretically depend on all past frames in a video , their effective temporal receptive field is typically much smaller .",
        "aspect": "LSTMs",
        "sentiment": 0
    },
    {
        "text": "To address this limitation of recurrent networks , the NLP community has introduced attention - based architectures as a way to take advantage of long range dependencies in sentences .",
        "aspect": "recurrent networks",
        "sentiment": 0
    },
    {
        "text": "To address this limitation of recurrent networks , the NLP community has introduced attention - based architectures as a way to take advantage of long range dependencies in sentences .",
        "aspect": "attention - based architectures",
        "sentiment": 0
    },
    {
        "text": "The vision community has followed suit with attention - based architectures that leverage longer term temporal context .",
        "aspect": "attention - based architectures",
        "sentiment": 0
    },
    {
        "text": "Along the same lines and most relevant to our work , there are a few recent works that rely on non - local attention mechanisms in order to aggregate information at the object level across time .",
        "aspect": "non - local attention mechanisms",
        "sentiment": 0
    },
    {
        "text": "For example , Wu et al applied non - local attention to person detections to accumulate context from pre - computed feature banks ( with frozen pre - trained feature extractors ) .",
        "aspect": "non - local attention",
        "sentiment": 1
    },
    {
        "text": "For example , Wu et al applied non - local attention to person detections to accumulate context from pre - computed feature banks ( with frozen pre - trained feature extractors ) .",
        "aspect": "feature extractors",
        "sentiment": 0
    },
    {
        "text": "For example , Wu et al applied non - local attention to person detections to accumulate context from pre - computed feature banks ( with frozen pre - trained feature extractors ) .",
        "aspect": "feature banks",
        "sentiment": 1
    },
    {
        "text": "These feature banks extend the time horizon of their network up to 60s in each direction , achieving strong results on spatiotemporal action localization .",
        "aspect": "feature banks",
        "sentiment": 0
    },
    {
        "text": "We similarly use a frozen feature extractor that allows us to create extremely long term memory banks which leverage the spatial consistency of static cameras and habitual behavior of the subjects across long time horizons ( up to a month ) .",
        "aspect": "frozen feature extractor",
        "sentiment": 1
    },
    {
        "text": "However Wu et al use a 3d convnet ( I3D ) for short term features which is not wellsuited to our setting due to low , irregular frame rate .",
        "aspect": "3d convnet",
        "sentiment": 0
    },
    {
        "text": "However Wu et al use a 3d convnet ( I3D ) for short term features which is not wellsuited to our setting due to low , irregular frame rate .",
        "aspect": "I3D",
        "sentiment": 0
    },
    {
        "text": "Instead we use a single frame model for the current frame which is more similar to who proposed variations of this idea for video object detection achieving strong results on the Imagenet Vid dataset .",
        "aspect": "frame model",
        "sentiment": 1
    },
    {
        "text": "In contrast to these three papers , we augment our model with an additional dedicated short term attention mechanism which we show to be effective in experiments .",
        "aspect": "dedicated short term attention mechanism",
        "sentiment": 1
    },
    {
        "text": "More generally , our paper adds to the growing evidence that this attention - based approach of temporally aggregating information at the object level is highly effective for incorporating more context in video understanding .",
        "aspect": "attention - based approach",
        "sentiment": 1
    },
    {
        "text": "Whereas a number of competing baselines like 3d convolutions and flow based techniques perform nearly as well as these attentionbased models on Imagenet Vid , the same baselines are not well - suited to our setting .",
        "aspect": "3d convolutions",
        "sentiment": 3
    },
    {
        "text": "Whereas a number of competing baselines like 3d convolutions and flow based techniques perform nearly as well as these attentionbased models on Imagenet Vid , the same baselines are not well - suited to our setting .",
        "aspect": "flow based techniques",
        "sentiment": 3
    },
    {
        "text": "Whereas a number of competing baselines like 3d convolutions and flow based techniques perform nearly as well as these attentionbased models on Imagenet Vid , the same baselines are not well - suited to our setting .",
        "aspect": "attentionbased models",
        "sentiment": 0
    },
    {
        "text": "Thus , we see a larger performance boost from prior , non - attention - based methods to our attention - based approach .",
        "aspect": "non - attention - based methods",
        "sentiment": 0
    },
    {
        "text": "Thus , we see a larger performance boost from prior , non - attention - based methods to our attention - based approach .",
        "aspect": "attention - based approach",
        "sentiment": 1
    },
    {
        "text": "We compare our results to a ( comparable ) single - frame baseline for all three datasets .",
        "aspect": "single - frame baseline",
        "sentiment": 3
    },
    {
        "text": "We focus the majority of our experiments on a single dataset , Snapshot Serengeti , investigating the effects of both short term and long term attention , the feature extractor , the long term time horizon , and the frame - wise sampling strategy for M long .",
        "aspect": "feature extractor",
        "sentiment": 0
    },
    {
        "text": "We focus the majority of our experiments on a single dataset , Snapshot Serengeti , investigating the effects of both short term and long term attention , the feature extractor , the long term time horizon , and the frame - wise sampling strategy for M long .",
        "aspect": "frame - wise sampling strategy",
        "sentiment": 0
    },
    {
        "text": "Context R - CNN strongly outperforms the single - frame Faster RCNN with Resnet-101 baseline on both the Snapshot Serengeti ( SS ) and Caltech Camera Traps ( CCT ) For SS , we also compare against several baselines with access to short term temporal information ( Table ( d ) ) .",
        "aspect": "Context R - CNN",
        "sentiment": 0
    },
    {
        "text": "Context R - CNN strongly outperforms the single - frame Faster RCNN with Resnet-101 baseline on both the Snapshot Serengeti ( SS ) and Caltech Camera Traps ( CCT ) For SS , we also compare against several baselines with access to short term temporal information ( Table ( d ) ) .",
        "aspect": "single - frame Faster RCNN",
        "sentiment": 3
    },
    {
        "text": "Context R - CNN strongly outperforms the single - frame Faster RCNN with Resnet-101 baseline on both the Snapshot Serengeti ( SS ) and Caltech Camera Traps ( CCT ) For SS , we also compare against several baselines with access to short term temporal information ( Table ( d ) ) .",
        "aspect": "Resnet-101 baseline",
        "sentiment": 0
    },
    {
        "text": "\u2022 We attempt to leverage the static - ness of the camera by taking a temporal - distance - weighted average of the RPN box classifier features from the key frame with the cropped RPN features from the same box locations from the surrounding frames ( ST Spatial ) , and find it outperforms the single - frame baseline by 1.9 % mAP .",
        "aspect": "temporal - distance - weighted average of the RPN box classifier features",
        "sentiment": 0
    },
    {
        "text": "\u2022 We attempt to leverage the static - ness of the camera by taking a temporal - distance - weighted average of the RPN box classifier features from the key frame with the cropped RPN features from the same box locations from the surrounding frames ( ST Spatial ) , and find it outperforms the single - frame baseline by 1.9 % mAP .",
        "aspect": "single - frame baseline",
        "sentiment": 3
    },
    {
        "text": "\u2022 We attempt to leverage the static - ness of the camera by taking a temporal - distance - weighted average of the RPN box classifier features from the key frame with the cropped RPN features from the same box locations from the surrounding frames ( ST Spatial ) , and find it outperforms the single - frame baseline by 1.9 % mAP .",
        "aspect": "ST Spatial",
        "sentiment": 0
    },
    {
        "text": "\u2022 S3D , a popular video object detection model , outperforms single - frame by 6.8 % mAP despite being designed for consistently sampled high frame rate video .",
        "aspect": "S3D",
        "sentiment": 0
    },
    {
        "text": "\u2022 S3D , a popular video object detection model , outperforms single - frame by 6.8 % mAP despite being designed for consistently sampled high frame rate video .",
        "aspect": "video object detection model",
        "sentiment": 0
    },
    {
        "text": "In this work , we contribute a model that leverages percamera temporal context up to a month , far beyond the time horizon of previous approaches , and show that in the static camera setting , attention - based temporal context is particularly beneficial .",
        "aspect": "attention - based temporal context",
        "sentiment": 1
    },
    {
        "text": "Our method , Context R - CNN , is general across static camera domains , improving detection performance over single - frame baselines on both camera trap and traffic camera data .",
        "aspect": "Context R - CNN",
        "sentiment": 2
    },
    {
        "text": "Our method , Context R - CNN , is general across static camera domains , improving detection performance over single - frame baselines on both camera trap and traffic camera data .",
        "aspect": "single - frame baselines",
        "sentiment": 1
    },
    {
        "text": "Additionally , Context R - CNN is adaptive and robust to passive - monitoring sampling strategies that provide data streams with low , irregular frame rates .",
        "aspect": "Context R - CNN",
        "sentiment": 0
    },
    {
        "text": "MobileBERT : a Compact Task - Agnostic BERT for Resource - Limited Devices",
        "aspect": "MobileBERT",
        "sentiment": 0
    },
    {
        "text": "MobileBERT : a Compact Task - Agnostic BERT for Resource - Limited Devices",
        "aspect": "Compact Task - Agnostic BERT",
        "sentiment": 0
    },
    {
        "text": "Natural Language Processing ( NLP ) has recently achieved great success by using huge pre - trained models with hundreds of millions of parameters .",
        "aspect": "pre - trained models",
        "sentiment": 0
    },
    {
        "text": "In this paper , we propose MobileBERT for compressing and accelerating the popular BERT model .",
        "aspect": "MobileBERT",
        "sentiment": 2
    },
    {
        "text": "In this paper , we propose MobileBERT for compressing and accelerating the popular BERT model .",
        "aspect": "BERT",
        "sentiment": 1
    },
    {
        "text": "Like the original BERT , MobileBERT is task - agnostic , that is , it can be generically applied to various downstream NLP tasks via simple fine - tuning .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Like the original BERT , MobileBERT is task - agnostic , that is , it can be generically applied to various downstream NLP tasks via simple fine - tuning .",
        "aspect": "MobileBERT",
        "sentiment": 0
    },
    {
        "text": "Like the original BERT , MobileBERT is task - agnostic , that is , it can be generically applied to various downstream NLP tasks via simple fine - tuning .",
        "aspect": "fine - tuning",
        "sentiment": 0
    },
    {
        "text": "Basically , MobileBERT is a thin version of BERT LARGE , while equipped with bottleneck structures and a carefully designed balance between self - attentions and feed - forward networks .",
        "aspect": "MobileBERT",
        "sentiment": 1
    },
    {
        "text": "Basically , MobileBERT is a thin version of BERT LARGE , while equipped with bottleneck structures and a carefully designed balance between self - attentions and feed - forward networks .",
        "aspect": "BERT",
        "sentiment": 1
    },
    {
        "text": "Basically , MobileBERT is a thin version of BERT LARGE , while equipped with bottleneck structures and a carefully designed balance between self - attentions and feed - forward networks .",
        "aspect": "bottleneck structures",
        "sentiment": 1
    },
    {
        "text": "Basically , MobileBERT is a thin version of BERT LARGE , while equipped with bottleneck structures and a carefully designed balance between self - attentions and feed - forward networks .",
        "aspect": "self - attentions",
        "sentiment": 1
    },
    {
        "text": "Basically , MobileBERT is a thin version of BERT LARGE , while equipped with bottleneck structures and a carefully designed balance between self - attentions and feed - forward networks .",
        "aspect": "feed - forward networks",
        "sentiment": 1
    },
    {
        "text": "To train MobileBERT , we first train a specially designed teacher model , an invertedbottleneck incorporated BERT LARGE model .",
        "aspect": "MobileBERT",
        "sentiment": 2
    },
    {
        "text": "To train MobileBERT , we first train a specially designed teacher model , an invertedbottleneck incorporated BERT LARGE model .",
        "aspect": "teacher model",
        "sentiment": 1
    },
    {
        "text": "To train MobileBERT , we first train a specially designed teacher model , an invertedbottleneck incorporated BERT LARGE model .",
        "aspect": "invertedbottleneck incorporated BERT LARGE model",
        "sentiment": 1
    },
    {
        "text": "Then , we conduct knowledge transfer from this teacher to MobileBERT .",
        "aspect": "MobileBERT",
        "sentiment": 2
    },
    {
        "text": "Empirical studies show that MobileBERT is 4.3\u00d7 smaller and 5.5\u00d7 faster than BERT BASE while achieving competitive results on well - known benchmarks .",
        "aspect": "MobileBERT",
        "sentiment": 2
    },
    {
        "text": "Empirical studies show that MobileBERT is 4.3\u00d7 smaller and 5.5\u00d7 faster than BERT BASE while achieving competitive results on well - known benchmarks .",
        "aspect": "BERT",
        "sentiment": 3
    },
    {
        "text": "On the natural language inference tasks of GLUE , MobileBERT achieves a GLUE score of 77.7 ( 0.6 lower than BERT BASE ) , and 62 ms latency on a Pixel 4 phone .",
        "aspect": "MobileBERT",
        "sentiment": 2
    },
    {
        "text": "On the natural language inference tasks of GLUE , MobileBERT achieves a GLUE score of 77.7 ( 0.6 lower than BERT BASE ) , and 62 ms latency on a Pixel 4 phone .",
        "aspect": "BERT",
        "sentiment": 3
    },
    {
        "text": "question answering task , MobileBERT achieves a dev F1 score of 90.0/79.2",
        "aspect": "MobileBERT",
        "sentiment": 2
    },
    {
        "text": "( 1.5/2.1 higher than BERT BASE ) .",
        "aspect": "BERT",
        "sentiment": 3
    },
    {
        "text": "The NLP community has witnessed a revolution of pre - training self - supervised models .",
        "aspect": "self - supervised models",
        "sentiment": 0
    },
    {
        "text": "Among these models , BERT shows substantial accuracy improvements .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "However , as one of the largest models ever in NLP , BERT suffers from the heavy model size and high latency , making it impractical for resource - limited mobile devices to deploy the power of BERT in mobile - based machine translation , dialogue modeling , and the like .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "However , as one of the largest models ever in NLP , BERT suffers from the heavy model size and high latency , making it impractical for resource - limited mobile devices to deploy the power of BERT in mobile - based machine translation , dialogue modeling , and the like .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "There have been some efforts that taskspecifically distill BERT into compact models .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "There have been some efforts that taskspecifically distill BERT into compact models .",
        "aspect": "compact models",
        "sentiment": 0
    },
    {
        "text": "To the best of our knowledge , there is not yet any work for building a taskagnostic lightweight pre - trained model , that is , a model that can be generically fine - tuned on different downstream NLP tasks as the original BERT does .",
        "aspect": "taskagnostic lightweight pre - trained model",
        "sentiment": 0
    },
    {
        "text": "To the best of our knowledge , there is not yet any work for building a taskagnostic lightweight pre - trained model , that is , a model that can be generically fine - tuned on different downstream NLP tasks as the original BERT does .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "In this paper , we propose MobileBERT to fill this gap .",
        "aspect": "MobileBERT",
        "sentiment": 2
    },
    {
        "text": "Task - specific compression needs to first fine - tune the original large BERT model into a task - specific teacher and then distill .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Such a process is much more complicated and costly than directly fine - tuning a task - agnostic compact model .",
        "aspect": "task - agnostic compact model",
        "sentiment": 0
    },
    {
        "text": "At first glance , it may seem straightforward to obtain a task - agnostic compact BERT .",
        "aspect": "task - agnostic compact BERT",
        "sentiment": 0
    },
    {
        "text": "For example , one may just take a narrower or shallower version of BERT , and train it until convergence by minimizing a convex combination of the prediction loss and distillation loss .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "It is well - known that shallow networks usually do not have enough representation power while narrow and deep networks are difficult to train .",
        "aspect": "shallow networks",
        "sentiment": 0
    },
    {
        "text": "It is well - known that shallow networks usually do not have enough representation power while narrow and deep networks are difficult to train .",
        "aspect": "narrow and deep networks",
        "sentiment": 0
    },
    {
        "text": "Our MobileBERT is designed to be as deep as BERT LARGE while each layer is made much narrower via adopting bottleneck structures and balancing between self - attentions and feed - forward networks ( Figure ) .",
        "aspect": "MobileBERT",
        "sentiment": 3
    },
    {
        "text": "Our MobileBERT is designed to be as deep as BERT LARGE while each layer is made much narrower via adopting bottleneck structures and balancing between self - attentions and feed - forward networks ( Figure ) .",
        "aspect": "bottleneck structures",
        "sentiment": 1
    },
    {
        "text": "Our MobileBERT is designed to be as deep as BERT LARGE while each layer is made much narrower via adopting bottleneck structures and balancing between self - attentions and feed - forward networks ( Figure ) .",
        "aspect": "self - attentions",
        "sentiment": 1
    },
    {
        "text": "Our MobileBERT is designed to be as deep as BERT LARGE while each layer is made much narrower via adopting bottleneck structures and balancing between self - attentions and feed - forward networks ( Figure ) .",
        "aspect": "feed - forward networks",
        "sentiment": 1
    },
    {
        "text": "Our MobileBERT is designed to be as deep as BERT LARGE while each layer is made much narrower via adopting bottleneck structures and balancing between self - attentions and feed - forward networks ( Figure ) .",
        "aspect": "BERT",
        "sentiment": 3
    },
    {
        "text": "To train MobileBERT , a deep and thin model , we first train a specially designed teacher model , an inverted - bottleneck incorporated BERT LARGE model ( IB - BERT ) .",
        "aspect": "MobileBERT",
        "sentiment": 2
    },
    {
        "text": "To train MobileBERT , a deep and thin model , we first train a specially designed teacher model , an inverted - bottleneck incorporated BERT LARGE model ( IB - BERT ) .",
        "aspect": "deep and thin model",
        "sentiment": 1
    },
    {
        "text": "To train MobileBERT , a deep and thin model , we first train a specially designed teacher model , an inverted - bottleneck incorporated BERT LARGE model ( IB - BERT ) .",
        "aspect": "teacher model",
        "sentiment": 1
    },
    {
        "text": "To train MobileBERT , a deep and thin model , we first train a specially designed teacher model , an inverted - bottleneck incorporated BERT LARGE model ( IB - BERT ) .",
        "aspect": "inverted - bottleneck incorporated BERT LARGE model",
        "sentiment": 1
    },
    {
        "text": "To train MobileBERT , a deep and thin model , we first train a specially designed teacher model , an inverted - bottleneck incorporated BERT LARGE model ( IB - BERT ) .",
        "aspect": "IB - BERT",
        "sentiment": 1
    },
    {
        "text": "Then , we conduct knowledge transfer from IB - BERT to MobileBERT .",
        "aspect": "IB - BERT",
        "sentiment": 1
    },
    {
        "text": "Then , we conduct knowledge transfer from IB - BERT to MobileBERT .",
        "aspect": "MobileBERT",
        "sentiment": 2
    },
    {
        "text": "A variety of knowledge transfer strategies are carefully investigated in our empirical studies .",
        "aspect": "knowledge transfer strategies",
        "sentiment": 0
    },
    {
        "text": "Empirical evaluations 1 show that MobileBERT is 4.3\u00d7 smaller and 5.5\u00d7 faster than BERT BASE , while it can still achieve competitive results on well - known NLP benchmarks .",
        "aspect": "MobileBERT",
        "sentiment": 0
    },
    {
        "text": "Empirical evaluations 1 show that MobileBERT is 4.3\u00d7 smaller and 5.5\u00d7 faster than BERT BASE , while it can still achieve competitive results on well - known NLP benchmarks .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "On the natural language inference tasks of GLUE , MobileBERT can achieve a GLUE score of 77.7 , which is only 0.6 lower than BERT BASE , with a latency of 62 ms on a Pixel 4 phone .",
        "aspect": "MobileBERT",
        "sentiment": 2
    },
    {
        "text": "On the natural language inference tasks of GLUE , MobileBERT can achieve a GLUE score of 77.7 , which is only 0.6 lower than BERT BASE , with a latency of 62 ms on a Pixel 4 phone .",
        "aspect": "BERT",
        "sentiment": 3
    },
    {
        "text": "question answering task , MobileBER obtains a dev F1 score of 90.3/80.2 ,",
        "aspect": "MobileBER",
        "sentiment": 2
    },
    {
        "text": "which is even 1.5/2.1 higher than BERT BASE .",
        "aspect": "BERT",
        "sentiment": 3
    },
    {
        "text": "Concurrently to our work , distill BERT into shallower students through knowledge distillation and an additional knowledge transfer of hidden states on multiple intermediate layers .",
        "aspect": "knowledge distillation",
        "sentiment": 1
    },
    {
        "text": "Concurrently to our work , distill BERT into shallower students through knowledge distillation and an additional knowledge transfer of hidden states on multiple intermediate layers .",
        "aspect": "knowledge transfer",
        "sentiment": 0
    },
    {
        "text": "Concurrently to our work , distill BERT into shallower students through knowledge distillation and an additional knowledge transfer of hidden states on multiple intermediate layers .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "In contrast to these existing literature , we only use knowledge transfer in the pre - training stage and do not require a fine - tuned teacher or data augmentation in the down - stream tasks .",
        "aspect": "knowledge transfer",
        "sentiment": 1
    },
    {
        "text": "In contrast to these existing literature , we only use knowledge transfer in the pre - training stage and do not require a fine - tuned teacher or data augmentation in the down - stream tasks .",
        "aspect": "teacher",
        "sentiment": 0
    },
    {
        "text": "In contrast to these existing literature , we only use knowledge transfer in the pre - training stage and do not require a fine - tuned teacher or data augmentation in the down - stream tasks .",
        "aspect": "data augmentation",
        "sentiment": 0
    },
    {
        "text": "Following BERT , we use the BooksCorpus and English Wikipedia as our pre - training data .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "To make the IB - BERT LARGE teacher reach the same accuracy as original BERT LARGE , we train IB - BERT LARGE on 256 TPU v3 chips for 500k steps with a batch size of 4096 and LAMB optimizer .",
        "aspect": "IB - BERT",
        "sentiment": 0
    },
    {
        "text": "To make the IB - BERT LARGE teacher reach the same accuracy as original BERT LARGE , we train IB - BERT LARGE on 256 TPU v3 chips for 500k steps with a batch size of 4096 and LAMB optimizer .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "To make the IB - BERT LARGE teacher reach the same accuracy as original BERT LARGE , we train IB - BERT LARGE on 256 TPU v3 chips for 500k steps with a batch size of 4096 and LAMB optimizer .",
        "aspect": "IB - BERT LARGE",
        "sentiment": 1
    },
    {
        "text": "To make the IB - BERT LARGE teacher reach the same accuracy as original BERT LARGE , we train IB - BERT LARGE on 256 TPU v3 chips for 500k steps with a batch size of 4096 and LAMB optimizer .",
        "aspect": "LAMB optimizer",
        "sentiment": 0
    },
    {
        "text": "For a fair comparison with the original BERT , we do not use training tricks in other BERT variants .",
        "aspect": "BERT",
        "sentiment": 3
    },
    {
        "text": "For a fair comparison with the original BERT , we do not use training tricks in other BERT variants .",
        "aspect": "BERT variants",
        "sentiment": 0
    },
    {
        "text": "For Mo - bileBERT , we use the same training schedule in the pre - training distillation stage .",
        "aspect": "Mo - bileBERT",
        "sentiment": 2
    },
    {
        "text": "Additionally , we use progressive knowledge transfer to train Mo - bileBERT , which takes additional 240k steps over 24 layers .",
        "aspect": "progressive knowledge transfer",
        "sentiment": 1
    },
    {
        "text": "Additionally , we use progressive knowledge transfer to train Mo - bileBERT , which takes additional 240k steps over 24 layers .",
        "aspect": "Mo - bileBERT",
        "sentiment": 2
    },
    {
        "text": "In ablation studies , we halve the pretraining distillation schedule of MobileBERT to accelerate experiments .",
        "aspect": "pretraining distillation schedule",
        "sentiment": 0
    },
    {
        "text": "In ablation studies , we halve the pretraining distillation schedule of MobileBERT to accelerate experiments .",
        "aspect": "MobileBERT",
        "sentiment": 2
    },
    {
        "text": "Moreover , in the ablation study of knowledge transfer strategies , for a fair comparison , joint knowledge transfer and auxiliary knowledge transfer also take additional 240k steps .",
        "aspect": "joint knowledge transfer",
        "sentiment": 0
    },
    {
        "text": "Moreover , in the ablation study of knowledge transfer strategies , for a fair comparison , joint knowledge transfer and auxiliary knowledge transfer also take additional 240k steps .",
        "aspect": "auxiliary knowledge transfer",
        "sentiment": 0
    },
    {
        "text": "For the downstream tasks , all reported results are obtained by simply fine - tuning MobileBERT just like what the original BERT does .",
        "aspect": "MobileBERT",
        "sentiment": 0
    },
    {
        "text": "For the downstream tasks , all reported results are obtained by simply fine - tuning MobileBERT just like what the original BERT does .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "To finetune the pre - trained models , we search the optimization hyperparameters in a search space including different batch sizes ( 16/32/48 ) , learning rates ( ( 1 - 10 ) * e-5 ) , and the number of epochs ( 2 - 10 ) .",
        "aspect": "pre - trained models",
        "sentiment": 0
    },
    {
        "text": "The search space is different from the original BERT because we find that MobileBERT usually needs a larger learning rate and more training epochs in fine - tuning .",
        "aspect": "MobileBERT",
        "sentiment": 2
    },
    {
        "text": "We have presented MobileBERT which is a taskagnostic compact variant of BERT .",
        "aspect": "MobileBERT",
        "sentiment": 2
    },
    {
        "text": "We have presented MobileBERT which is a taskagnostic compact variant of BERT .",
        "aspect": "BERT",
        "sentiment": 3
    },
    {
        "text": "Empirical results on popular NLP benchmarks show that Mo - bileBERT is comparable with BERT BASE while being much smaller and faster .",
        "aspect": "Mo - bileBERT",
        "sentiment": 2
    },
    {
        "text": "Empirical results on popular NLP benchmarks show that Mo - bileBERT is comparable with BERT BASE while being much smaller and faster .",
        "aspect": "BERT",
        "sentiment": 3
    },
    {
        "text": "In this paper , we show that 1 ) it is crucial to keep MobileBERT deep and thin , 2 ) bottleneck / invertedbottleneck structures enable effective layer - wise knowledge transfer , and 3 ) progressive knowledge transfer can efficiently train MobileBERT .",
        "aspect": "progressive knowledge transfer",
        "sentiment": 1
    },
    {
        "text": "In this paper , we show that 1 ) it is crucial to keep MobileBERT deep and thin , 2 ) bottleneck / invertedbottleneck structures enable effective layer - wise knowledge transfer , and 3 ) progressive knowledge transfer can efficiently train MobileBERT .",
        "aspect": "MobileBERT",
        "sentiment": 2
    },
    {
        "text": "In this paper , we show that 1 ) it is crucial to keep MobileBERT deep and thin , 2 ) bottleneck / invertedbottleneck structures enable effective layer - wise knowledge transfer , and 3 ) progressive knowledge transfer can efficiently train MobileBERT .",
        "aspect": "MobileBERT",
        "sentiment": 2
    },
    {
        "text": "Exploiting knowledge transfer to compress model size was first proposed by .",
        "aspect": "knowledge transfer",
        "sentiment": 1
    },
    {
        "text": "The idea was then adopted in knowledge distillation , which requires the smaller student network to mimic the class distribution output of the larger teacher network .",
        "aspect": "student network",
        "sentiment": 0
    },
    {
        "text": "The idea was then adopted in knowledge distillation , which requires the smaller student network to mimic the class distribution output of the larger teacher network .",
        "aspect": "teacher network",
        "sentiment": 0
    },
    {
        "text": "Fitnets make the student mimic the intermediate hidden layers of the teacher to train narrow and deep networks .",
        "aspect": "Fitnets",
        "sentiment": 0
    },
    {
        "text": "Fitnets make the student mimic the intermediate hidden layers of the teacher to train narrow and deep networks .",
        "aspect": "narrow and deep networks",
        "sentiment": 0
    },
    {
        "text": "Similar to our proposed progressive knowledge transfer scheme , proposed a sequential knowledge transfer scheme to distill knowledge from a deep teacher into a shallow student in a sequential way .",
        "aspect": "progressive knowledge transfer scheme",
        "sentiment": 1
    },
    {
        "text": "Similar to our proposed progressive knowledge transfer scheme , proposed a sequential knowledge transfer scheme to distill knowledge from a deep teacher into a shallow student in a sequential way .",
        "aspect": "sequential knowledge transfer scheme",
        "sentiment": 1
    },
    {
        "text": "proposed to transfer the similarity of hidden states and word alignment from an autoregressive Transformer teacher to a non - autoregressive student .",
        "aspect": "autoregressive Transformer teacher",
        "sentiment": 0
    },
    {
        "text": "proposed to transfer the similarity of hidden states and word alignment from an autoregressive Transformer teacher to a non - autoregressive student .",
        "aspect": "autoregressive",
        "sentiment": 0
    },
    {
        "text": "YOLOv3 : An Incremental Improvement",
        "aspect": "YOLOv3",
        "sentiment": 0
    },
    {
        "text": "We present some updates to YOLO !",
        "aspect": "YOLO",
        "sentiment": 0
    },
    {
        "text": "At 320 \u00d7 320 YOLOv3 runs in 22 ms at 28.2 mAP , as accurate as SSD but three times faster .",
        "aspect": "YOLOv3",
        "sentiment": 2
    },
    {
        "text": "At 320 \u00d7 320 YOLOv3 runs in 22 ms at 28.2 mAP , as accurate as SSD but three times faster .",
        "aspect": "SSD",
        "sentiment": 3
    },
    {
        "text": "When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good .",
        "aspect": "YOLOv3",
        "sentiment": 2
    },
    {
        "text": "It achieves 57.9 AP 50 in 51 ms on a Titan X , compared to 57.5 AP 50 in 198 ms by RetinaNet , similar performance but 3.8\u00d7 faster .",
        "aspect": "RetinaNet",
        "sentiment": 3
    },
    {
        "text": "Played around with GANs a little .",
        "aspect": "GANs",
        "sentiment": 0
    },
    {
        "text": "I had a little momentum left over from last year ; I managed to make some improvements to YOLO .",
        "aspect": "YOLO",
        "sentiment": 0
    },
    {
        "text": "We have a camera - ready deadline and we need to cite some of the random updates I made to YOLO but we do n't have a source .",
        "aspect": "YOLO",
        "sentiment": 0
    },
    {
        "text": "First we 'll tell you what the deal is with YOLOv3 .",
        "aspect": "YOLOv3",
        "sentiment": 2
    },
    {
        "text": "The surprising impact of mask - head architecture on novel class segmentation",
        "aspect": "mask - head architecture",
        "sentiment": 0
    },
    {
        "text": "Instance segmentation models today are very accurate when trained on large annotated datasets , but collecting mask annotations at scale is prohibitively expensive .",
        "aspect": "Instance segmentation models",
        "sentiment": 0
    },
    {
        "text": "In this work , we focus on a popular family of models which apply differentiable cropping to a feature map and predict a mask based on the resulting crop .",
        "aspect": "differentiable cropping",
        "sentiment": 0
    },
    {
        "text": "We call this phenomenon the strong mask generalization effect , which we exploit by replacing the typical mask - head of 2 - 4 layers with significantly deeper offthe - shelf architectures ( e.g.",
        "aspect": "mask - head of 2 - 4 layers",
        "sentiment": 0
    },
    {
        "text": "We call this phenomenon the strong mask generalization effect , which we exploit by replacing the typical mask - head of 2 - 4 layers with significantly deeper offthe - shelf architectures ( e.g.",
        "aspect": "offthe - shelf architectures",
        "sentiment": 0
    },
    {
        "text": "ResNet , Hourglass models ) .",
        "aspect": "ResNet",
        "sentiment": 0
    },
    {
        "text": "ResNet , Hourglass models ) .",
        "aspect": "Hourglass models",
        "sentiment": 0
    },
    {
        "text": "We also show that the choice of mask - head architecture alone can lead to SOTA results on the partially supervised COCO benchmark without the need of specialty modules or losses proposed by prior literature .",
        "aspect": "mask - head architecture",
        "sentiment": 1
    },
    {
        "text": "Finally , we demonstrate that our effect is general , holding across underlying detection methodologies , ( e.g. both anchor - based or anchor free or no detector at all ) and across different backbone networks .",
        "aspect": "detection methodologies",
        "sentiment": 0
    },
    {
        "text": "Finally , we demonstrate that our effect is general , holding across underlying detection methodologies , ( e.g. both anchor - based or anchor free or no detector at all ) and across different backbone networks .",
        "aspect": "no detector",
        "sentiment": 0
    },
    {
        "text": "Finally , we demonstrate that our effect is general , holding across underlying detection methodologies , ( e.g. both anchor - based or anchor free or no detector at all ) and across different backbone networks .",
        "aspect": "backbone networks",
        "sentiment": 0
    },
    {
        "text": "Finally , we demonstrate that our effect is general , holding across underlying detection methodologies , ( e.g. both anchor - based or anchor free or no detector at all ) and across different backbone networks .",
        "aspect": "anchor - based",
        "sentiment": 0
    },
    {
        "text": "Finally , we demonstrate that our effect is general , holding across underlying detection methodologies , ( e.g. both anchor - based or anchor free or no detector at all ) and across different backbone networks .",
        "aspect": "anchor free",
        "sentiment": 0
    },
    {
        "text": "Large labeled datasets like COCO are crucial for deep neural network based instance segmentation methods .",
        "aspect": "deep neural network based instance segmentation methods",
        "sentiment": 0
    },
    {
        "text": "However , collecting groundtruth masks can take > 10\u00d7 more time than bounding box annotations .",
        "aspect": "bounding box",
        "sentiment": 0
    },
    {
        "text": "In COCO , mask annotations required \u2248 80 seconds on average whereas methods such as Extreme Clicking yield bounding boxes in 7 seconds .",
        "aspect": "Extreme Clicking",
        "sentiment": 0
    },
    {
        "text": "Given that boxes are much cheaper to annotate than masks , we address the \" partially supervised \" instance seg- Figure : The effect of mask - head architecture on mask predictions for unseen classes .",
        "aspect": "mask - head architecture",
        "sentiment": 0
    },
    {
        "text": "Despite never having seen masks from the ' parking meter ' , ' pizza ' or ' mobile phone ' class , the rightmost mask - head architecture can segment these classes correctly .",
        "aspect": "mask - head architecture",
        "sentiment": 0
    },
    {
        "text": "From left to right , we show better maskhead architectures predicting better masks .",
        "aspect": "maskhead architectures",
        "sentiment": 0
    },
    {
        "text": "We consider a general family of segmentation models where one extracts a feature map over an image , then given a tight bounding box around an instance , performs a differentiable crop ( e.g.",
        "aspect": "segmentation models",
        "sentiment": 0
    },
    {
        "text": "The cropped feature map is then fed to a mask - head subnetwork to yield a final mask prediction .",
        "aspect": "mask - head subnetwork",
        "sentiment": 1
    },
    {
        "text": "This mask prediction is performed in a class agnostic manner so that a model trained from a subset of classes can be applied unchanged to novel classes .",
        "aspect": "class agnostic manner",
        "sentiment": 0
    },
    {
        "text": "Previous approaches in this family have used , e.g. , offline - trained shape priors or specialty losses .",
        "aspect": "offline - trained shape priors",
        "sentiment": 0
    },
    {
        "text": "In contrast , we focus on an overlooked part of the problem : the architectural design of the mask - head .",
        "aspect": "mask - head",
        "sentiment": 0
    },
    {
        "text": "While trying alternative mask - head architectures may seem an obvious approach , it is underexplored in the prior work because the choice of mask - head architecture has limited impact in the fully supervised setting and ( 2 ) heavier mask - heads adversely impact running time .",
        "aspect": "mask - head architectures",
        "sentiment": 0
    },
    {
        "text": "While trying alternative mask - head architectures may seem an obvious approach , it is underexplored in the prior work because the choice of mask - head architecture has limited impact in the fully supervised setting and ( 2 ) heavier mask - heads adversely impact running time .",
        "aspect": "mask - head architecture",
        "sentiment": 0
    },
    {
        "text": "Thus most prior works in instance segmentation have settled on using shallow ( 2 - 4 layer ) fully connected or convolution based mask - heads .",
        "aspect": "shallow ( 2 - 4 layer",
        "sentiment": 0
    },
    {
        "text": "Thus most prior works in instance segmentation have settled on using shallow ( 2 - 4 layer ) fully connected or convolution based mask - heads .",
        "aspect": "fully connected",
        "sentiment": 0
    },
    {
        "text": "Thus most prior works in instance segmentation have settled on using shallow ( 2 - 4 layer ) fully connected or convolution based mask - heads .",
        "aspect": "convolution based mask - heads",
        "sentiment": 0
    },
    {
        "text": "We will refer to this effect of certain mask - head architectures on unseen classes as the \" strong mask generalization effect \" and illustrate it with 3 representative model classes : an anchor - free and anchor - based model , and one that discards detection altogether .",
        "aspect": "mask - head architectures",
        "sentiment": 0
    },
    {
        "text": "We will refer to this effect of certain mask - head architectures on unseen classes as the \" strong mask generalization effect \" and illustrate it with 3 representative model classes : an anchor - free and anchor - based model , and one that discards detection altogether .",
        "aspect": "anchor - free",
        "sentiment": 0
    },
    {
        "text": "We will refer to this effect of certain mask - head architectures on unseen classes as the \" strong mask generalization effect \" and illustrate it with 3 representative model classes : an anchor - free and anchor - based model , and one that discards detection altogether .",
        "aspect": "anchor - based",
        "sentiment": 0
    },
    {
        "text": "We show that our effect is general , holding across underlying detection methodologies ( or no detector at all ) and across different backbone networks .",
        "aspect": "detection methodologies",
        "sentiment": 0
    },
    {
        "text": "We show that our effect is general , holding across underlying detection methodologies ( or no detector at all ) and across different backbone networks .",
        "aspect": "backbone networks",
        "sentiment": 0
    },
    {
        "text": "Our strongest results come from our anchor - free model , which adds a mask - head to CenterNet .",
        "aspect": "anchor - free model",
        "sentiment": 0
    },
    {
        "text": "Our strongest results come from our anchor - free model , which adds a mask - head to CenterNet .",
        "aspect": "CenterNet",
        "sentiment": 0
    },
    {
        "text": "Under this family , we show that choosing the right out - of - the - box maskhead architecture alone allows us to surpass the state of the art on the partially supervised COCO benchmark ( 35.5 % mAP ) without need of hand designed priors or additional losses .",
        "aspect": "out - of - the - box maskhead architecture",
        "sentiment": 0
    },
    {
        "text": "Our best mask - head uses up to 100 layers following an hourglass pattern ( we find that deeper mask - heads generalize better despite being counter - intuitively more overparameterized than shallower ones ) .",
        "aspect": "mask - head",
        "sentiment": 1
    },
    {
        "text": "We call this model \" Deep Mask - heads Above CenterNet \" ( Deep - MAC ) and focus our most exhaustive experiments on Deep - MAC .",
        "aspect": "Deep - MAC",
        "sentiment": 2
    },
    {
        "text": "We call this model \" Deep Mask - heads Above CenterNet \" ( Deep - MAC ) and focus our most exhaustive experiments on Deep - MAC .",
        "aspect": "Deep - MAC",
        "sentiment": 2
    },
    {
        "text": "We call this model \" Deep Mask - heads Above CenterNet \" ( Deep - MAC ) and focus our most exhaustive experiments on Deep - MAC .",
        "aspect": "Deep Mask - heads Above CenterNet",
        "sentiment": 2
    },
    {
        "text": "We also study Mask R - CNN , which continues to be one of the most popular and enduring anchor - based segmentation models .",
        "aspect": "Mask R - CNN",
        "sentiment": 0
    },
    {
        "text": "We also study Mask R - CNN , which continues to be one of the most popular and enduring anchor - based segmentation models .",
        "aspect": "anchor - based segmentation models",
        "sentiment": 0
    },
    {
        "text": "Prior works have shown canonical implementations of Mask R - CNN ( with a class agnostic mask - head ) to perform poorly on the partially supervised segmentation task .",
        "aspect": "Mask R - CNN",
        "sentiment": 1
    },
    {
        "text": "Prior works have shown canonical implementations of Mask R - CNN ( with a class agnostic mask - head ) to perform poorly on the partially supervised segmentation task .",
        "aspect": "class agnostic mask - head",
        "sentiment": 0
    },
    {
        "text": "We show that a major culprit for this poor performance is the way that Mask R - CNN 's maskhead is trained with ( typically noisy ) proposals instead of groundtruth boxes .",
        "aspect": "Mask R - CNN",
        "sentiment": 0
    },
    {
        "text": "By modifying the training procedure slightly to crop using groundtruth boxes , we dramatically improve the performance of Mask R - CNN on unseen categories with the standard mask - head ( +7 % mask mAP ) .",
        "aspect": "Mask R - CNN",
        "sentiment": 3
    },
    {
        "text": "By modifying the training procedure slightly to crop using groundtruth boxes , we dramatically improve the performance of Mask R - CNN on unseen categories with the standard mask - head ( +7 % mask mAP ) .",
        "aspect": "mask - head",
        "sentiment": 1
    },
    {
        "text": "With this change , we evaluate alternative mask - head architectures and show that Mask R - CNN also exhibits the strong mask generalization effect with the strongest mask - head architectures similarly reaching performance on - par with the state of the art .",
        "aspect": "mask - head architectures",
        "sentiment": 0
    },
    {
        "text": "With this change , we evaluate alternative mask - head architectures and show that Mask R - CNN also exhibits the strong mask generalization effect with the strongest mask - head architectures similarly reaching performance on - par with the state of the art .",
        "aspect": "Mask R - CNN",
        "sentiment": 3
    },
    {
        "text": "With this change , we evaluate alternative mask - head architectures and show that Mask R - CNN also exhibits the strong mask generalization effect with the strongest mask - head architectures similarly reaching performance on - par with the state of the art .",
        "aspect": "mask - head architectures",
        "sentiment": 0
    },
    {
        "text": "Finally , we consider a segmentation only model ( which takes boxes as an input rather than requiring boxes to also be detected ) and show that the strong mask generalization effect can also be observed in this detection - free setting .",
        "aspect": "segmentation only model",
        "sentiment": 0
    },
    {
        "text": "The implication of this finding is that any future improvements on this task are likely to come from better detection ( which does not require strong generalization since we assume that all classes have box annotations ) .",
        "aspect": "strong generalization",
        "sentiment": 0
    },
    {
        "text": "To illustrate , we use a two - stage training procedure , employing Deep - MAC to label masks for unseen categories and training a stronger detector via a second phase in fully supervised mode on these pseudo - labels .",
        "aspect": "Deep - MAC",
        "sentiment": 0
    },
    {
        "text": "To illustrate , we use a two - stage training procedure , employing Deep - MAC to label masks for unseen categories and training a stronger detector via a second phase in fully supervised mode on these pseudo - labels .",
        "aspect": "detector",
        "sentiment": 0
    },
    {
        "text": "To illustrate , we use a two - stage training procedure , employing Deep - MAC to label masks for unseen categories and training a stronger detector via a second phase in fully supervised mode on these pseudo - labels .",
        "aspect": "two - stage training procedure",
        "sentiment": 0
    },
    {
        "text": "Going in the opposite direction , another benefit of this two stage procedure is that it allows us to train a lightweight mask - head based on pseudo - masks generated from a heavier mask - head .",
        "aspect": "mask - head",
        "sentiment": 0
    },
    {
        "text": "While the heavier mask - head has better mask generalization , we can exploit the fact that both heads perform equally well when fully supervised -thus in the second stage re - training phase , we obtain a model that performs well on the unseen categories while retaining the computational benefits of a lightweight mask - head .",
        "aspect": "mask - head",
        "sentiment": 0
    },
    {
        "text": "While the heavier mask - head has better mask generalization , we can exploit the fact that both heads perform equally well when fully supervised -thus in the second stage re - training phase , we obtain a model that performs well on the unseen categories while retaining the computational benefits of a lightweight mask - head .",
        "aspect": "mask generalization",
        "sentiment": 0
    },
    {
        "text": "While the heavier mask - head has better mask generalization , we can exploit the fact that both heads perform equally well when fully supervised -thus in the second stage re - training phase , we obtain a model that performs well on the unseen categories while retaining the computational benefits of a lightweight mask - head .",
        "aspect": "stage re - training phase",
        "sentiment": 0
    },
    {
        "text": "While the heavier mask - head has better mask generalization , we can exploit the fact that both heads perform equally well when fully supervised -thus in the second stage re - training phase , we obtain a model that performs well on the unseen categories while retaining the computational benefits of a lightweight mask - head .",
        "aspect": "mask - head",
        "sentiment": 0
    },
    {
        "text": "\u2022 We identify the strong mask generalization effect in partially supervised instance segmentation architectures and show that it is general , holding across underlying detectors like CenterNet ( Section 4 ) , Mask R - CNN or with no detector at all and across different backbones ( Section 5 ) .",
        "aspect": "mask generalization effect",
        "sentiment": 0
    },
    {
        "text": "\u2022 We identify the strong mask generalization effect in partially supervised instance segmentation architectures and show that it is general , holding across underlying detectors like CenterNet ( Section 4 ) , Mask R - CNN or with no detector at all and across different backbones ( Section 5 ) .",
        "aspect": "detectors",
        "sentiment": 0
    },
    {
        "text": "\u2022 We identify the strong mask generalization effect in partially supervised instance segmentation architectures and show that it is general , holding across underlying detectors like CenterNet ( Section 4 ) , Mask R - CNN or with no detector at all and across different backbones ( Section 5 ) .",
        "aspect": "CenterNet",
        "sentiment": 0
    },
    {
        "text": "\u2022 We identify the strong mask generalization effect in partially supervised instance segmentation architectures and show that it is general , holding across underlying detectors like CenterNet ( Section 4 ) , Mask R - CNN or with no detector at all and across different backbones ( Section 5 ) .",
        "aspect": "Mask R - CNN",
        "sentiment": 0
    },
    {
        "text": "\u2022 We identify the strong mask generalization effect in partially supervised instance segmentation architectures and show that it is general , holding across underlying detectors like CenterNet ( Section 4 ) , Mask R - CNN or with no detector at all and across different backbones ( Section 5 ) .",
        "aspect": "detector",
        "sentiment": 0
    },
    {
        "text": "\u2022 We identify a simple but critical tweak to training for Mask R - CNN which dramatically improves performance on unseen classes ( Table ) and unlocks strong mask generalization .",
        "aspect": "Mask R - CNN",
        "sentiment": 1
    },
    {
        "text": "\u2022 We identify a simple but critical tweak to training for Mask R - CNN which dramatically improves performance on unseen classes ( Table ) and unlocks strong mask generalization .",
        "aspect": "mask generalization",
        "sentiment": 1
    },
    {
        "text": "\u2022 We identify characteristics of mask - head architectures ( Section 6 ) that lead to strong mask generalization .",
        "aspect": "mask - head architectures",
        "sentiment": 1
    },
    {
        "text": "Among other things , we find that Hourglass architectures offer excellent performance .",
        "aspect": "Hourglass architectures",
        "sentiment": 1
    },
    {
        "text": "We use these findings to achieve state - of - the - art results on the COCO partial supervision task ( Table , Table ) with models based on CenterNet and Mask R - CNN .",
        "aspect": "CenterNet",
        "sentiment": 1
    },
    {
        "text": "We use these findings to achieve state - of - the - art results on the COCO partial supervision task ( Table , Table ) with models based on CenterNet and Mask R - CNN .",
        "aspect": "Mask R - CNN",
        "sentiment": 1
    },
    {
        "text": "\u2022 Finally , we argue that with respect to mask quality , we are close to saturating performance on COCO and show via experiments that it is much easier to achieve gains on this task simply by using a stronger detector ( that need not exhibit strong mask generalization ) ( Section 7 ) .",
        "aspect": "detector",
        "sentiment": 0
    },
    {
        "text": "\u2022 Finally , we argue that with respect to mask quality , we are close to saturating performance on COCO and show via experiments that it is much easier to achieve gains on this task simply by using a stronger detector ( that need not exhibit strong mask generalization ) ( Section 7 ) .",
        "aspect": "mask generalization",
        "sentiment": 0
    },
    {
        "text": "There has been a significant progress over the last decade in detection with successful convolutional models like Over - Feat , YOLO , Multibox , SSD , RetinaNet , R - CNN and Fast / Faster versions , EfficientDet , etc .",
        "aspect": "detection",
        "sentiment": 0
    },
    {
        "text": "There has been a significant progress over the last decade in detection with successful convolutional models like Over - Feat , YOLO , Multibox , SSD , RetinaNet , R - CNN and Fast / Faster versions , EfficientDet , etc .",
        "aspect": "convolutional models",
        "sentiment": 0
    },
    {
        "text": "There has been a significant progress over the last decade in detection with successful convolutional models like Over - Feat , YOLO , Multibox , SSD , RetinaNet , R - CNN and Fast / Faster versions , EfficientDet , etc .",
        "aspect": "Over - Feat",
        "sentiment": 0
    },
    {
        "text": "There has been a significant progress over the last decade in detection with successful convolutional models like Over - Feat , YOLO , Multibox , SSD , RetinaNet , R - CNN and Fast / Faster versions , EfficientDet , etc .",
        "aspect": "YOLO",
        "sentiment": 0
    },
    {
        "text": "There has been a significant progress over the last decade in detection with successful convolutional models like Over - Feat , YOLO , Multibox , SSD , RetinaNet , R - CNN and Fast / Faster versions , EfficientDet , etc .",
        "aspect": "Multibox",
        "sentiment": 0
    },
    {
        "text": "There has been a significant progress over the last decade in detection with successful convolutional models like Over - Feat , YOLO , Multibox , SSD , RetinaNet , R - CNN and Fast / Faster versions , EfficientDet , etc .",
        "aspect": "SSD",
        "sentiment": 0
    },
    {
        "text": "There has been a significant progress over the last decade in detection with successful convolutional models like Over - Feat , YOLO , Multibox , SSD , RetinaNet , R - CNN and Fast / Faster versions , EfficientDet , etc .",
        "aspect": "RetinaNet",
        "sentiment": 0
    },
    {
        "text": "There has been a significant progress over the last decade in detection with successful convolutional models like Over - Feat , YOLO , Multibox , SSD , RetinaNet , R - CNN and Fast / Faster versions , EfficientDet , etc .",
        "aspect": "R - CNN",
        "sentiment": 0
    },
    {
        "text": "There has been a significant progress over the last decade in detection with successful convolutional models like Over - Feat , YOLO , Multibox , SSD , RetinaNet , R - CNN and Fast / Faster versions , EfficientDet , etc .",
        "aspect": "EfficientDet",
        "sentiment": 0
    },
    {
        "text": "A major milestone in this literature was Mask R - CNN which influenced many SOTA approaches today ( e.g. , ) and by itself continues to serve as a strong baseline .",
        "aspect": "SOTA approaches",
        "sentiment": 0
    },
    {
        "text": "A major milestone in this literature was Mask R - CNN which influenced many SOTA approaches today ( e.g. , ) and by itself continues to serve as a strong baseline .",
        "aspect": "Mask R - CNN",
        "sentiment": 0
    },
    {
        "text": "Anchor - free methods .",
        "aspect": "Anchor - free methods",
        "sentiment": 0
    },
    {
        "text": "State - of - the - art methods today are predominantly built on anchor - based approaches which predict classification / box offsets relative to a collection of fixed boxes arranged in sliding window fashion ( called \" anchors \" ) .",
        "aspect": "anchor - based approaches",
        "sentiment": 0
    },
    {
        "text": "While effective , the performance of anchor - based methods often depend on manually - specified design decisions , e.g.",
        "aspect": "anchor - based methods",
        "sentiment": 0
    },
    {
        "text": "anchor layouts and target assignment heuristics , a complex space to navigate for practitioners .",
        "aspect": "anchor layouts",
        "sentiment": 0
    },
    {
        "text": "anchor layouts and target assignment heuristics , a complex space to navigate for practitioners .",
        "aspect": "target assignment heuristics",
        "sentiment": 0
    },
    {
        "text": "In recent years , however , this monopoly has been broken with the introduction of competitive \" anchor - free \" approaches .",
        "aspect": "anchor - free \" approaches",
        "sentiment": 0
    },
    {
        "text": "These newer anchorfree methods are simpler , more amenable to extension , offer competitive performance and consequently are beginning to be popular .",
        "aspect": "anchorfree methods",
        "sentiment": 0
    },
    {
        "text": "Our anchor - free model ( Section 3 ) in particular builds on the \" CenterNet \" architecture .",
        "aspect": "anchor - free model",
        "sentiment": 1
    },
    {
        "text": "Our anchor - free model ( Section 3 ) in particular builds on the \" CenterNet \" architecture .",
        "aspect": "CenterNet \" architecture",
        "sentiment": 1
    },
    {
        "text": "Due to the recency of competitive anchor - free methods there are fewer anchor - free instance segmentation approaches in literature .",
        "aspect": "anchor - free methods",
        "sentiment": 0
    },
    {
        "text": "Due to the recency of competitive anchor - free methods there are fewer anchor - free instance segmentation approaches in literature .",
        "aspect": "anchor - free instance segmentation",
        "sentiment": 0
    },
    {
        "text": "While the primary focus of our work is partial supervision , the fully supervised version of our model adds to this growing body of work , offering strong performance among anchor - free instance segmentation approaches .",
        "aspect": "fully supervised version",
        "sentiment": 0
    },
    {
        "text": "While the primary focus of our work is partial supervision , the fully supervised version of our model adds to this growing body of work , offering strong performance among anchor - free instance segmentation approaches .",
        "aspect": "anchor - free instance segmentation approaches",
        "sentiment": 0
    },
    {
        "text": "Box - only supervision for instance segmentation .",
        "aspect": "Box - only supervision",
        "sentiment": 0
    },
    {
        "text": "In one formulation of this problem ( which we might call strictly box - supervised ) we ask to learn an instance segmentation model given only box annotations and no masks .",
        "aspect": "instance segmentation model",
        "sentiment": 1
    },
    {
        "text": "Partial supervision for instance segmentation Instead of going to the extreme end of discarding all mask annotations , Hu et al . introduced the partial supervision formulation which allows for mask annotations from a small subset of classes to be used along with all box annotations .",
        "aspect": "partial supervision formulation",
        "sentiment": 0
    },
    {
        "text": "Later papers however revisited the approach of attaching a class - agnostic mask - head on top of a detector , in both cases introducing novel architectures and additional losses to significantly improve generalization to novel classes .",
        "aspect": "class - agnostic mask - head",
        "sentiment": 0
    },
    {
        "text": "Later papers however revisited the approach of attaching a class - agnostic mask - head on top of a detector , in both cases introducing novel architectures and additional losses to significantly improve generalization to novel classes .",
        "aspect": "detector",
        "sentiment": 0
    },
    {
        "text": "ShapeMask builds on RetinaNet , learning a low dimensional shape space from observed masks and uses projections to this space to guide mask estimation ; they also introduce a simple method to \" condition \" features cropped from the backbone on the instance that is being segmented .",
        "aspect": "ShapeMask",
        "sentiment": 0
    },
    {
        "text": "ShapeMask builds on RetinaNet , learning a low dimensional shape space from observed masks and uses projections to this space to guide mask estimation ; they also introduce a simple method to \" condition \" features cropped from the backbone on the instance that is being segmented .",
        "aspect": "RetinaNet",
        "sentiment": 0
    },
    {
        "text": "CP - Net , which is the current state of the art on this problem builds on FCOS , adding boundary prediction and attention - based aggregation in the mask branch .",
        "aspect": "CP - Net",
        "sentiment": 0
    },
    {
        "text": "CP - Net , which is the current state of the art on this problem builds on FCOS , adding boundary prediction and attention - based aggregation in the mask branch .",
        "aspect": "FCOS",
        "sentiment": 0
    },
    {
        "text": "CP - Net , which is the current state of the art on this problem builds on FCOS , adding boundary prediction and attention - based aggregation in the mask branch .",
        "aspect": "boundary prediction",
        "sentiment": 0
    },
    {
        "text": "CP - Net , which is the current state of the art on this problem builds on FCOS , adding boundary prediction and attention - based aggregation in the mask branch .",
        "aspect": "attention - based aggregation",
        "sentiment": 0
    },
    {
        "text": "We take a similar approach of using a class - agnostic mask - head , but while the ideas explored in these prior works are clearly beneficial , our objective is to demonstrate that mask - head architecture itself plays an underappreciated but significant role in generalization .",
        "aspect": "class - agnostic mask - head",
        "sentiment": 1
    },
    {
        "text": "We take a similar approach of using a class - agnostic mask - head , but while the ideas explored in these prior works are clearly beneficial , our objective is to demonstrate that mask - head architecture itself plays an underappreciated but significant role in generalization .",
        "aspect": "mask - head architecture",
        "sentiment": 1
    },
    {
        "text": "Notably , by exploiting out - of - the - box architectures with strong mask generalization properties , we show that with only minor tweaks to the training procedure ( Section 5.2 ) , even Mask R - CNN has state of the art performance in the partial supervision task .",
        "aspect": "out - of - the - box architectures",
        "sentiment": 1
    },
    {
        "text": "Notably , by exploiting out - of - the - box architectures with strong mask generalization properties , we show that with only minor tweaks to the training procedure ( Section 5.2 ) , even Mask R - CNN has state of the art performance in the partial supervision task .",
        "aspect": "Mask R - CNN",
        "sentiment": 1
    },
    {
        "text": "In this work , we have identified and studied the surprising extent to which the mask - head architecture impacts generalization to unseen categories .",
        "aspect": "mask - head architecture",
        "sentiment": 1
    },
    {
        "text": "Through extensive experiments , we demonstrated the generality of this effect across detection methodologies and backbone networks .",
        "aspect": "detection methodologies",
        "sentiment": 0
    },
    {
        "text": "Through extensive experiments , we demonstrated the generality of this effect across detection methodologies and backbone networks .",
        "aspect": "backbone networks",
        "sentiment": 0
    },
    {
        "text": "While we have taken initial steps in understanding strong mask generalization , how to better understand the inductive biases encoded within mask - head architectures and how to explain our results theoretically remain important directions .",
        "aspect": "mask - head architectures",
        "sentiment": 1
    },
    {
        "text": "The Deep Image Priors work similarly observed that Hourglass - style networks seem to automatically capture image level statistics in a natural way without being trained on data .",
        "aspect": "Hourglass - style networks",
        "sentiment": 0
    },
    {
        "text": "We use a pixel embedding layer with 16 channels and an instance embedding layer with 32 channels .",
        "aspect": "pixel embedding layer",
        "sentiment": 1
    },
    {
        "text": "We use a pixel embedding layer with 16 channels and an instance embedding layer with 32 channels .",
        "aspect": "instance embedding layer",
        "sentiment": 1
    },
    {
        "text": "For mask head architectures , we experiment with Hourglass networks and Residual Networks ( using both basic and bottleneck variants ) .",
        "aspect": "mask head architectures",
        "sentiment": 0
    },
    {
        "text": "For mask head architectures , we experiment with Hourglass networks and Residual Networks ( using both basic and bottleneck variants ) .",
        "aspect": "Hourglass networks",
        "sentiment": 0
    },
    {
        "text": "For mask head architectures , we experiment with Hourglass networks and Residual Networks ( using both basic and bottleneck variants ) .",
        "aspect": "Residual Networks",
        "sentiment": 0
    },
    {
        "text": "For mask head architectures , we experiment with Hourglass networks and Residual Networks ( using both basic and bottleneck variants ) .",
        "aspect": "basic and bottleneck variants",
        "sentiment": 0
    },
    {
        "text": "We train using Adam with synchronized batch normalization with batch size 128 for 50 K steps and apply a loss weight of 5 to the mask loss .",
        "aspect": "Adam",
        "sentiment": 1
    },
    {
        "text": "We train using Adam with synchronized batch normalization with batch size 128 for 50 K steps and apply a loss weight of 5 to the mask loss .",
        "aspect": "synchronized batch normalization",
        "sentiment": 1
    },
    {
        "text": "All detection related hyperparameters are unchanged from CenterNet defaults as described in including data augmentation .",
        "aspect": "CenterNet defaults",
        "sentiment": 0
    },
    {
        "text": "We do not use test time augmentation .",
        "aspect": "test time augmentation",
        "sentiment": 0
    },
    {
        "text": "Implementation details Deep - MAC is built on top of the publicly available CenterNet implementation in the Tensorflow Object Detection API .",
        "aspect": "Deep - MAC",
        "sentiment": 0
    },
    {
        "text": "Implementation details Deep - MAC is built on top of the publicly available CenterNet implementation in the Tensorflow Object Detection API .",
        "aspect": "CenterNet implementation",
        "sentiment": 0
    },
    {
        "text": "For our best results as well as most ablations , we use an Hourglass-104 backbone , though we show that our findings hold when using ResNet - FPN backbones as well .",
        "aspect": "Hourglass-104 backbone",
        "sentiment": 1
    },
    {
        "text": "For our best results as well as most ablations , we use an Hourglass-104 backbone , though we show that our findings hold when using ResNet - FPN backbones as well .",
        "aspect": "ResNet - FPN backbones",
        "sentiment": 1
    },
    {
        "text": "Since the Stacked Hourglass model produces predictions at the end of multiple Hourglass modules , we follow the common approach of applying prediction heads and losses at the end of each such module , using only predictions from the final stage at test time .",
        "aspect": "Stacked Hourglass model",
        "sentiment": 0
    },
    {
        "text": "Since the Stacked Hourglass model produces predictions at the end of multiple Hourglass modules , we follow the common approach of applying prediction heads and losses at the end of each such module , using only predictions from the final stage at test time .",
        "aspect": "Hourglass modules",
        "sentiment": 0
    },
    {
        "text": "MobileNetV2 : Inverted Residuals and Linear Bottlenecks",
        "aspect": "MobileNetV2",
        "sentiment": 0
    },
    {
        "text": "MobileNetV2 : Inverted Residuals and Linear Bottlenecks",
        "aspect": "Linear Bottlenecks",
        "sentiment": 0
    },
    {
        "text": "Neural networks have revolutionized many areas of machine intelligence , enabling superhuman accuracy for challenging image recognition tasks .",
        "aspect": "Neural networks",
        "sentiment": 0
    },
    {
        "text": "This paper introduces a new neural network architecture that is specifically tailored for mobile and resource constrained environments .",
        "aspect": "neural network architecture",
        "sentiment": 1
    },
    {
        "text": "Our main contribution is a novel layer module : the inverted residual with linear bottleneck .",
        "aspect": "layer module",
        "sentiment": 1
    },
    {
        "text": "Our main contribution is a novel layer module : the inverted residual with linear bottleneck .",
        "aspect": "inverted residual",
        "sentiment": 1
    },
    {
        "text": "Our main contribution is a novel layer module : the inverted residual with linear bottleneck .",
        "aspect": "linear bottleneck",
        "sentiment": 1
    },
    {
        "text": "This module takes as an input a low - dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution .",
        "aspect": "low - dimensional compressed representation",
        "sentiment": 0
    },
    {
        "text": "This module takes as an input a low - dimensional compressed representation which is first expanded to high dimension and filtered with a lightweight depthwise convolution .",
        "aspect": "lightweight depthwise convolution",
        "sentiment": 0
    },
    {
        "text": "Features are subsequently projected back to a low - dimensional representation with a linear convolution .",
        "aspect": "low - dimensional representation",
        "sentiment": 0
    },
    {
        "text": "Features are subsequently projected back to a low - dimensional representation with a linear convolution .",
        "aspect": "linear convolution",
        "sentiment": 0
    },
    {
        "text": "Furthermore , this convolutional module is particularly suitable for mobile designs , because it allows to significantly reduce the memory footprint needed during inference by never fully materializing large intermediate tensors .",
        "aspect": "convolutional module",
        "sentiment": 0
    },
    {
        "text": "This reduces the need for main memory access in many embedded hardware designs , that provide small amounts of very fast software controlled cache memory .",
        "aspect": "embedded hardware designs",
        "sentiment": 0
    },
    {
        "text": "Tuning deep neural architectures to strike an optimal balance between accuracy and performance has been an area of active research for the last several years .",
        "aspect": "deep neural architectures",
        "sentiment": 0
    },
    {
        "text": "Both manual architecture search and improvements in training algorithms , carried out by numerous teams has lead to dramatic improvements over early designs such as AlexNet , VGGNet , GoogLeNet .",
        "aspect": "training algorithms",
        "sentiment": 0
    },
    {
        "text": "Both manual architecture search and improvements in training algorithms , carried out by numerous teams has lead to dramatic improvements over early designs such as AlexNet , VGGNet , GoogLeNet .",
        "aspect": "AlexNet",
        "sentiment": 0
    },
    {
        "text": "Both manual architecture search and improvements in training algorithms , carried out by numerous teams has lead to dramatic improvements over early designs such as AlexNet , VGGNet , GoogLeNet .",
        "aspect": "VGGNet",
        "sentiment": 0
    },
    {
        "text": "Both manual architecture search and improvements in training algorithms , carried out by numerous teams has lead to dramatic improvements over early designs such as AlexNet , VGGNet , GoogLeNet .",
        "aspect": "GoogLeNet",
        "sentiment": 0
    },
    {
        "text": ", and ResNet .",
        "aspect": "ResNet",
        "sentiment": 0
    },
    {
        "text": "Recently there has been lots of progress in algorithmic architecture exploration included hyperparameter optimization as well as various methods of network pruning and connectivity learning .",
        "aspect": "network pruning",
        "sentiment": 0
    },
    {
        "text": "Recently there has been lots of progress in algorithmic architecture exploration included hyperparameter optimization as well as various methods of network pruning and connectivity learning .",
        "aspect": "connectivity learning",
        "sentiment": 0
    },
    {
        "text": "A substantial amount of work has also been dedicated to changing the connectivity structure of the internal convolutional blocks such as in ShuffleNet or introducing sparsity and others .",
        "aspect": "ShuffleNet",
        "sentiment": 0
    },
    {
        "text": "Recently , , opened up a new direction of bringing optimization methods including genetic algorithms and reinforcement learning to architectural search .",
        "aspect": "optimization methods",
        "sentiment": 0
    },
    {
        "text": "Recently , , opened up a new direction of bringing optimization methods including genetic algorithms and reinforcement learning to architectural search .",
        "aspect": "genetic algorithms",
        "sentiment": 0
    },
    {
        "text": "Recently , , opened up a new direction of bringing optimization methods including genetic algorithms and reinforcement learning to architectural search .",
        "aspect": "reinforcement learning",
        "sentiment": 0
    },
    {
        "text": "In this paper , we pursue the goal of developing better intuition about how neural networks operate and use that to guide the simplest possible network design .",
        "aspect": "neural networks",
        "sentiment": 1
    },
    {
        "text": "Our network design is based on MobileNetV1 .",
        "aspect": "MobileNetV1",
        "sentiment": 1
    },
    {
        "text": "The new result reported in this paper is that the shortcut connecting bottleneck perform better than shortcuts connecting the expanded layers ( see Figure for comparison ) .",
        "aspect": "shortcut connecting bottleneck",
        "sentiment": 0
    },
    {
        "text": "Importance of linear bottlenecks .",
        "aspect": "linear bottlenecks",
        "sentiment": 0
    },
    {
        "text": "The linear bottleneck models are strictly less powerful than models with non - linearities , because the activations can always operate in linear regime with appropriate changes to biases and scaling .",
        "aspect": "linear bottleneck models",
        "sentiment": 0
    },
    {
        "text": "However our experiments shown in Figure indicate that linear bottlenecks improve performance , providing support that non - linearity destroys information in low - dimensional space .",
        "aspect": "linear bottlenecks",
        "sentiment": 1
    },
    {
        "text": "We described a very simple network architecture that allowed us to build a family of highly efficient mobile models .",
        "aspect": "network architecture",
        "sentiment": 1
    },
    {
        "text": "We described a very simple network architecture that allowed us to build a family of highly efficient mobile models .",
        "aspect": "mobile models",
        "sentiment": 0
    },
    {
        "text": "Our basic building unit , has several properties that make it particularly suitable for mobile applications .",
        "aspect": "building unit",
        "sentiment": 0
    },
    {
        "text": "It allows very memory - efficient inference and relies utilize standard operations present in all neural frameworks .",
        "aspect": "neural frameworks",
        "sentiment": 0
    },
    {
        "text": "For object detection task , our network outperforms state - of - art realtime detectors on COCO dataset both in terms of accuracy and model complexity .",
        "aspect": "realtime detectors",
        "sentiment": 0
    },
    {
        "text": "Notably , our architecture combined with the SSDLite detection module is 20\u00d7 less computation and 10\u00d7 less parameters than YOLOv2 .",
        "aspect": "SSDLite detection module",
        "sentiment": 1
    },
    {
        "text": "Notably , our architecture combined with the SSDLite detection module is 20\u00d7 less computation and 10\u00d7 less parameters than YOLOv2 .",
        "aspect": "YOLOv2",
        "sentiment": 3
    },
    {
        "text": "On the theoretical side : the proposed convolutional block has a unique property that allows to separate the network expressiveness ( encoded by expansion layers ) from its capacity ( encoded by bottleneck inputs ) .",
        "aspect": "convolutional block",
        "sentiment": 1
    },
    {
        "text": "VATT : Transformers for Multimodal Self - Supervised Learning from Raw Video , Audio and Text",
        "aspect": "VATT",
        "sentiment": 0
    },
    {
        "text": "VATT : Transformers for Multimodal Self - Supervised Learning from Raw Video , Audio and Text",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "We present a framework for learning multimodal representations from unlabeled data using convolution - free Transformer architectures .",
        "aspect": "convolution - free Transformer architectures",
        "sentiment": 1
    },
    {
        "text": "Specifically , our Video - Audio - Text Transformer ( VATT ) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks .",
        "aspect": "Video - Audio - Text Transformer",
        "sentiment": 2
    },
    {
        "text": "Specifically , our Video - Audio - Text Transformer ( VATT ) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks .",
        "aspect": "VATT",
        "sentiment": 2
    },
    {
        "text": "Specifically , our Video - Audio - Text Transformer ( VATT ) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks .",
        "aspect": "multimodal representations",
        "sentiment": 0
    },
    {
        "text": "We train VATT endto - end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition , audio event classification , image classification , and text - to - video retrieval .",
        "aspect": "VATT",
        "sentiment": 2
    },
    {
        "text": "Furthermore , we study a modality - agnostic , single - backbone Transformer by sharing weights among the three modalities .",
        "aspect": "modality - agnostic , single - backbone Transformer",
        "sentiment": 1
    },
    {
        "text": "We show that the convolution - free VATT outperforms state - of - the - art ConvNet - based architectures in the downstream tasks .",
        "aspect": "convolution - free VATT",
        "sentiment": 0
    },
    {
        "text": "We show that the convolution - free VATT outperforms state - of - the - art ConvNet - based architectures in the downstream tasks .",
        "aspect": "ConvNet - based architectures",
        "sentiment": 3
    },
    {
        "text": "Especially , VATT 's vision Transformer achieves the top-1 accuracy of 82.1 % on Kinetics-400 , 83.6 % on Kinetics-600 , and 41.1 % on Moments in Time , new records while avoiding supervised pre - training .",
        "aspect": "VATT",
        "sentiment": 0
    },
    {
        "text": "Especially , VATT 's vision Transformer achieves the top-1 accuracy of 82.1 % on Kinetics-400 , 83.6 % on Kinetics-600 , and 41.1 % on Moments in Time , new records while avoiding supervised pre - training .",
        "aspect": "vision Transformer",
        "sentiment": 0
    },
    {
        "text": "Especially , VATT 's vision Transformer achieves the top-1 accuracy of 82.1 % on Kinetics-400 , 83.6 % on Kinetics-600 , and 41.1 % on Moments in Time , new records while avoiding supervised pre - training .",
        "aspect": "supervised pre - training",
        "sentiment": 0
    },
    {
        "text": "VATT 's audio Transformer also sets a new record on waveform - based audio event recognition by achieving the mAP of 39.4 % on Au - dioSet without any supervised pre - training .",
        "aspect": "VATT 's audio Transformer",
        "sentiment": 2
    },
    {
        "text": "VATT 's audio Transformer also sets a new record on waveform - based audio event recognition by achieving the mAP of 39.4 % on Au - dioSet without any supervised pre - training .",
        "aspect": "supervised pre - training",
        "sentiment": 0
    },
    {
        "text": "Convolutional neural networks ( CNNs ) have triumphed over various computer vision tasks .",
        "aspect": "Convolutional neural networks",
        "sentiment": 0
    },
    {
        "text": "Convolutional neural networks ( CNNs ) have triumphed over various computer vision tasks .",
        "aspect": "CNNs",
        "sentiment": 0
    },
    {
        "text": "In the meantime , however , we witness in the natural language processing ( NLP ) community a paradigm shift from the models with strong inductive bias , such as recurrent neu- * Work done during an internship at Google .",
        "aspect": "recurrent neu-",
        "sentiment": 0
    },
    {
        "text": "ral networks and CNNs , to more general architectures constructed upon self - attention .",
        "aspect": "ral networks",
        "sentiment": 0
    },
    {
        "text": "ral networks and CNNs , to more general architectures constructed upon self - attention .",
        "aspect": "CNNs",
        "sentiment": 0
    },
    {
        "text": "ral networks and CNNs , to more general architectures constructed upon self - attention .",
        "aspect": "self - attention",
        "sentiment": 0
    },
    {
        "text": "Particularly , Transformers become the de facto model architecture for NLP tasks .",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "Particularly , Transformers become the de facto model architecture for NLP tasks .",
        "aspect": "model architecture",
        "sentiment": 0
    },
    {
        "text": "Pre - training a Transformer on large text corpora followed by fine - tuning gives rise to state - of - the - art results for different downstream tasks .",
        "aspect": "Transformer",
        "sentiment": 0
    },
    {
        "text": "Pre - training a Transformer on large text corpora followed by fine - tuning gives rise to state - of - the - art results for different downstream tasks .",
        "aspect": "fine - tuning",
        "sentiment": 0
    },
    {
        "text": "In view of the success of the attention mechanism in NLP , there has been a rich line of works exploring its potential in computer vision .",
        "aspect": "attention mechanism",
        "sentiment": 0
    },
    {
        "text": "Early work studied hybrid models consisting of both convolutions and attention modules .",
        "aspect": "hybrid models",
        "sentiment": 0
    },
    {
        "text": "Early work studied hybrid models consisting of both convolutions and attention modules .",
        "aspect": "convolutions",
        "sentiment": 0
    },
    {
        "text": "Early work studied hybrid models consisting of both convolutions and attention modules .",
        "aspect": "attention modules",
        "sentiment": 0
    },
    {
        "text": "Recent studies showed that convolution - free , specially designed all - attention models can match CNNs ' performance on image recognition tasks .",
        "aspect": "convolution - free",
        "sentiment": 0
    },
    {
        "text": "Recent studies showed that convolution - free , specially designed all - attention models can match CNNs ' performance on image recognition tasks .",
        "aspect": "all - attention models",
        "sentiment": 0
    },
    {
        "text": "Recent studies showed that convolution - free , specially designed all - attention models can match CNNs ' performance on image recognition tasks .",
        "aspect": "CNNs",
        "sentiment": 0
    },
    {
        "text": "Most recently , Dosovitskiy et al . achieved impressive performance on several image recognition tasks , including Ima - geNet , using a pre - trained Transformer with minimal architecture changes .",
        "aspect": "Transformer",
        "sentiment": 0
    },
    {
        "text": "As a result , the supervised training strategy could produce biased systems that require even more labeled data to correct their biases .",
        "aspect": "supervised training strategy",
        "sentiment": 0
    },
    {
        "text": "As a result , the supervised training strategy could produce biased systems that require even more labeled data to correct their biases .",
        "aspect": "biased systems",
        "sentiment": 0
    },
    {
        "text": "Second , this strategy fundamentally limits the application scope of Transformers in computer vision because it is costly and extremely time - consuming to collect enough labeled images or videos for training the millions of parameters , choosing hyper - parameters , and validating their expected generalization .",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "Hence , this work poses another pressing question about the Transformers that take raw signals as input .",
        "aspect": "Transformers",
        "sentiment": 1
    },
    {
        "text": "To answer this question , we draw insights from NLP . BERT and the GPT family use masked language modeling as their pre - training tasks .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "To answer this question , we draw insights from NLP . BERT and the GPT family use masked language modeling as their pre - training tasks .",
        "aspect": "GPT family",
        "sentiment": 0
    },
    {
        "text": "To answer this question , we draw insights from NLP . BERT and the GPT family use masked language modeling as their pre - training tasks .",
        "aspect": "masked language modeling",
        "sentiment": 0
    },
    {
        "text": "Natural languages are organic supervision for Transformers .",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "To this end , we study self - supervised , multimodal pretraining of three Transformers , which take as input the raw RGB frames of internet videos , audio waveforms , and text transcripts of the speech audio , respectively .",
        "aspect": "Transformers",
        "sentiment": 1
    },
    {
        "text": "We call the video , audio , text Transformers VATT . Figure illustrates the architecture .",
        "aspect": "VATT",
        "sentiment": 2
    },
    {
        "text": "VATT borrows the exact architecture from BERT and ViT except the layer of tokenization and linear projection reserved for each modality separately .",
        "aspect": "VATT",
        "sentiment": 2
    },
    {
        "text": "VATT borrows the exact architecture from BERT and ViT except the layer of tokenization and linear projection reserved for each modality separately .",
        "aspect": "BERT",
        "sentiment": 1
    },
    {
        "text": "VATT borrows the exact architecture from BERT and ViT except the layer of tokenization and linear projection reserved for each modality separately .",
        "aspect": "tokenization and linear projection",
        "sentiment": 0
    },
    {
        "text": "This design follows the same spirit as ViT that we make the minimal changes to the architecture so that the learned model can transfer its weights to various frameworks and tasks .",
        "aspect": "ViT",
        "sentiment": 1
    },
    {
        "text": "Furthermore , the self - supervised , multimodal learning strategy resonates the spirit of BERT and GPT that the pre - training requires minimal human curated labels .",
        "aspect": "self - supervised , multimodal learning strategy",
        "sentiment": 1
    },
    {
        "text": "Furthermore , the self - supervised , multimodal learning strategy resonates the spirit of BERT and GPT that the pre - training requires minimal human curated labels .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Furthermore , the self - supervised , multimodal learning strategy resonates the spirit of BERT and GPT that the pre - training requires minimal human curated labels .",
        "aspect": "GPT",
        "sentiment": 0
    },
    {
        "text": "We evaluate the pre - trained Transformers on a variety of downstream tasks : image classification , video action recognition , audio event classification , and zero - shot video retrieval .",
        "aspect": "pre - trained Transformers",
        "sentiment": 0
    },
    {
        "text": "Fine - tuning the vision - modality Transformer on ImageNet obtains the top-1 accuracy of 78.7 % , which is comparable to 79.9 % achieved by ViT.",
        "aspect": "vision - modality Transformer",
        "sentiment": 0
    },
    {
        "text": "Fine - tuning the vision - modality Transformer on ImageNet obtains the top-1 accuracy of 78.7 % , which is comparable to 79.9 % achieved by ViT.",
        "aspect": "ViT.",
        "sentiment": 0
    },
    {
        "text": "This result is especially appealing considering the domain gap between videos and images , and that ViT is pre - trained using a large - scale , human - curated image dataset .",
        "aspect": "ViT",
        "sentiment": 0
    },
    {
        "text": "Furthermore , we set new records on Kinetics-400 , Kinetics-600 , Moments in Time , and AudioSet without supervised pre - training .",
        "aspect": "supervised pre - training",
        "sentiment": 0
    },
    {
        "text": "Our VATT results , along with others reported for NLP tasks , image recognition , semantic segmentation , point cloud classification , and action recoginition , demonstrate that Transformer is a capable general - purpose architecture for different types of data .",
        "aspect": "VATT",
        "sentiment": 2
    },
    {
        "text": "Our VATT results , along with others reported for NLP tasks , image recognition , semantic segmentation , point cloud classification , and action recoginition , demonstrate that Transformer is a capable general - purpose architecture for different types of data .",
        "aspect": "Transformer",
        "sentiment": 0
    },
    {
        "text": "Our VATT results , along with others reported for NLP tasks , image recognition , semantic segmentation , point cloud classification , and action recoginition , demonstrate that Transformer is a capable general - purpose architecture for different types of data .",
        "aspect": "general - purpose architecture",
        "sentiment": 0
    },
    {
        "text": "To move one step forward , we challenge the Transformers in VATT by a seemingly too strong constraint : sharing weights among the video , audio , and text modalities .",
        "aspect": "VATT",
        "sentiment": 0
    },
    {
        "text": "The idea is to test whether there exists a single , general - purpose model for all the modalities -of course , they still have their own layers of tokenization and linear projection .",
        "aspect": "tokenization",
        "sentiment": 0
    },
    {
        "text": "The idea is to test whether there exists a single , general - purpose model for all the modalities -of course , they still have their own layers of tokenization and linear projection .",
        "aspect": "linear projection",
        "sentiment": 0
    },
    {
        "text": "This modality - agnostic Transformer performs comparably with the modality - specific ones .",
        "aspect": "modality - agnostic Transformer",
        "sentiment": 0
    },
    {
        "text": "Finally , another contribution of this work is DropToken , a simple yet effective technique to reduce the training complexity for Transformers with a minor reduction of the end model 's performance .",
        "aspect": "DropToken",
        "sentiment": 1
    },
    {
        "text": "DropToken randomly drops a portion of the video and audio tokens from each input sequence during training , leveraging high - resolution multimodal inputs .",
        "aspect": "DropToken",
        "sentiment": 0
    },
    {
        "text": "In this paper , we present a self - supervised multimodal representation learning framework based on Transformer architecture .",
        "aspect": "self - supervised multimodal representation learning framework",
        "sentiment": 1
    },
    {
        "text": "In this paper , we present a self - supervised multimodal representation learning framework based on Transformer architecture .",
        "aspect": "Transformer architecture",
        "sentiment": 1
    },
    {
        "text": "With pure attention - based model on multimodal video inputs , our study suggests that large - scale selfsupervised pre - training is a promising direction to lighten the data burden for Transformer architectures and enables the Transformers to triumph Convolutional Neural Network ( CNN ) on various downstream tasks .",
        "aspect": "attention - based model",
        "sentiment": 1
    },
    {
        "text": "With pure attention - based model on multimodal video inputs , our study suggests that large - scale selfsupervised pre - training is a promising direction to lighten the data burden for Transformer architectures and enables the Transformers to triumph Convolutional Neural Network ( CNN ) on various downstream tasks .",
        "aspect": "Transformer architectures",
        "sentiment": 1
    },
    {
        "text": "With pure attention - based model on multimodal video inputs , our study suggests that large - scale selfsupervised pre - training is a promising direction to lighten the data burden for Transformer architectures and enables the Transformers to triumph Convolutional Neural Network ( CNN ) on various downstream tasks .",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "With pure attention - based model on multimodal video inputs , our study suggests that large - scale selfsupervised pre - training is a promising direction to lighten the data burden for Transformer architectures and enables the Transformers to triumph Convolutional Neural Network ( CNN ) on various downstream tasks .",
        "aspect": "Convolutional Neural Network",
        "sentiment": 0
    },
    {
        "text": "With pure attention - based model on multimodal video inputs , our study suggests that large - scale selfsupervised pre - training is a promising direction to lighten the data burden for Transformer architectures and enables the Transformers to triumph Convolutional Neural Network ( CNN ) on various downstream tasks .",
        "aspect": "CNN",
        "sentiment": 0
    },
    {
        "text": "A simple yet effective technique , DropToken , is proposed to solve the quadratic training complexity with respect to the input length of attention based model , making it more approachable for vision and raw audio processing .",
        "aspect": "DropToken",
        "sentiment": 2
    },
    {
        "text": "A simple yet effective technique , DropToken , is proposed to solve the quadratic training complexity with respect to the input length of attention based model , making it more approachable for vision and raw audio processing .",
        "aspect": "attention based model",
        "sentiment": 1
    },
    {
        "text": "Achieving state - of - the - art in video action recognition and audio event classification , and also competitive performance on image classification and video retrieval tasks also showed the great generalizability and transferrability of our learned representations by selfsupervised learning across different modalities .",
        "aspect": "selfsupervised learning",
        "sentiment": 0
    },
    {
        "text": "We hope our work make one step further in leveraging the strong expressiveness of the Transformer - based model for multimodal understanding and open the door for developing the grand multimodal model .",
        "aspect": "Transformer - based model",
        "sentiment": 0
    },
    {
        "text": "We hope our work make one step further in leveraging the strong expressiveness of the Transformer - based model for multimodal understanding and open the door for developing the grand multimodal model .",
        "aspect": "multimodal model",
        "sentiment": 0
    },
    {
        "text": "In the future , we plan to study data augmentation techniques to train the Transformer and how to properly regularize the model , especially the modality - agnostic backbone , for diverse multimodal tasks .",
        "aspect": "data augmentation techniques",
        "sentiment": 0
    },
    {
        "text": "In the future , we plan to study data augmentation techniques to train the Transformer and how to properly regularize the model , especially the modality - agnostic backbone , for diverse multimodal tasks .",
        "aspect": "Transformer",
        "sentiment": 0
    },
    {
        "text": "In the future , we plan to study data augmentation techniques to train the Transformer and how to properly regularize the model , especially the modality - agnostic backbone , for diverse multimodal tasks .",
        "aspect": "modality - agnostic backbone",
        "sentiment": 0
    },
    {
        "text": "Semi - Supervised Sequence Modeling with Cross - View Training",
        "aspect": "Cross - View Training",
        "sentiment": 0
    },
    {
        "text": "Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models , mainly because they can take advantage of large amounts of unlabeled text .",
        "aspect": "Unsupervised representation learning algorithms",
        "sentiment": 0
    },
    {
        "text": "Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models , mainly because they can take advantage of large amounts of unlabeled text .",
        "aspect": "word2vec",
        "sentiment": 0
    },
    {
        "text": "Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models , mainly because they can take advantage of large amounts of unlabeled text .",
        "aspect": "ELMo",
        "sentiment": 0
    },
    {
        "text": "Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models , mainly because they can take advantage of large amounts of unlabeled text .",
        "aspect": "supervised NLP models",
        "sentiment": 0
    },
    {
        "text": "We therefore propose Cross - View Training ( CVT ) , a semi - supervised learning algorithm that improves the representations of a Bi - LSTM sentence encoder using a mix of labeled and unlabeled data .",
        "aspect": "Cross - View Training",
        "sentiment": 2
    },
    {
        "text": "We therefore propose Cross - View Training ( CVT ) , a semi - supervised learning algorithm that improves the representations of a Bi - LSTM sentence encoder using a mix of labeled and unlabeled data .",
        "aspect": "CVT )",
        "sentiment": 2
    },
    {
        "text": "We therefore propose Cross - View Training ( CVT ) , a semi - supervised learning algorithm that improves the representations of a Bi - LSTM sentence encoder using a mix of labeled and unlabeled data .",
        "aspect": "semi - supervised learning algorithm",
        "sentiment": 0
    },
    {
        "text": "We therefore propose Cross - View Training ( CVT ) , a semi - supervised learning algorithm that improves the representations of a Bi - LSTM sentence encoder using a mix of labeled and unlabeled data .",
        "aspect": "Bi - LSTM sentence encoder",
        "sentiment": 1
    },
    {
        "text": "On labeled examples , standard supervised learning is used .",
        "aspect": "supervised learning",
        "sentiment": 1
    },
    {
        "text": "On unlabeled examples , CVT teaches auxiliary prediction modules that see restricted views of the input ( e.g. , only part of a sentence ) to match the predictions of the full model seeing the whole input .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "On unlabeled examples , CVT teaches auxiliary prediction modules that see restricted views of the input ( e.g. , only part of a sentence ) to match the predictions of the full model seeing the whole input .",
        "aspect": "auxiliary prediction modules",
        "sentiment": 0
    },
    {
        "text": "Moreover , we show that CVT is particularly effective when combined with multitask learning .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "Moreover , we show that CVT is particularly effective when combined with multitask learning .",
        "aspect": "multitask learning",
        "sentiment": 0
    },
    {
        "text": "We evaluate CVT on five sequence tagging tasks , machine translation , and dependency parsing , achieving state - of - the - art results . 1",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "Deep learning models work best when trained on large amounts of labeled data .",
        "aspect": "Deep learning models",
        "sentiment": 0
    },
    {
        "text": "However , acquiring labels is costly , motivating the need for effective semi - supervised learning techniques that leverage unlabeled examples .",
        "aspect": "semi - supervised learning techniques",
        "sentiment": 0
    },
    {
        "text": "A widely successful semi - supervised learning strategy for neural NLP is pre - training word vectors .",
        "aspect": "semi - supervised learning strategy",
        "sentiment": 0
    },
    {
        "text": "More recent work trains a Bi - LSTM sentence encoder to do language modeling and then incorporates its context - sensitive representations into supervised models .",
        "aspect": "Bi - LSTM sentence encoder",
        "sentiment": 0
    },
    {
        "text": "More recent work trains a Bi - LSTM sentence encoder to do language modeling and then incorporates its context - sensitive representations into supervised models .",
        "aspect": "context - sensitive representations",
        "sentiment": 0
    },
    {
        "text": "More recent work trains a Bi - LSTM sentence encoder to do language modeling and then incorporates its context - sensitive representations into supervised models .",
        "aspect": "supervised models",
        "sentiment": 0
    },
    {
        "text": "Such pre - training methods perform unsupervised representation learning on a large corpus of unlabeled data followed by supervised training .",
        "aspect": "pre - training methods",
        "sentiment": 0
    },
    {
        "text": "Such pre - training methods perform unsupervised representation learning on a large corpus of unlabeled data followed by supervised training .",
        "aspect": "unsupervised representation learning",
        "sentiment": 0
    },
    {
        "text": "Such pre - training methods perform unsupervised representation learning on a large corpus of unlabeled data followed by supervised training .",
        "aspect": "supervised training",
        "sentiment": 0
    },
    {
        "text": "A key disadvantage of pre - training is that the first representation learning phase does not take advantage of labeled data -the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .",
        "aspect": "pre - training",
        "sentiment": 0
    },
    {
        "text": "A key disadvantage of pre - training is that the first representation learning phase does not take advantage of labeled data -the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task .",
        "aspect": "representation learning phase",
        "sentiment": 0
    },
    {
        "text": "Older semi - supervised learning algorithms like self - training do not suffer from this problem because they continually learn about a task on a mix of labeled and unlabeled data .",
        "aspect": "semi - supervised learning algorithms",
        "sentiment": 0
    },
    {
        "text": "Older semi - supervised learning algorithms like self - training do not suffer from this problem because they continually learn about a task on a mix of labeled and unlabeled data .",
        "aspect": "self - training",
        "sentiment": 0
    },
    {
        "text": "Selftraining has historically been effective for NLP , but is less commonly used with neural models .",
        "aspect": "Selftraining",
        "sentiment": 0
    },
    {
        "text": "Selftraining has historically been effective for NLP , but is less commonly used with neural models .",
        "aspect": "neural models",
        "sentiment": 0
    },
    {
        "text": "This paper presents Cross - View Training ( CVT ) , a new self - training algorithm that works well for neural sequence models .",
        "aspect": "Cross - View Training",
        "sentiment": 2
    },
    {
        "text": "This paper presents Cross - View Training ( CVT ) , a new self - training algorithm that works well for neural sequence models .",
        "aspect": "CVT )",
        "sentiment": 2
    },
    {
        "text": "This paper presents Cross - View Training ( CVT ) , a new self - training algorithm that works well for neural sequence models .",
        "aspect": "self - training algorithm",
        "sentiment": 0
    },
    {
        "text": "This paper presents Cross - View Training ( CVT ) , a new self - training algorithm that works well for neural sequence models .",
        "aspect": "neural sequence models",
        "sentiment": 1
    },
    {
        "text": "As a solution , we take inspiration from multiview learning and train the model to produce consistent predictions across different views of the input .",
        "aspect": "multiview learning",
        "sentiment": 1
    },
    {
        "text": "Instead of only training the full model as a student , CVT adds auxiliary prediction modules -neural networks that transform vector representations into predictions -to the model and also trains them as students .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "Instead of only training the full model as a student , CVT adds auxiliary prediction modules -neural networks that transform vector representations into predictions -to the model and also trains them as students .",
        "aspect": "auxiliary prediction modules",
        "sentiment": 0
    },
    {
        "text": "The input to each student prediction module is a subset of the model 's intermediate rep - arXiv:1809.08370v1",
        "aspect": "student prediction module",
        "sentiment": 0
    },
    {
        "text": "For example , one auxiliary prediction module for sequence tagging is attached to only the \" forward \" LSTM in the model 's first Bi - LSTM layer , so it makes predictions without seeing any tokens to the right of the current one .",
        "aspect": "auxiliary prediction module",
        "sentiment": 0
    },
    {
        "text": "For example , one auxiliary prediction module for sequence tagging is attached to only the \" forward \" LSTM in the model 's first Bi - LSTM layer , so it makes predictions without seeing any tokens to the right of the current one .",
        "aspect": "forward \" LSTM",
        "sentiment": 0
    },
    {
        "text": "For example , one auxiliary prediction module for sequence tagging is attached to only the \" forward \" LSTM in the model 's first Bi - LSTM layer , so it makes predictions without seeing any tokens to the right of the current one .",
        "aspect": "Bi - LSTM layer",
        "sentiment": 0
    },
    {
        "text": "CVT works by improving the model 's representation learning .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "CVT works by improving the model 's representation learning .",
        "aspect": "model 's representation learning",
        "sentiment": 0
    },
    {
        "text": "The auxiliary prediction modules can learn from the full model 's predictions because the full model has a better , unrestricted view of the input .",
        "aspect": "auxiliary prediction modules",
        "sentiment": 0
    },
    {
        "text": "As the auxiliary modules learn to make accurate predictions despite their restricted views of the input , they improve the quality of the representations they are built on top of .",
        "aspect": "auxiliary modules",
        "sentiment": 0
    },
    {
        "text": "In short , our method combines the idea of representation learning on unlabeled data with classic self - training .",
        "aspect": "representation learning",
        "sentiment": 1
    },
    {
        "text": "In short , our method combines the idea of representation learning on unlabeled data with classic self - training .",
        "aspect": "self - training",
        "sentiment": 1
    },
    {
        "text": "CVT can be applied to a variety of tasks and neural architectures , but we focus on sequence modeling tasks where the prediction modules are attached to a shared Bi - LSTM encoder .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "CVT can be applied to a variety of tasks and neural architectures , but we focus on sequence modeling tasks where the prediction modules are attached to a shared Bi - LSTM encoder .",
        "aspect": "neural architectures",
        "sentiment": 0
    },
    {
        "text": "CVT can be applied to a variety of tasks and neural architectures , but we focus on sequence modeling tasks where the prediction modules are attached to a shared Bi - LSTM encoder .",
        "aspect": "Bi - LSTM",
        "sentiment": 1
    },
    {
        "text": "We propose auxiliary prediction modules that work well for sequence taggers , graph - based dependency parsers , and sequence - to - sequence models .",
        "aspect": "auxiliary prediction modules",
        "sentiment": 2
    },
    {
        "text": "We propose auxiliary prediction modules that work well for sequence taggers , graph - based dependency parsers , and sequence - to - sequence models .",
        "aspect": "sequence taggers",
        "sentiment": 1
    },
    {
        "text": "We propose auxiliary prediction modules that work well for sequence taggers , graph - based dependency parsers , and sequence - to - sequence models .",
        "aspect": "graph - based dependency parsers",
        "sentiment": 1
    },
    {
        "text": "We propose auxiliary prediction modules that work well for sequence taggers , graph - based dependency parsers , and sequence - to - sequence models .",
        "aspect": "sequence - to - sequence models",
        "sentiment": 1
    },
    {
        "text": "CVT improves over previously published results on all these tasks .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "Furthermore , CVT can easily and effectively be combined with multi - task learning : we just add additional prediction modules for the different tasks on top of the shared Bi - LSTM encoder .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "Furthermore , CVT can easily and effectively be combined with multi - task learning : we just add additional prediction modules for the different tasks on top of the shared Bi - LSTM encoder .",
        "aspect": "multi - task learning",
        "sentiment": 0
    },
    {
        "text": "Furthermore , CVT can easily and effectively be combined with multi - task learning : we just add additional prediction modules for the different tasks on top of the shared Bi - LSTM encoder .",
        "aspect": "prediction modules",
        "sentiment": 1
    },
    {
        "text": "Furthermore , CVT can easily and effectively be combined with multi - task learning : we just add additional prediction modules for the different tasks on top of the shared Bi - LSTM encoder .",
        "aspect": "Bi - LSTM encoder",
        "sentiment": 1
    },
    {
        "text": "Training a unified model to jointly perform all of the tasks except machine translation improves results ( outperforming a multi - task ELMo model ) while decreasing the total training time .",
        "aspect": "unified model",
        "sentiment": 0
    },
    {
        "text": "Training a unified model to jointly perform all of the tasks except machine translation improves results ( outperforming a multi - task ELMo model ) while decreasing the total training time .",
        "aspect": "multi - task ELMo model",
        "sentiment": 3
    },
    {
        "text": "CVT on its own outperforms or is comparable to the best previously published results on all tasks .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "Figure shows an example win for CVT over supervised learning . .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "Figure shows an example win for CVT over supervised learning . .",
        "aspect": "supervised learning",
        "sentiment": 3
    },
    {
        "text": "Of the prior results listed in Table , only TagLM and ELMo are semi - supervised .",
        "aspect": "TagLM",
        "sentiment": 0
    },
    {
        "text": "Of the prior results listed in Table , only TagLM and ELMo are semi - supervised .",
        "aspect": "ELMo",
        "sentiment": 0
    },
    {
        "text": "These methods first train an enormous language model on unlabeled data and incorporate the representations produced by the language model into a supervised classifier .",
        "aspect": "language model",
        "sentiment": 0
    },
    {
        "text": "These methods first train an enormous language model on unlabeled data and incorporate the representations produced by the language model into a supervised classifier .",
        "aspect": "language model",
        "sentiment": 0
    },
    {
        "text": "These methods first train an enormous language model on unlabeled data and incorporate the representations produced by the language model into a supervised classifier .",
        "aspect": "supervised classifier",
        "sentiment": 0
    },
    {
        "text": "Our base models use 1024 hidden units in their LSTMs ( compared to 4096 in ELMo ) , require fewer training steps ( around one pass over the billion - word benchmark rather than many passes ) , and do not require a pipelined training procedure .",
        "aspect": "LSTMs",
        "sentiment": 1
    },
    {
        "text": "Our base models use 1024 hidden units in their LSTMs ( compared to 4096 in ELMo ) , require fewer training steps ( around one pass over the billion - word benchmark rather than many passes ) , and do not require a pipelined training procedure .",
        "aspect": "ELMo",
        "sentiment": 3
    },
    {
        "text": "Our base models use 1024 hidden units in their LSTMs ( compared to 4096 in ELMo ) , require fewer training steps ( around one pass over the billion - word benchmark rather than many passes ) , and do not require a pipelined training procedure .",
        "aspect": "pipelined training procedure",
        "sentiment": 0
    },
    {
        "text": "Therefore , although they perform Figure : An NER example that CVT classifies correctly but supervised learning does not .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "Therefore , although they perform Figure : An NER example that CVT classifies correctly but supervised learning does not .",
        "aspect": "supervised learning",
        "sentiment": 3
    },
    {
        "text": "\" Warner \" only occurs as a last name in the train set , so the supervised model classifies \" Warner Bros \" as a person .",
        "aspect": "supervised model",
        "sentiment": 0
    },
    {
        "text": "The CVT model also mistakenly classifies \" Warner Bros \" as a person to start with , but as it sees more of the unlabeled data ( in which \" Warner \" occurs thousands of times ) it learns that \" Warner Bros \" is an organization .",
        "aspect": "CVT model",
        "sentiment": 0
    },
    {
        "text": "on par with ELMo , they are faster and simpler to train .",
        "aspect": "ELMo",
        "sentiment": 3
    },
    {
        "text": "Increasing the size of our CVT+Multitask model so it has 4096 units in its LSTMs like ELMo improves results further so they are significantly better than the ELMo+Multi - task ones .",
        "aspect": "CVT+Multitask model",
        "sentiment": 2
    },
    {
        "text": "Increasing the size of our CVT+Multitask model so it has 4096 units in its LSTMs like ELMo improves results further so they are significantly better than the ELMo+Multi - task ones .",
        "aspect": "LSTMs",
        "sentiment": 1
    },
    {
        "text": "Increasing the size of our CVT+Multitask model so it has 4096 units in its LSTMs like ELMo improves results further so they are significantly better than the ELMo+Multi - task ones .",
        "aspect": "ELMo",
        "sentiment": 3
    },
    {
        "text": "Increasing the size of our CVT+Multitask model so it has 4096 units in its LSTMs like ELMo improves results further so they are significantly better than the ELMo+Multi - task ones .",
        "aspect": "ELMo+Multi - task ones",
        "sentiment": 3
    },
    {
        "text": "We suspect there could be further gains from combining our method with language model pre - training , which we leave for future work .",
        "aspect": "language model pre - training",
        "sentiment": 0
    },
    {
        "text": "We train a single shared - encoder CVT model to perform all of the tasks except machine translation ( as it is quite different and requires more training time than the other ones ) .",
        "aspect": "shared - encoder CVT model",
        "sentiment": 0
    },
    {
        "text": "Our result shows that simple parameter sharing can be enough for effective many - task learning when the model is big and trained on a large amount of data .",
        "aspect": "parameter sharing",
        "sentiment": 0
    },
    {
        "text": "Interestingly , multi - task learning works better in conjunction with CVT than with ELMo .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "Interestingly , multi - task learning works better in conjunction with CVT than with ELMo .",
        "aspect": "ELMo",
        "sentiment": 0
    },
    {
        "text": "We hypothesize that the ELMo models quickly fit to the data primarily using the ELMo vectors , which perhaps hinders the model from learning effective representations that transfer across tasks .",
        "aspect": "ELMo models",
        "sentiment": 0
    },
    {
        "text": "We hypothesize that the ELMo models quickly fit to the data primarily using the ELMo vectors , which perhaps hinders the model from learning effective representations that transfer across tasks .",
        "aspect": "ELMo vectors",
        "sentiment": 0
    },
    {
        "text": "We also believe CVT alleviates the danger of the model \" forgetting \" one task while training on the other ones , a well - known problem in many - task learning .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "In fact , multi - task learning plus self training is similar to the Learning without Forgetting algorithm , which trains the model to keep its predictions on an old task unchanged when learning a new task .",
        "aspect": "self training",
        "sentiment": 0
    },
    {
        "text": "In fact , multi - task learning plus self training is similar to the Learning without Forgetting algorithm , which trains the model to keep its predictions on an old task unchanged when learning a new task .",
        "aspect": "Learning without Forgetting algorithm",
        "sentiment": 0
    },
    {
        "text": "To test the value of all - tasks - labeled examples , we trained a multi - task CVT model that only computes L CVT on one task at a time ( chosen randomly for each unlabeled minibatch ) instead of for all tasks in parallel .",
        "aspect": "multi - task CVT model",
        "sentiment": 0
    },
    {
        "text": "To test the value of all - tasks - labeled examples , we trained a multi - task CVT model that only computes L CVT on one task at a time ( chosen randomly for each unlabeled minibatch ) instead of for all tasks in parallel .",
        "aspect": "L CVT",
        "sentiment": 0
    },
    {
        "text": "The one - at - a - time model performs substantially worse ( see Table Model Generalization .",
        "aspect": "one - at - a - time model",
        "sentiment": 0
    },
    {
        "text": "Both CVT and multi - task learning improve model generalization : for the same train accuracy , the models get better dev accuracy than purely supervised learning .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "Both CVT and multi - task learning improve model generalization : for the same train accuracy , the models get better dev accuracy than purely supervised learning .",
        "aspect": "multi - task learning",
        "sentiment": 0
    },
    {
        "text": "Both CVT and multi - task learning improve model generalization : for the same train accuracy , the models get better dev accuracy than purely supervised learning .",
        "aspect": "model generalization",
        "sentiment": 0
    },
    {
        "text": "Both CVT and multi - task learning improve model generalization : for the same train accuracy , the models get better dev accuracy than purely supervised learning .",
        "aspect": "supervised learning",
        "sentiment": 0
    },
    {
        "text": "Interestingly , CVT continues to improve in dev set accuracy while close to 100 % train ac- curacy for CCG , Chunking , and NER , perhaps because the model is still learning from unlabeled data even when it has completely fit to the train set .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "We also show results for a smaller multi - task + CVT model .",
        "aspect": "multi - task + CVT model",
        "sentiment": 0
    },
    {
        "text": "This suggests it is important to use sufficiently large neural networks for multitask learning : otherwise the model does not have the capacity to fit to all the training data .",
        "aspect": "neural networks",
        "sentiment": 0
    },
    {
        "text": "Auxiliary Prediction Module Ablation .",
        "aspect": "Auxiliary Prediction Module Ablation",
        "sentiment": 0
    },
    {
        "text": "We briefly explore which auxiliary prediction modules are more important for the sequence tagging tasks in Table .",
        "aspect": "auxiliary prediction modules",
        "sentiment": 0
    },
    {
        "text": "We find that both kinds of auxiliary prediction modules improve performance , but that the future and past modules improve results more than the forward and backward ones , perhaps because they see a more restricted and challenging view of the input .",
        "aspect": "auxiliary prediction modules",
        "sentiment": 0
    },
    {
        "text": "We find that both kinds of auxiliary prediction modules improve performance , but that the future and past modules improve results more than the forward and backward ones , perhaps because they see a more restricted and challenging view of the input .",
        "aspect": "future and past modules",
        "sentiment": 0
    },
    {
        "text": "We explore how CVT scales with dataset size by varying the amount of training data the model has access to .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "Unsurprisingly , the improvement of CVT over purely supervised learning grows larger as the amount of labeled data decreases ( see Figure , left ) .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "Unsurprisingly , the improvement of CVT over purely supervised learning grows larger as the amount of labeled data decreases ( see Figure , left ) .",
        "aspect": "supervised learning",
        "sentiment": 0
    },
    {
        "text": "Using only 25 % of the labeled data , our approach already performs as well or better than a fully supervised model using 100 % of the training data , demonstrating that CVT is particularly useful on small datasets .",
        "aspect": "fully supervised model",
        "sentiment": 0
    },
    {
        "text": "Using only 25 % of the labeled data , our approach already performs as well or better than a fully supervised model using 100 % of the training data , demonstrating that CVT is particularly useful on small datasets .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "Most sequence taggers and dependency parsers in prior work use small LSTMs ( hidden state sizes of around 300 ) because larger models yield little to no gains in performance .",
        "aspect": "sequence taggers",
        "sentiment": 0
    },
    {
        "text": "Most sequence taggers and dependency parsers in prior work use small LSTMs ( hidden state sizes of around 300 ) because larger models yield little to no gains in performance .",
        "aspect": "dependency parsers",
        "sentiment": 0
    },
    {
        "text": "Most sequence taggers and dependency parsers in prior work use small LSTMs ( hidden state sizes of around 300 ) because larger models yield little to no gains in performance .",
        "aspect": "LSTMs",
        "sentiment": 0
    },
    {
        "text": "We found our own supervised approaches also do not benefit greatly from increasing the model size .",
        "aspect": "supervised approaches",
        "sentiment": 0
    },
    {
        "text": "This finding suggests the appropriate semi - supervised learning methods may enable the development of larger , more sophisticated models for NLP tasks with limited amounts of labeled data .",
        "aspect": "semi - supervised learning methods",
        "sentiment": 0
    },
    {
        "text": "even a vanilla supervised model , showing the multi - task model is building up effective representations for language .",
        "aspect": "vanilla supervised model",
        "sentiment": 0
    },
    {
        "text": "even a vanilla supervised model , showing the multi - task model is building up effective representations for language .",
        "aspect": "multi - task model",
        "sentiment": 0
    },
    {
        "text": "In particular , the representations could be used like skip - thought vectors to quickly train models on new tasks without slow representation learning .",
        "aspect": "slow representation learning",
        "sentiment": 0
    },
    {
        "text": "Unsupervised Representation Learning .",
        "aspect": "Unsupervised Representation Learning",
        "sentiment": 0
    },
    {
        "text": "Early approaches to deep semi - supervised learning pretrain neural models on unlabeled data , which has been successful for applications in computer vision and NLP .",
        "aspect": "neural models",
        "sentiment": 0
    },
    {
        "text": "Other approaches train \" thought vectors \" representing sentences through unsupervised or supervised learning .",
        "aspect": "unsupervised or supervised learning",
        "sentiment": 0
    },
    {
        "text": "Many recent approaches ( including the consistentency regularization methods discussed below and our own method ) train the student with soft targets from the teacher 's output distribution rather than a hard label , making the procedure more akin to knowledge distillation .",
        "aspect": "consistentency regularization methods",
        "sentiment": 0
    },
    {
        "text": "It is also possible to use multiple models or prediction modules for the teacher , such as in tri - training .",
        "aspect": "prediction modules",
        "sentiment": 0
    },
    {
        "text": "Consistency Regularization .",
        "aspect": "Consistency Regularization",
        "sentiment": 0
    },
    {
        "text": "Recent works add noise ( e.g. , drawn from a Gaussian distribution ) or apply stochastic transformations ( e.g. , horizontally flipping an image ) to the student 's inputs .",
        "aspect": "Gaussian distribution",
        "sentiment": 0
    },
    {
        "text": "Recent works add noise ( e.g. , drawn from a Gaussian distribution ) or apply stochastic transformations ( e.g. , horizontally flipping an image ) to the student 's inputs .",
        "aspect": "stochastic transformations",
        "sentiment": 0
    },
    {
        "text": "Recent works add noise ( e.g. , drawn from a Gaussian distribution ) or apply stochastic transformations ( e.g. , horizontally flipping an image ) to the student 's inputs .",
        "aspect": "horizontally flipping an image",
        "sentiment": 0
    },
    {
        "text": "Consistency regularization has been very successful for computer vision applications .",
        "aspect": "Consistency regularization",
        "sentiment": 0
    },
    {
        "text": "However , stochastic input alterations are more difficult to apply to discrete data like text , making consistency regularization less used for natural language processing .",
        "aspect": "consistency regularization",
        "sentiment": 0
    },
    {
        "text": "CVT is easily applicable to text because it does not require changing the student 's inputs .",
        "aspect": "CVT",
        "sentiment": 0
    },
    {
        "text": "Particularly relevant are co - training and co - regularization , which trains two models with disjoint views of the input .",
        "aspect": "co - training",
        "sentiment": 0
    },
    {
        "text": "Particularly relevant are co - training and co - regularization , which trains two models with disjoint views of the input .",
        "aspect": "co - regularization",
        "sentiment": 0
    },
    {
        "text": "In contrast to these methods , our approach trains a single unified model where auxiliary prediction modules see different , but not necessarily independent views of the input .",
        "aspect": "auxiliary prediction modules",
        "sentiment": 0
    },
    {
        "text": "Self - supervised learning methods train auxiliary prediction modules on tasks where performance can be measured without human - provided labels .",
        "aspect": "Self - supervised learning methods",
        "sentiment": 0
    },
    {
        "text": "Self - supervised learning methods train auxiliary prediction modules on tasks where performance can be measured without human - provided labels .",
        "aspect": "auxiliary prediction modules",
        "sentiment": 0
    },
    {
        "text": "Recent work has jointly trained image classifiers with tasks like relative position and colorization , sequence taggers with language modeling , and reinforcement learning agents with predicting changes in the environment .",
        "aspect": "image classifiers",
        "sentiment": 0
    },
    {
        "text": "Recent work has jointly trained image classifiers with tasks like relative position and colorization , sequence taggers with language modeling , and reinforcement learning agents with predicting changes in the environment .",
        "aspect": "sequence taggers",
        "sentiment": 0
    },
    {
        "text": "Recent work has jointly trained image classifiers with tasks like relative position and colorization , sequence taggers with language modeling , and reinforcement learning agents with predicting changes in the environment .",
        "aspect": "reinforcement learning agents",
        "sentiment": 0
    },
    {
        "text": "Unlike these approaches , our auxiliary losses are based on self - labeling , not labels deterministically constructed from the input .",
        "aspect": "auxiliary losses",
        "sentiment": 0
    },
    {
        "text": "Unlike these approaches , our auxiliary losses are based on self - labeling , not labels deterministically constructed from the input .",
        "aspect": "self - labeling",
        "sentiment": 0
    },
    {
        "text": "PARAMETER SPACE NOISE FOR EXPLORATION",
        "aspect": "PARAMETER SPACE NOISE",
        "sentiment": 0
    },
    {
        "text": "Deep reinforcement learning ( RL ) methods generally engage in exploratory behavior through noise injection in the action space .",
        "aspect": "Deep reinforcement learning",
        "sentiment": 0
    },
    {
        "text": "Deep reinforcement learning ( RL ) methods generally engage in exploratory behavior through noise injection in the action space .",
        "aspect": "RL",
        "sentiment": 0
    },
    {
        "text": "Methods such as evolutionary strategies use parameter perturbations , but discard all temporal structure in the process and require significantly more samples .",
        "aspect": "evolutionary strategies",
        "sentiment": 0
    },
    {
        "text": "Methods such as evolutionary strategies use parameter perturbations , but discard all temporal structure in the process and require significantly more samples .",
        "aspect": "parameter perturbations",
        "sentiment": 0
    },
    {
        "text": "Combining parameter noise with traditional RL methods allows to combine the best of both worlds .",
        "aspect": "RL methods",
        "sentiment": 0
    },
    {
        "text": "We demonstrate that both off - and on - policy methods benefit from this approach through experimental comparison of DQN , DDPG , and TRPO on high - dimensional discrete action environments as well as continuous control tasks .",
        "aspect": "off - and on - policy methods",
        "sentiment": 0
    },
    {
        "text": "We demonstrate that both off - and on - policy methods benefit from this approach through experimental comparison of DQN , DDPG , and TRPO on high - dimensional discrete action environments as well as continuous control tasks .",
        "aspect": "DQN",
        "sentiment": 3
    },
    {
        "text": "We demonstrate that both off - and on - policy methods benefit from this approach through experimental comparison of DQN , DDPG , and TRPO on high - dimensional discrete action environments as well as continuous control tasks .",
        "aspect": "DDPG",
        "sentiment": 3
    },
    {
        "text": "We demonstrate that both off - and on - policy methods benefit from this approach through experimental comparison of DQN , DDPG , and TRPO on high - dimensional discrete action environments as well as continuous control tasks .",
        "aspect": "TRPO",
        "sentiment": 3
    },
    {
        "text": "Enabling efficient and effective exploration is , however , not trivial since it is not directed by the reward function of the underlying Markov decision process ( MDP ) .",
        "aspect": "Markov decision process",
        "sentiment": 0
    },
    {
        "text": "Enabling efficient and effective exploration is , however , not trivial since it is not directed by the reward function of the underlying Markov decision process ( MDP ) .",
        "aspect": "MDP",
        "sentiment": 0
    },
    {
        "text": "Although a plethora of methods have been proposed to tackle this challenge in high - dimensional and/or continuous - action MDPs , they often rely on complex additional structures such as counting tables , density modeling of the state space , learned dynamics models , or self - supervised curiosity .",
        "aspect": "density modeling of the state space",
        "sentiment": 0
    },
    {
        "text": "Although a plethora of methods have been proposed to tackle this challenge in high - dimensional and/or continuous - action MDPs , they often rely on complex additional structures such as counting tables , density modeling of the state space , learned dynamics models , or self - supervised curiosity .",
        "aspect": "learned dynamics models",
        "sentiment": 0
    },
    {
        "text": "Although a plethora of methods have been proposed to tackle this challenge in high - dimensional and/or continuous - action MDPs , they often rely on complex additional structures such as counting tables , density modeling of the state space , learned dynamics models , or self - supervised curiosity .",
        "aspect": "self - supervised curiosity",
        "sentiment": 0
    },
    {
        "text": "Although a plethora of methods have been proposed to tackle this challenge in high - dimensional and/or continuous - action MDPs , they often rely on complex additional structures such as counting tables , density modeling of the state space , learned dynamics models , or self - supervised curiosity .",
        "aspect": "counting tables",
        "sentiment": 0
    },
    {
        "text": "An orthogonal way of increasing the exploratory nature of these algorithms is through the addition of temporally - correlated noise , for example as done in bootstrapped DQN .",
        "aspect": "bootstrapped DQN",
        "sentiment": 0
    },
    {
        "text": "We discuss these related approaches in greater detail in Section 5 . Their main limitation , however , is that they are either only proposed and evaluated for the on - policy setting with relatively small and shallow function approximators or disregard all temporal structure and gradient information .",
        "aspect": "shallow function approximators",
        "sentiment": 0
    },
    {
        "text": "This paper investigates how parameter space noise can be effectively combined with off - the - shelf deep RL algorithms such as DQN , DDPG , and TRPO to improve their exploratory behavior .",
        "aspect": "deep RL algorithms",
        "sentiment": 1
    },
    {
        "text": "This paper investigates how parameter space noise can be effectively combined with off - the - shelf deep RL algorithms such as DQN , DDPG , and TRPO to improve their exploratory behavior .",
        "aspect": "DQN",
        "sentiment": 1
    },
    {
        "text": "This paper investigates how parameter space noise can be effectively combined with off - the - shelf deep RL algorithms such as DQN , DDPG , and TRPO to improve their exploratory behavior .",
        "aspect": "DDPG",
        "sentiment": 1
    },
    {
        "text": "This paper investigates how parameter space noise can be effectively combined with off - the - shelf deep RL algorithms such as DQN , DDPG , and TRPO to improve their exploratory behavior .",
        "aspect": "TRPO",
        "sentiment": 1
    },
    {
        "text": "Experiments show that this form of exploration is applicable to both high - dimensional discrete environments and continuous control tasks , using on - and off - policy methods .",
        "aspect": "on - and off - policy methods",
        "sentiment": 0
    },
    {
        "text": "Our results indicate that parameter noise outperforms traditional action space noise - based baselines , especially in tasks where the reward signal is extremely sparse .",
        "aspect": "parameter noise",
        "sentiment": 0
    },
    {
        "text": "Our results indicate that parameter noise outperforms traditional action space noise - based baselines , especially in tasks where the reward signal is extremely sparse .",
        "aspect": "action space noise",
        "sentiment": 0
    },
    {
        "text": "We consider the standard RL framework consisting of an agent interacting with an environment .",
        "aspect": "RL framework",
        "sentiment": 1
    },
    {
        "text": "An environment is modeled as a Markov decision process ( MDP ) and is defined by a set of states S , a set of actions A , a distribution over initial states p(s 0 ) , a reward function r : S \u00d7 A \u2192 R , transition probabilities 1 arXiv:1706.01905v2",
        "aspect": "Markov decision process",
        "sentiment": 1
    },
    {
        "text": "An environment is modeled as a Markov decision process ( MDP ) and is defined by a set of states S , a set of actions A , a distribution over initial states p(s 0 ) , a reward function r : S \u00d7 A \u2192 R , transition probabilities 1 arXiv:1706.01905v2",
        "aspect": "MDP",
        "sentiment": 1
    },
    {
        "text": "Experimental evaluation is based on the undiscounted return E \u03c4 [ T t=0 r(s t , a t ) ] . 1 2.1 OFF - POLICY METHODS Off - policy RL methods allow learning based on data captured by arbitrary policies .",
        "aspect": "OFF - POLICY METHODS",
        "sentiment": 0
    },
    {
        "text": "Experimental evaluation is based on the undiscounted return E \u03c4 [ T t=0 r(s t , a t ) ] . 1 2.1 OFF - POLICY METHODS Off - policy RL methods allow learning based on data captured by arbitrary policies .",
        "aspect": "Off - policy RL methods",
        "sentiment": 0
    },
    {
        "text": "This paper considers two popular off - policy algorithms , namely Deep Q - Networks ( DQN , ) and Deep Deterministic Policy Gradients ( DDPG , ) .",
        "aspect": "off - policy algorithms",
        "sentiment": 1
    },
    {
        "text": "This paper considers two popular off - policy algorithms , namely Deep Q - Networks ( DQN , ) and Deep Deterministic Policy Gradients ( DDPG , ) .",
        "aspect": "Deep Q - Networks",
        "sentiment": 1
    },
    {
        "text": "This paper considers two popular off - policy algorithms , namely Deep Q - Networks ( DQN , ) and Deep Deterministic Policy Gradients ( DDPG , ) .",
        "aspect": "DQN",
        "sentiment": 1
    },
    {
        "text": "This paper considers two popular off - policy algorithms , namely Deep Q - Networks ( DQN , ) and Deep Deterministic Policy Gradients ( DDPG , ) .",
        "aspect": "Deep Deterministic Policy Gradients",
        "sentiment": 1
    },
    {
        "text": "This paper considers two popular off - policy algorithms , namely Deep Q - Networks ( DQN , ) and Deep Deterministic Policy Gradients ( DDPG , ) .",
        "aspect": "DDPG ,",
        "sentiment": 1
    },
    {
        "text": "Deep Q - Networks ( DQN ) DQN uses a deep neural network as a function approximator to estimate the optimal Q - value function , which conforms to the Bellman optimality equation :",
        "aspect": "Deep Q - Networks",
        "sentiment": 0
    },
    {
        "text": "Deep Q - Networks ( DQN ) DQN uses a deep neural network as a function approximator to estimate the optimal Q - value function , which conforms to the Bellman optimality equation :",
        "aspect": "DQN",
        "sentiment": 0
    },
    {
        "text": "Deep Q - Networks ( DQN ) DQN uses a deep neural network as a function approximator to estimate the optimal Q - value function , which conforms to the Bellman optimality equation :",
        "aspect": "DQN",
        "sentiment": 0
    },
    {
        "text": "Deep Q - Networks ( DQN ) DQN uses a deep neural network as a function approximator to estimate the optimal Q - value function , which conforms to the Bellman optimality equation :",
        "aspect": "deep neural network",
        "sentiment": 0
    },
    {
        "text": "Deep Q - Networks ( DQN ) DQN uses a deep neural network as a function approximator to estimate the optimal Q - value function , which conforms to the Bellman optimality equation :",
        "aspect": "function approximator",
        "sentiment": 0
    },
    {
        "text": "Typically , a stochasticgreedy or Boltzmann policy is derived from the Q - value function to encourage exploration , which relies on sampling noise in the action space .",
        "aspect": "Q - value function",
        "sentiment": 0
    },
    {
        "text": "Typically , a stochasticgreedy or Boltzmann policy is derived from the Q - value function to encourage exploration , which relies on sampling noise in the action space .",
        "aspect": "stochasticgreedy",
        "sentiment": 0
    },
    {
        "text": "Typically , a stochasticgreedy or Boltzmann policy is derived from the Q - value function to encourage exploration , which relies on sampling noise in the action space .",
        "aspect": "Boltzmann policy",
        "sentiment": 0
    },
    {
        "text": "The Q - network predicts a Q - value for each action and is updated using off - policy data from a replay buffer .",
        "aspect": "Q - network",
        "sentiment": 0
    },
    {
        "text": "Deep Deterministic Policy Gradients ( DDPG ) DDPG is an actor - critic algorithm , applicable to continuous action spaces .",
        "aspect": "Deep Deterministic Policy Gradients",
        "sentiment": 0
    },
    {
        "text": "Deep Deterministic Policy Gradients ( DDPG ) DDPG is an actor - critic algorithm , applicable to continuous action spaces .",
        "aspect": "DDPG",
        "sentiment": 0
    },
    {
        "text": "Deep Deterministic Policy Gradients ( DDPG ) DDPG is an actor - critic algorithm , applicable to continuous action spaces .",
        "aspect": "DDPG",
        "sentiment": 0
    },
    {
        "text": "Deep Deterministic Policy Gradients ( DDPG ) DDPG is an actor - critic algorithm , applicable to continuous action spaces .",
        "aspect": "actor - critic algorithm",
        "sentiment": 0
    },
    {
        "text": "Similar to DQN , the critic estimates the Q - value function using off - policy data and the recursive Bellman equation :",
        "aspect": "DQN",
        "sentiment": 0
    },
    {
        "text": "Similar to DQN , the critic estimates the Q - value function using off - policy data and the recursive Bellman equation :",
        "aspect": "Q - value function",
        "sentiment": 0
    },
    {
        "text": "Similar to DQN , the critic estimates the Q - value function using off - policy data and the recursive Bellman equation :",
        "aspect": "recursive Bellman equation",
        "sentiment": 0
    },
    {
        "text": "For exploration , DDPG uses a stochastic policy of the form \u03c0 \u03b8 ( s t ) = \u03c0 \u03b8 ( s t ) + w , where w is either w \u223c N ( 0 , \u03c3 2 I ) ( uncorrelated ) or w \u223c OU(0 , \u03c3 2 ) ( correlated ) . 2 Again , exploration is realized through action space noise .",
        "aspect": "DDPG",
        "sentiment": 0
    },
    {
        "text": "For exploration , DDPG uses a stochastic policy of the form \u03c0 \u03b8 ( s t ) = \u03c0 \u03b8 ( s t ) + w , where w is either w \u223c N ( 0 , \u03c3 2 I ) ( uncorrelated ) or w \u223c OU(0 , \u03c3 2 ) ( correlated ) . 2 Again , exploration is realized through action space noise .",
        "aspect": "stochastic policy",
        "sentiment": 0
    },
    {
        "text": "The idea of perturbing the parameters of a policy has been proposed by for policy gradient methods .",
        "aspect": "policy gradient methods",
        "sentiment": 0
    },
    {
        "text": "The authors show that this form of perturbation generally outperforms random exploration and evaluate their exploration strategy with the REINFORCE ( Williams , 1992a ) and Natural Actor - Critic algorithms .",
        "aspect": "random exploration",
        "sentiment": 0
    },
    {
        "text": "The authors show that this form of perturbation generally outperforms random exploration and evaluate their exploration strategy with the REINFORCE ( Williams , 1992a ) and Natural Actor - Critic algorithms .",
        "aspect": "exploration strategy",
        "sentiment": 0
    },
    {
        "text": "The authors show that this form of perturbation generally outperforms random exploration and evaluate their exploration strategy with the REINFORCE ( Williams , 1992a ) and Natural Actor - Critic algorithms .",
        "aspect": "Natural Actor - Critic algorithms",
        "sentiment": 0
    },
    {
        "text": "However , their policies are relatively lowdimensional compared to modern deep architectures , they use environments with low - dimensional state spaces , and their contribution is strictly limited to the policy gradient case .",
        "aspect": "deep architectures",
        "sentiment": 0
    },
    {
        "text": "In contrast , our method is applied and evaluated for both on and off - policy setting , we use high - dimensional policies , and environments with large state spaces .",
        "aspect": "high - dimensional policies",
        "sentiment": 1
    },
    {
        "text": "Our work is also closely related to evolution strategies ( ES , ; ) , and especially neural evolution strategies ( NES , ; ; ) .",
        "aspect": "evolution strategies",
        "sentiment": 0
    },
    {
        "text": "Our work is also closely related to evolution strategies ( ES , ; ) , and especially neural evolution strategies ( NES , ; ; ) .",
        "aspect": "ES",
        "sentiment": 0
    },
    {
        "text": "Our work is also closely related to evolution strategies ( ES , ; ) , and especially neural evolution strategies ( NES , ; ; ) .",
        "aspect": "neural evolution strategies",
        "sentiment": 0
    },
    {
        "text": "Our work is also closely related to evolution strategies ( ES , ; ) , and especially neural evolution strategies ( NES , ; ; ) .",
        "aspect": "NES",
        "sentiment": 0
    },
    {
        "text": "More recently , showed that ES can work for high - dimensional environments like Atari and OpenAI Gym continuous control problems .",
        "aspect": "ES",
        "sentiment": 0
    },
    {
        "text": "Bootstrapped DQN has been proposed to aid with more directed and consistent exploration by using a network with multiple heads , where one specific head is selected at the beginning of each episode .",
        "aspect": "Bootstrapped DQN",
        "sentiment": 0
    },
    {
        "text": "Concurrently to our work , Fortunato et al . ( ) have proposed a similar approach that utilizes parameter perturbations for more efficient exploration .",
        "aspect": "parameter perturbations",
        "sentiment": 3
    },
    {
        "text": "In this work , we propose parameter space noise as a conceptually simple yet effective replacement for traditional action space noise like -greedy and additive Gaussian noise .",
        "aspect": "parameter space noise",
        "sentiment": 2
    },
    {
        "text": "In this work , we propose parameter space noise as a conceptually simple yet effective replacement for traditional action space noise like -greedy and additive Gaussian noise .",
        "aspect": "action space noise",
        "sentiment": 3
    },
    {
        "text": "In this work , we propose parameter space noise as a conceptually simple yet effective replacement for traditional action space noise like -greedy and additive Gaussian noise .",
        "aspect": "and additive Gaussian noise .",
        "sentiment": 3
    },
    {
        "text": "This work shows that parameter perturbations can successfully be combined with contemporary on - and off - policy deep RL algorithms such as DQN , DDPG , and TRPO and often results in improved performance compared to action noise .",
        "aspect": "parameter perturbations",
        "sentiment": 0
    },
    {
        "text": "This work shows that parameter perturbations can successfully be combined with contemporary on - and off - policy deep RL algorithms such as DQN , DDPG , and TRPO and often results in improved performance compared to action noise .",
        "aspect": "on - and off - policy deep RL algorithms",
        "sentiment": 0
    },
    {
        "text": "This work shows that parameter perturbations can successfully be combined with contemporary on - and off - policy deep RL algorithms such as DQN , DDPG , and TRPO and often results in improved performance compared to action noise .",
        "aspect": "DQN",
        "sentiment": 0
    },
    {
        "text": "This work shows that parameter perturbations can successfully be combined with contemporary on - and off - policy deep RL algorithms such as DQN , DDPG , and TRPO and often results in improved performance compared to action noise .",
        "aspect": "DDPG",
        "sentiment": 0
    },
    {
        "text": "This work shows that parameter perturbations can successfully be combined with contemporary on - and off - policy deep RL algorithms such as DQN , DDPG , and TRPO and often results in improved performance compared to action noise .",
        "aspect": "TRPO",
        "sentiment": 0
    },
    {
        "text": "Experimental results further demonstrate that using parameter noise allows solving environments with very sparse rewards , in which action noise is unlikely to succeed .",
        "aspect": "parameter noise",
        "sentiment": 0
    },
    {
        "text": "This consists of 3 convolutional layers ( 32 filters of size 8 \u00d7 8 and stride 4 , 64 filters of size 4 \u00d7 4 and stride 2 , 64 filters of size 3 \u00d7 3 and stride 1 ) followed by 1 hidden layer with 512 units followed by a linear output layer with one unit for each action .",
        "aspect": "convolutional layers",
        "sentiment": 1
    },
    {
        "text": "ReLUs are used in each layer , while layer normalization is used in the fully connected part of the network .",
        "aspect": "ReLUs",
        "sentiment": 1
    },
    {
        "text": "ReLUs are used in each layer , while layer normalization is used in the fully connected part of the network .",
        "aspect": "layer normalization",
        "sentiment": 1
    },
    {
        "text": "For parameter space noise , we also include a second head after the convolutional stack of layers .",
        "aspect": "convolutional stack of layers",
        "sentiment": 1
    },
    {
        "text": "This head determines a policy network with the same architecture as the Q - value network , except for a softmax output layer .",
        "aspect": "policy network",
        "sentiment": 0
    },
    {
        "text": "This head determines a policy network with the same architecture as the Q - value network , except for a softmax output layer .",
        "aspect": "Q - value network",
        "sentiment": 0
    },
    {
        "text": "This head determines a policy network with the same architecture as the Q - value network , except for a softmax output layer .",
        "aspect": "softmax output layer",
        "sentiment": 0
    },
    {
        "text": "The Q - value network is trained using the Adam optimizer with a learning rate of 10 \u22124 and a batch size of 32 .",
        "aspect": "Q - value network",
        "sentiment": 1
    },
    {
        "text": "The Q - value network is trained using the Adam optimizer with a learning rate of 10 \u22124 and a batch size of 32 .",
        "aspect": "Adam optimizer",
        "sentiment": 1
    },
    {
        "text": "For parameter space noise , we adaptively scale the noise to have a similar effect in action space ( see Section C.1 for details ) , effectively ensuring that the maximum KL divergence between perturbed and non - perturbed \u03c0 is softly enforced .",
        "aspect": "maximum KL divergence",
        "sentiment": 0
    },
    {
        "text": "Notice that we only perturb the policy head after the convolutional part of the network ( i.e. the fully connected part , which is also why we only include layer normalization in this part of the network ) .",
        "aspect": "convolutional part",
        "sentiment": 0
    },
    {
        "text": "Notice that we only perturb the policy head after the convolutional part of the network ( i.e. the fully connected part , which is also why we only include layer normalization in this part of the network ) .",
        "aspect": "fully connected part",
        "sentiment": 0
    },
    {
        "text": "Notice that we only perturb the policy head after the convolutional part of the network ( i.e. the fully connected part , which is also why we only include layer normalization in this part of the network ) .",
        "aspect": "layer normalization",
        "sentiment": 0
    },
    {
        "text": "To avoid getting stuck ( which can potentially happen for a perturbed policy ) , we also use -greedy action selection with = 0.01 .",
        "aspect": "perturbed policy",
        "sentiment": 0
    },
    {
        "text": "To avoid getting stuck ( which can potentially happen for a perturbed policy ) , we also use -greedy action selection with = 0.01 .",
        "aspect": "-greedy action selection",
        "sentiment": 1
    },
    {
        "text": "TRUST - PCL : AN OFF - POLICY TRUST REGION METHOD FOR CONTINUOUS CONTROL",
        "aspect": "TRUST - PCL",
        "sentiment": 0
    },
    {
        "text": "TRUST - PCL : AN OFF - POLICY TRUST REGION METHOD FOR CONTINUOUS CONTROL",
        "aspect": "OFF - POLICY TRUST REGION METHOD",
        "sentiment": 0
    },
    {
        "text": "Trust region methods , such as TRPO , are often used to stabilize policy optimization algorithms in reinforcement learning ( RL ) .",
        "aspect": "Trust region methods",
        "sentiment": 0
    },
    {
        "text": "Trust region methods , such as TRPO , are often used to stabilize policy optimization algorithms in reinforcement learning ( RL ) .",
        "aspect": "TRPO",
        "sentiment": 0
    },
    {
        "text": "Trust region methods , such as TRPO , are often used to stabilize policy optimization algorithms in reinforcement learning ( RL ) .",
        "aspect": "policy optimization algorithms",
        "sentiment": 0
    },
    {
        "text": "Trust region methods , such as TRPO , are often used to stabilize policy optimization algorithms in reinforcement learning ( RL ) .",
        "aspect": "reinforcement learning",
        "sentiment": 0
    },
    {
        "text": "Trust region methods , such as TRPO , are often used to stabilize policy optimization algorithms in reinforcement learning ( RL ) .",
        "aspect": "RL",
        "sentiment": 0
    },
    {
        "text": "While current trust region strategies are effective for continuous control , they typically require a large amount of on - policy interaction with the environment .",
        "aspect": "trust region strategies",
        "sentiment": 0
    },
    {
        "text": "To address this problem , we propose an off - policy trust region method , Trust - PCL , which exploits an observation that the optimal policy and state values of a maximum reward objective with a relative - entropy regularizer satisfy a set of multi - step pathwise consistencies along any path .",
        "aspect": "off - policy trust region method",
        "sentiment": 2
    },
    {
        "text": "To address this problem , we propose an off - policy trust region method , Trust - PCL , which exploits an observation that the optimal policy and state values of a maximum reward objective with a relative - entropy regularizer satisfy a set of multi - step pathwise consistencies along any path .",
        "aspect": "Trust - PCL",
        "sentiment": 2
    },
    {
        "text": "To address this problem , we propose an off - policy trust region method , Trust - PCL , which exploits an observation that the optimal policy and state values of a maximum reward objective with a relative - entropy regularizer satisfy a set of multi - step pathwise consistencies along any path .",
        "aspect": "relative - entropy regularizer",
        "sentiment": 0
    },
    {
        "text": "The introduction of relative entropy regularization allows Trust - PCL to maintain optimization stability while exploiting off - policy data to improve sample efficiency .",
        "aspect": "relative entropy regularization",
        "sentiment": 0
    },
    {
        "text": "The introduction of relative entropy regularization allows Trust - PCL to maintain optimization stability while exploiting off - policy data to improve sample efficiency .",
        "aspect": "Trust - PCL",
        "sentiment": 0
    },
    {
        "text": "When evaluated on a number of continuous control tasks , Trust - PCL significantly improves the solution quality and sample efficiency of TRPO . 1",
        "aspect": "Trust - PCL",
        "sentiment": 0
    },
    {
        "text": "When evaluated on a number of continuous control tasks , Trust - PCL significantly improves the solution quality and sample efficiency of TRPO . 1",
        "aspect": "TRPO",
        "sentiment": 0
    },
    {
        "text": "The goal of model - free reinforcement learning ( RL ) is to optimize an agent 's behavior policy through trial and error interaction with a black box environment .",
        "aspect": "model - free reinforcement learning",
        "sentiment": 0
    },
    {
        "text": "The goal of model - free reinforcement learning ( RL ) is to optimize an agent 's behavior policy through trial and error interaction with a black box environment .",
        "aspect": "RL",
        "sentiment": 0
    },
    {
        "text": "The goal of model - free reinforcement learning ( RL ) is to optimize an agent 's behavior policy through trial and error interaction with a black box environment .",
        "aspect": "trial and error interaction",
        "sentiment": 0
    },
    {
        "text": "Value - based RL algorithms such as Q - learning and policy - based algorithms such as actor - critic have achieved well - known successes in environments with enumerable action spaces and predictable but possibly complex dynamics , e.g. , as in Atari games .",
        "aspect": "Value - based RL algorithms",
        "sentiment": 0
    },
    {
        "text": "Value - based RL algorithms such as Q - learning and policy - based algorithms such as actor - critic have achieved well - known successes in environments with enumerable action spaces and predictable but possibly complex dynamics , e.g. , as in Atari games .",
        "aspect": "Q - learning",
        "sentiment": 0
    },
    {
        "text": "Value - based RL algorithms such as Q - learning and policy - based algorithms such as actor - critic have achieved well - known successes in environments with enumerable action spaces and predictable but possibly complex dynamics , e.g. , as in Atari games .",
        "aspect": "policy - based algorithms",
        "sentiment": 0
    },
    {
        "text": "Value - based RL algorithms such as Q - learning and policy - based algorithms such as actor - critic have achieved well - known successes in environments with enumerable action spaces and predictable but possibly complex dynamics , e.g. , as in Atari games .",
        "aspect": "actor - critic",
        "sentiment": 0
    },
    {
        "text": "In an attempt to improve the applicability of Q - learning to continuous control , and developed an off - policy algorithm DDPG , leading to promising results on continuous control environments .",
        "aspect": "Q - learning",
        "sentiment": 0
    },
    {
        "text": "In an attempt to improve the applicability of Q - learning to continuous control , and developed an off - policy algorithm DDPG , leading to promising results on continuous control environments .",
        "aspect": "off - policy algorithm DDPG",
        "sentiment": 0
    },
    {
        "text": "That said , current off - policy methods including DDPG often improve data efficiency at the cost of optimization stability .",
        "aspect": "off - policy methods",
        "sentiment": 0
    },
    {
        "text": "That said , current off - policy methods including DDPG often improve data efficiency at the cost of optimization stability .",
        "aspect": "DDPG",
        "sentiment": 0
    },
    {
        "text": "The behaviour of DDPG is known to be highly dependent on hyperparameter selection and initialization ; even when using optimal hyperparameters , individual training runs can display highly varying outcomes .",
        "aspect": "DDPG",
        "sentiment": 0
    },
    {
        "text": "The behaviour of DDPG is known to be highly dependent on hyperparameter selection and initialization ; even when using optimal hyperparameters , individual training runs can display highly varying outcomes .",
        "aspect": "hyperparameter selection",
        "sentiment": 0
    },
    {
        "text": "On the other hand , in an attempt to improve the stability and convergence speed of policy - based RL methods , developed a natural policy gradient algorithm based on Amari ( 1998 ) , which subsequently led to the development of trust region policy optimization ( TRPO ) .",
        "aspect": "policy - based RL methods",
        "sentiment": 0
    },
    {
        "text": "On the other hand , in an attempt to improve the stability and convergence speed of policy - based RL methods , developed a natural policy gradient algorithm based on Amari ( 1998 ) , which subsequently led to the development of trust region policy optimization ( TRPO ) .",
        "aspect": "natural policy gradient algorithm",
        "sentiment": 0
    },
    {
        "text": "On the other hand , in an attempt to improve the stability and convergence speed of policy - based RL methods , developed a natural policy gradient algorithm based on Amari ( 1998 ) , which subsequently led to the development of trust region policy optimization ( TRPO ) .",
        "aspect": "trust region policy optimization",
        "sentiment": 0
    },
    {
        "text": "On the other hand , in an attempt to improve the stability and convergence speed of policy - based RL methods , developed a natural policy gradient algorithm based on Amari ( 1998 ) , which subsequently led to the development of trust region policy optimization ( TRPO ) .",
        "aspect": "TRPO",
        "sentiment": 0
    },
    {
        "text": "TRPO has shown strong empirical performance on difficult continuous control tasks often outperforming value - based methods like DDPG .",
        "aspect": "TRPO",
        "sentiment": 0
    },
    {
        "text": "TRPO has shown strong empirical performance on difficult continuous control tasks often outperforming value - based methods like DDPG .",
        "aspect": "value - based methods",
        "sentiment": 0
    },
    {
        "text": "TRPO has shown strong empirical performance on difficult continuous control tasks often outperforming value - based methods like DDPG .",
        "aspect": "DDPG",
        "sentiment": 0
    },
    {
        "text": "Efforts at combining the stability of trust region policy - based methods with the sample efficiency of value - based methods have focused on using off - policy data to better train a value estimate , which can be used as a control variate for variance reduction .",
        "aspect": "trust region policy - based methods",
        "sentiment": 0
    },
    {
        "text": "Efforts at combining the stability of trust region policy - based methods with the sample efficiency of value - based methods have focused on using off - policy data to better train a value estimate , which can be used as a control variate for variance reduction .",
        "aspect": "sample efficiency of value - based methods",
        "sentiment": 0
    },
    {
        "text": "Efforts at combining the stability of trust region policy - based methods with the sample efficiency of value - based methods have focused on using off - policy data to better train a value estimate , which can be used as a control variate for variance reduction .",
        "aspect": "value estimate",
        "sentiment": 0
    },
    {
        "text": "In this paper , we investigate an alternative approach to improving the sample efficiency of trust region policy - based RL methods .",
        "aspect": "trust region policy - based RL methods",
        "sentiment": 1
    },
    {
        "text": "We exploit the key fact that , under entropy regularization , the optimal policy and value function satisfy a set of pathwise consistency properties along any sampled path , which allows both on and off - policy data to be incorporated in an actor - critic algorithm , PCL .",
        "aspect": "entropy regularization",
        "sentiment": 0
    },
    {
        "text": "We exploit the key fact that , under entropy regularization , the optimal policy and value function satisfy a set of pathwise consistency properties along any sampled path , which allows both on and off - policy data to be incorporated in an actor - critic algorithm , PCL .",
        "aspect": "value function",
        "sentiment": 0
    },
    {
        "text": "We exploit the key fact that , under entropy regularization , the optimal policy and value function satisfy a set of pathwise consistency properties along any sampled path , which allows both on and off - policy data to be incorporated in an actor - critic algorithm , PCL .",
        "aspect": "actor - critic algorithm",
        "sentiment": 0
    },
    {
        "text": "We exploit the key fact that , under entropy regularization , the optimal policy and value function satisfy a set of pathwise consistency properties along any sampled path , which allows both on and off - policy data to be incorporated in an actor - critic algorithm , PCL .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "We exploit the key fact that , under entropy regularization , the optimal policy and value function satisfy a set of pathwise consistency properties along any sampled path , which allows both on and off - policy data to be incorporated in an actor - critic algorithm , PCL .",
        "aspect": "the optimal policy",
        "sentiment": 0
    },
    {
        "text": "The original PCL algorithm optimized an entropy regularized maximum reward objective and was evaluated on relatively simple tasks .",
        "aspect": "PCL algorithm",
        "sentiment": 0
    },
    {
        "text": "The original PCL algorithm optimized an entropy regularized maximum reward objective and was evaluated on relatively simple tasks .",
        "aspect": "entropy regularized maximum reward objective",
        "sentiment": 0
    },
    {
        "text": "Here we extend the ideas of PCL to achieve strong results on standard , challenging continuous control benchmarks .",
        "aspect": "PCL",
        "sentiment": 1
    },
    {
        "text": "The main observation is that by alternatively augmenting the maximum reward objective with a relative entropy regularizer , the optimal policy and values still satisfy a certain set of pathwise consistencies along any sampled trajectory .",
        "aspect": "relative entropy regularizer",
        "sentiment": 0
    },
    {
        "text": "We exploit this observation to propose a new off - policy trust region algorithm , Trust - PCL , that is able to exploit off - policy data to train policy and value estimates .",
        "aspect": "off - policy trust region algorithm",
        "sentiment": 2
    },
    {
        "text": "We exploit this observation to propose a new off - policy trust region algorithm , Trust - PCL , that is able to exploit off - policy data to train policy and value estimates .",
        "aspect": "Trust - PCL",
        "sentiment": 2
    },
    {
        "text": "We exploit this observation to propose a new off - policy trust region algorithm , Trust - PCL , that is able to exploit off - policy data to train policy and value estimates .",
        "aspect": "policy and value estimates",
        "sentiment": 0
    },
    {
        "text": "Moreover , we present a simple method for determining the coefficient on the relative entropy regularizer to remain agnostic to reward scale , hence ameliorating the task of hyperparameter tuning .",
        "aspect": "determining the coefficient on the relative entropy regularizer",
        "sentiment": 2
    },
    {
        "text": "We find that the incorporation of a relative entropy regularizer is crucial for good and stable performance .",
        "aspect": "relative entropy regularizer",
        "sentiment": 0
    },
    {
        "text": "We evaluate Trust - PCL against TRPO , and observe that Trust - PCL is able to solve difficult continuous control tasks , while improving the performance of TRPO both in terms of the final reward achieved as well as sample - efficiency .",
        "aspect": "Trust - PCL",
        "sentiment": 0
    },
    {
        "text": "We evaluate Trust - PCL against TRPO , and observe that Trust - PCL is able to solve difficult continuous control tasks , while improving the performance of TRPO both in terms of the final reward achieved as well as sample - efficiency .",
        "aspect": "TRPO",
        "sentiment": 0
    },
    {
        "text": "We evaluate Trust - PCL against TRPO , and observe that Trust - PCL is able to solve difficult continuous control tasks , while improving the performance of TRPO both in terms of the final reward achieved as well as sample - efficiency .",
        "aspect": "Trust - PCL",
        "sentiment": 0
    },
    {
        "text": "We evaluate Trust - PCL against TRPO , and observe that Trust - PCL is able to solve difficult continuous control tasks , while improving the performance of TRPO both in terms of the final reward achieved as well as sample - efficiency .",
        "aspect": "TRPO",
        "sentiment": 0
    },
    {
        "text": "Trust Region Methods .",
        "aspect": "Trust Region Methods",
        "sentiment": 0
    },
    {
        "text": "Gradient descent is the predominant optimization method for neural networks .",
        "aspect": "Gradient descent",
        "sentiment": 0
    },
    {
        "text": "Gradient descent is the predominant optimization method for neural networks .",
        "aspect": "optimization method",
        "sentiment": 0
    },
    {
        "text": "Gradient descent is the predominant optimization method for neural networks .",
        "aspect": "neural networks",
        "sentiment": 0
    },
    {
        "text": "A gradient descent step is equivalent to solving a trust region constrained optimization ,",
        "aspect": "gradient descent step",
        "sentiment": 0
    },
    {
        "text": "which yields the locally optimal update d\u03b8 = \u2212\u03b7\u2207 ( \u03b8 ) such that \u03b7 = \u221a / \u2207 ( \u03b8 ) ; hence by considering a Euclidean ball , gradient descent assumes the parameters lie in a Euclidean space .",
        "aspect": "locally optimal update",
        "sentiment": 0
    },
    {
        "text": "which yields the locally optimal update d\u03b8 = \u2212\u03b7\u2207 ( \u03b8 ) such that \u03b7 = \u221a / \u2207 ( \u03b8 ) ; hence by considering a Euclidean ball , gradient descent assumes the parameters lie in a Euclidean space .",
        "aspect": "gradient descent",
        "sentiment": 0
    },
    {
        "text": "The natural gradient ( Amari , 1998 ) is a generalization of gradient descent where the Fisher information matrix F ( \u03b8 ) is used to define the local geometry of the parameter space around \u03b8 .",
        "aspect": "generalization of gradient descent",
        "sentiment": 0
    },
    {
        "text": "This geometry is especially effective for optimizing the log - likelihood of a conditional probabilistic model , where the objective is in fact the KL divergence D KL ( \u03b8 * , \u03b8 ) .",
        "aspect": "conditional probabilistic model",
        "sentiment": 0
    },
    {
        "text": ", which is accurate up to a second order Taylor approximation .",
        "aspect": "second order Taylor approximation",
        "sentiment": 0
    },
    {
        "text": "Previous work has applied natural gradient to policy optimization , locally improving expected reward subject to variants of d\u03b8 T F ( \u03b8)d\u03b8 \u2264 .",
        "aspect": "natural gradient",
        "sentiment": 0
    },
    {
        "text": "Recently , TRPO has achieved state - of - the - art results in continuous control by adding several approximations to the natural gradient to make nonlinear policy optimization feasible .",
        "aspect": "TRPO",
        "sentiment": 0
    },
    {
        "text": "Another approach to trust region optimization is given by proximal gradient methods .",
        "aspect": "proximal gradient methods",
        "sentiment": 0
    },
    {
        "text": "The class of proximal gradient methods most similar to our work are those that replace the hard constraint in ( 2 ) with a penalty added to the objective .",
        "aspect": "proximal gradient methods",
        "sentiment": 3
    },
    {
        "text": "These techniques have recently become popular in RL , although in terms of final reward performance on continuous control benchmarks , TRPO is still considered to be the state - of - the - art .",
        "aspect": "TRPO",
        "sentiment": 0
    },
    {
        "text": "The use of F ( \u03b8 ) in previous work can be considered to be an approximation when entropy regularization is present , but it is not ideal , particularly if d\u03b8 is large .",
        "aspect": "entropy regularization",
        "sentiment": 0
    },
    {
        "text": "Defining the constraint in this way appears to be more natural and effective than that of TRPO .",
        "aspect": "TRPO",
        "sentiment": 0
    },
    {
        "text": "The existence of multi - step softmax consistencies has been noted by prior work - first by in the presence of entropy regularization .",
        "aspect": "entropy regularization",
        "sentiment": 0
    },
    {
        "text": "Our work presents multi - step consistency relations for a hybrid relative entropy plus entropy regularized expected reward objective , interpreting relative entropy regularization as a trust region constraint .",
        "aspect": "relative entropy regularization",
        "sentiment": 0
    },
    {
        "text": "More recently , showed that soft Qlearning ( a single - step special case of PCL ) can succeed on more challenging environments , such as a variant of the Swimmer task we consider below .",
        "aspect": "soft Qlearning",
        "sentiment": 0
    },
    {
        "text": "More recently , showed that soft Qlearning ( a single - step special case of PCL ) can succeed on more challenging environments , such as a variant of the Swimmer task we consider below .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "By contrast , this paper presents a successful application of the softmax consistency concept to difficult and standard continuous - control benchmarks , resulting in performance that is competitive with and in some cases beats the state - of - the - art .",
        "aspect": "softmax consistency concept",
        "sentiment": 0
    },
    {
        "text": "We have presented Trust - PCL , an off - policy algorithm employing a relative - entropy penalty to impose a trust region on a maximum reward objective .",
        "aspect": "Trust - PCL",
        "sentiment": 2
    },
    {
        "text": "We have presented Trust - PCL , an off - policy algorithm employing a relative - entropy penalty to impose a trust region on a maximum reward objective .",
        "aspect": "off - policy algorithm",
        "sentiment": 1
    },
    {
        "text": "We found that Trust - PCL can perform well on a set of standard control tasks , improving upon TRPO both in terms of average reward and sample efficiency .",
        "aspect": "Trust - PCL",
        "sentiment": 0
    },
    {
        "text": "Our best results on Trust - PCL are able to maintain the stability and solution quality of TRPO while approaching the sample - efficiency of value - based methods ( see e.g. , ) .",
        "aspect": "Trust - PCL",
        "sentiment": 0
    },
    {
        "text": "Our best results on Trust - PCL are able to maintain the stability and solution quality of TRPO while approaching the sample - efficiency of value - based methods ( see e.g. , ) .",
        "aspect": "TRPO",
        "sentiment": 0
    },
    {
        "text": "Our best results on Trust - PCL are able to maintain the stability and solution quality of TRPO while approaching the sample - efficiency of value - based methods ( see e.g. , ) .",
        "aspect": "value - based methods",
        "sentiment": 0
    },
    {
        "text": "This gives hope that the goal of achieving both stability and sample - efficiency without trading - off one for the other is attainable in a single unifying RL algorithm .",
        "aspect": "unifying RL algorithm",
        "sentiment": 0
    },
    {
        "text": "MnasFPN : Learning Latency - aware Pyramid Architecture for Object Detection on Mobile Devices",
        "aspect": "MnasFPN",
        "sentiment": 0
    },
    {
        "text": "MnasFPN : Learning Latency - aware Pyramid Architecture for Object Detection on Mobile Devices",
        "aspect": "Latency - aware Pyramid Architecture",
        "sentiment": 0
    },
    {
        "text": "We propose MnasFPN , a mobile - friendly search space for the detection head , and combine it with latency - aware architecture search to produce efficient object detection models .",
        "aspect": "MnasFPN",
        "sentiment": 2
    },
    {
        "text": "We propose MnasFPN , a mobile - friendly search space for the detection head , and combine it with latency - aware architecture search to produce efficient object detection models .",
        "aspect": "latency - aware architecture search",
        "sentiment": 0
    },
    {
        "text": "The learned MnasFPN head , when paired with MobileNetV2 body , outperforms MobileNetV3+SSDLite by 1.8 mAP at similar latency on Pixel .",
        "aspect": "MnasFPN",
        "sentiment": 0
    },
    {
        "text": "The learned MnasFPN head , when paired with MobileNetV2 body , outperforms MobileNetV3+SSDLite by 1.8 mAP at similar latency on Pixel .",
        "aspect": "MobileNetV2 body",
        "sentiment": 1
    },
    {
        "text": "The learned MnasFPN head , when paired with MobileNetV2 body , outperforms MobileNetV3+SSDLite by 1.8 mAP at similar latency on Pixel .",
        "aspect": "MobileNetV3+SSDLite",
        "sentiment": 3
    },
    {
        "text": "It is both 1 mAP more accurate and 10 % faster than NAS - FPNLite .",
        "aspect": "NAS - FPNLite",
        "sentiment": 3
    },
    {
        "text": "Further explorations reveal an interesting coupling between the search space design and the search algorithm , for which the complexity of MnasFPN search space is opportune 1 .",
        "aspect": "search space design",
        "sentiment": 0
    },
    {
        "text": "Further explorations reveal an interesting coupling between the search space design and the search algorithm , for which the complexity of MnasFPN search space is opportune 1 .",
        "aspect": "search algorithm",
        "sentiment": 0
    },
    {
        "text": "Further explorations reveal an interesting coupling between the search space design and the search algorithm , for which the complexity of MnasFPN search space is opportune 1 .",
        "aspect": "MnasFPN",
        "sentiment": 0
    },
    {
        "text": "Designing neural network architectures for efficient deployment on mobile devices is not an easy task : one has to judiciously trade off the amount of computation with accuracy , while taking into consideration the set of operations that are supported and favored by the devices .",
        "aspect": "neural network architectures",
        "sentiment": 0
    },
    {
        "text": "Neural architecture search ( NAS , ) provides the framework to automate the design process , where a RL controller will learn to generate fast and accuracy models within a user - specified search space .",
        "aspect": "Neural architecture search",
        "sentiment": 0
    },
    {
        "text": "Neural architecture search ( NAS , ) provides the framework to automate the design process , where a RL controller will learn to generate fast and accuracy models within a user - specified search space .",
        "aspect": "NAS",
        "sentiment": 0
    },
    {
        "text": "Neural architecture search ( NAS , ) provides the framework to automate the design process , where a RL controller will learn to generate fast and accuracy models within a user - specified search space .",
        "aspect": "RL controller",
        "sentiment": 0
    },
    {
        "text": "Neural architecture search ( NAS , ) provides the framework to automate the design process , where a RL controller will learn to generate fast and accuracy models within a user - specified search space .",
        "aspect": "accuracy models",
        "sentiment": 0
    },
    {
        "text": "While the focus of NAS papers have been on improving the search algorithm , the search space design remains a critical performance factor that is less visited .",
        "aspect": "search algorithm",
        "sentiment": 0
    },
    {
        "text": "While the focus of NAS papers have been on improving the search algorithm , the search space design remains a critical performance factor that is less visited .",
        "aspect": "search space design",
        "sentiment": 0
    },
    {
        "text": "While the focus of NAS papers have been on improving the search algorithm , the search space design remains a critical performance factor that is less visited .",
        "aspect": "NAS",
        "sentiment": 0
    },
    {
        "text": "Despite the significant advances on NAS for image classification both in the server setting and in the mobile setting , relatively fewer at- Table .",
        "aspect": "NAS",
        "sentiment": 0
    },
    {
        "text": "MnasFPN variations compared with other mobile detection models on COCO test - dev .",
        "aspect": "MnasFPN variations",
        "sentiment": 0
    },
    {
        "text": "MnasFPN variations compared with other mobile detection models on COCO test - dev .",
        "aspect": "mobile detection models",
        "sentiment": 0
    },
    {
        "text": "Latency numbers with ' * ' are remeasured in the same configuration ( same benchmarker binary and same device ) as MnasFPN models to ensure fairness of comparison .",
        "aspect": "MnasFPN models",
        "sentiment": 0
    },
    {
        "text": "Models with \u2020 employs the channel - halving trick .",
        "aspect": "channel - halving trick",
        "sentiment": 0
    },
    {
        "text": "The backbone is a feature extractor that sequentially extracts features at increasingly finer scales , which behaves the same way as the feature extractor for image classification .",
        "aspect": "feature extractor",
        "sentiment": 0
    },
    {
        "text": "The backbone is a feature extractor that sequentially extracts features at increasingly finer scales , which behaves the same way as the feature extractor for image classification .",
        "aspect": "feature extractor",
        "sentiment": 0
    },
    {
        "text": "Therefore , current NAS approaches either repurpose classification feature extractors for detection , or search the backbone while fixing the detection head .",
        "aspect": "NAS",
        "sentiment": 0
    },
    {
        "text": "Therefore , current NAS approaches either repurpose classification feature extractors for detection , or search the backbone while fixing the detection head .",
        "aspect": "classification feature extractors",
        "sentiment": 0
    },
    {
        "text": "This is a challenging task that few NAS frameworks have demonstrated the ability to handle .",
        "aspect": "NAS",
        "sentiment": 0
    },
    {
        "text": "One exception is NAS - FPN , which was the first NAS paper that tackles the non - sequential search space of the detection head .",
        "aspect": "NAS - FPN",
        "sentiment": 0
    },
    {
        "text": "It demonstrates state - of - the - art performance when optimized for accuracy only , and its manually designed variant called NAS - FPNLite performs competitively on mobile devices .",
        "aspect": "NAS - FPNLite",
        "sentiment": 0
    },
    {
        "text": "However , NAS - FPNLite is limited in three aspects .",
        "aspect": "NAS - FPNLite",
        "sentiment": 0
    },
    {
        "text": "1 ) The search process that produces the architecture is not guided by computational complexity or on - device latency ; 2 ) The architecture was manually adapted to work with mobile devices , of which the process may be further optimized ; 3 ) The original NAS - FPN search space was not tailored towards mobile use cases .",
        "aspect": "search process",
        "sentiment": 0
    },
    {
        "text": "1 ) The search process that produces the architecture is not guided by computational complexity or on - device latency ; 2 ) The architecture was manually adapted to work with mobile devices , of which the process may be further optimized ; 3 ) The original NAS - FPN search space was not tailored towards mobile use cases .",
        "aspect": "NAS - FPN search space",
        "sentiment": 0
    },
    {
        "text": "We propose a search space called MnasFPN , which is specifically designed for mobile devices where depthwise convolutions are reasonably optimized .",
        "aspect": "search space",
        "sentiment": 1
    },
    {
        "text": "We propose a search space called MnasFPN , which is specifically designed for mobile devices where depthwise convolutions are reasonably optimized .",
        "aspect": "MnasFPN",
        "sentiment": 2
    },
    {
        "text": "We propose a search space called MnasFPN , which is specifically designed for mobile devices where depthwise convolutions are reasonably optimized .",
        "aspect": "depthwise convolutions",
        "sentiment": 0
    },
    {
        "text": "Our search space re - introduces the inverted residual block , which is proven to be effective for mobile CPUs , into the detection head .",
        "aspect": "search space",
        "sentiment": 0
    },
    {
        "text": "Our search space re - introduces the inverted residual block , which is proven to be effective for mobile CPUs , into the detection head .",
        "aspect": "mobile CPUs",
        "sentiment": 0
    },
    {
        "text": "We conduct NAS on the search space that is guided by on - device latency signals .",
        "aspect": "NAS",
        "sentiment": 1
    },
    {
        "text": "Our contributions include : 1 ) A mobile - specific search space for the detection head ; 2 ) The first attempt to conduct latency - aware search for object detection ; 3 ) A set of detection head architectures that outperform SS - DLite and NAS - FPNLite ; 4 ) Ablation studies showing that our search space design is judiciously chosen for the current NAS controller .",
        "aspect": "detection head architectures",
        "sentiment": 1
    },
    {
        "text": "Our contributions include : 1 ) A mobile - specific search space for the detection head ; 2 ) The first attempt to conduct latency - aware search for object detection ; 3 ) A set of detection head architectures that outperform SS - DLite and NAS - FPNLite ; 4 ) Ablation studies showing that our search space design is judiciously chosen for the current NAS controller .",
        "aspect": "SS - DLite",
        "sentiment": 3
    },
    {
        "text": "Our contributions include : 1 ) A mobile - specific search space for the detection head ; 2 ) The first attempt to conduct latency - aware search for object detection ; 3 ) A set of detection head architectures that outperform SS - DLite and NAS - FPNLite ; 4 ) Ablation studies showing that our search space design is judiciously chosen for the current NAS controller .",
        "aspect": "NAS - FPNLite",
        "sentiment": 3
    },
    {
        "text": "Our contributions include : 1 ) A mobile - specific search space for the detection head ; 2 ) The first attempt to conduct latency - aware search for object detection ; 3 ) A set of detection head architectures that outperform SS - DLite and NAS - FPNLite ; 4 ) Ablation studies showing that our search space design is judiciously chosen for the current NAS controller .",
        "aspect": "search space design",
        "sentiment": 1
    },
    {
        "text": "Our contributions include : 1 ) A mobile - specific search space for the detection head ; 2 ) The first attempt to conduct latency - aware search for object detection ; 3 ) A set of detection head architectures that outperform SS - DLite and NAS - FPNLite ; 4 ) Ablation studies showing that our search space design is judiciously chosen for the current NAS controller .",
        "aspect": "NAS controller",
        "sentiment": 0
    },
    {
        "text": "Additionally , we design the search process and , more importantly , the search space to incorporate knowledge about the targeted platform .",
        "aspect": "search process",
        "sentiment": 1
    },
    {
        "text": "Our proposed MnasFPN search space has two innovations .",
        "aspect": "MnasFPN",
        "sentiment": 2
    },
    {
        "text": "First , MnasFPN incorporates inverted residual blocks into the detection head , which is proven to be favored on mobile CPUs .",
        "aspect": "MnasFPN",
        "sentiment": 0
    },
    {
        "text": "Second , MnasFPN restructured the reshaping and convolution operations in the head to facilitate efficient merging of information across scales .",
        "aspect": "MnasFPN",
        "sentiment": 0
    },
    {
        "text": "On the other hand , further expanding the search space in feature map connectivity seems to overwhelm the NAS framework .",
        "aspect": "NAS framework",
        "sentiment": 0
    },
    {
        "text": "As a result , we conclude that the proposed MnasFPN search space may be close to the capacity of this controller .",
        "aspect": "MnasFPN",
        "sentiment": 2
    },
    {
        "text": "As the controller becomes more powerful , the MnasFPN with connectivity search could become viable again .",
        "aspect": "MnasFPN with connectivity search",
        "sentiment": 0
    },
    {
        "text": "On COCO test - dev MnasFPN leads to a 25 % improvement in non - backbone latency over NAS - FPNLite .",
        "aspect": "MnasFPN",
        "sentiment": 0
    },
    {
        "text": "On COCO test - dev MnasFPN leads to a 25 % improvement in non - backbone latency over NAS - FPNLite .",
        "aspect": "NAS - FPNLite",
        "sentiment": 3
    },
    {
        "text": "For example , the backbone , which currently occupies over 60 % of the total latency , could be searched either conditioning on or jointly with the MnasFPN head .",
        "aspect": "MnasFPN head",
        "sentiment": 0
    },
    {
        "text": "This seems promising with our anecdotal evidence in Table that MnasFPN pairs well with MobileNetV3 and depth - multiplied MobileNetV2 backbones .",
        "aspect": "MnasFPN",
        "sentiment": 0
    },
    {
        "text": "This seems promising with our anecdotal evidence in Table that MnasFPN pairs well with MobileNetV3 and depth - multiplied MobileNetV2 backbones .",
        "aspect": "MobileNetV3",
        "sentiment": 0
    },
    {
        "text": "This seems promising with our anecdotal evidence in Table that MnasFPN pairs well with MobileNetV3 and depth - multiplied MobileNetV2 backbones .",
        "aspect": "depth - multiplied MobileNetV2 backbones",
        "sentiment": 0
    },
    {
        "text": "While the cardinality of a joint - search of backbone and the head is challenging for our current controller , recent one - shot NAS methods are opening avenues for more ambitious search spaces , of which MnasFPN could be an ideal component .",
        "aspect": "joint - search of backbone",
        "sentiment": 0
    },
    {
        "text": "While the cardinality of a joint - search of backbone and the head is challenging for our current controller , recent one - shot NAS methods are opening avenues for more ambitious search spaces , of which MnasFPN could be an ideal component .",
        "aspect": "MnasFPN",
        "sentiment": 0
    },
    {
        "text": "While the cardinality of a joint - search of backbone and the head is challenging for our current controller , recent one - shot NAS methods are opening avenues for more ambitious search spaces , of which MnasFPN could be an ideal component .",
        "aspect": "one - shot NAS",
        "sentiment": 0
    },
    {
        "text": "Pooling Pyramid Network for Object Detection",
        "aspect": "Pooling Pyramid Network",
        "sentiment": 0
    },
    {
        "text": "We 'd like to share a simple tweak of the Single Shot Multibox Detector ( SSD ) family of detectors , which is effective in reducing model size while maintaining the same quality .",
        "aspect": "Single Shot Multibox Detector",
        "sentiment": 2
    },
    {
        "text": "We 'd like to share a simple tweak of the Single Shot Multibox Detector ( SSD ) family of detectors , which is effective in reducing model size while maintaining the same quality .",
        "aspect": "SSD",
        "sentiment": 2
    },
    {
        "text": "We share box predictors across all scales , and replace convolution between scales with max pooling .",
        "aspect": "box predictors",
        "sentiment": 1
    },
    {
        "text": "We share box predictors across all scales , and replace convolution between scales with max pooling .",
        "aspect": "convolution",
        "sentiment": 0
    },
    {
        "text": "We share box predictors across all scales , and replace convolution between scales with max pooling .",
        "aspect": "max pooling",
        "sentiment": 1
    },
    {
        "text": "This has two advantages over vanilla SSD : ( 1 ) it avoids score miscalibration across scales ; ( 2 ) the shared predictor sees the training data over all scales .",
        "aspect": "SSD",
        "sentiment": 0
    },
    {
        "text": "This has two advantages over vanilla SSD : ( 1 ) it avoids score miscalibration across scales ; ( 2 ) the shared predictor sees the training data over all scales .",
        "aspect": "shared predictor",
        "sentiment": 0
    },
    {
        "text": "We empirically show that these changes do not hurt model quality compared to vanilla SSD .",
        "aspect": "SSD",
        "sentiment": 0
    },
    {
        "text": "SSD detectors have been popular as they run fast , are simple to implement and easily portable to different types of hardware .",
        "aspect": "SSD detectors",
        "sentiment": 0
    },
    {
        "text": "Most SSD detectors have several feature maps representing different scales , each of which uses its own predictor to produce boxes and class scores .",
        "aspect": "SSD detectors",
        "sentiment": 0
    },
    {
        "text": "Most SSD detectors have several feature maps representing different scales , each of which uses its own predictor to produce boxes and class scores .",
        "aspect": "feature maps",
        "sentiment": 0
    },
    {
        "text": "We propose simple changes to vanilla SSD : use the same predictor for all scales .",
        "aspect": "SSD",
        "sentiment": 0
    },
    {
        "text": "In order for the predictor to work in the same feature space , we replace convolutions between feature maps with max pooling .",
        "aspect": "convolutions between feature maps",
        "sentiment": 3
    },
    {
        "text": "In order for the predictor to work in the same feature space , we replace convolutions between feature maps with max pooling .",
        "aspect": "max pooling",
        "sentiment": 1
    },
    {
        "text": "We run experiments on the COCO detection dataset and compare the performance of PPN with vanilla SSD .",
        "aspect": "PPN",
        "sentiment": 2
    },
    {
        "text": "We run experiments on the COCO detection dataset and compare the performance of PPN with vanilla SSD .",
        "aspect": "vanilla SSD",
        "sentiment": 3
    },
    {
        "text": "We use MobileNet v1 as the backbone network and set the input resolution to be 300 \u00d7 300 .",
        "aspect": "MobileNet v1",
        "sentiment": 1
    },
    {
        "text": "Both models use the standard implementation of MobileNet - v1 SSD in the Tensorflow Object Detection API .",
        "aspect": "MobileNet - v1 SSD",
        "sentiment": 0
    },
    {
        "text": "For PPN , we extract the layer Conv2d 11 pointwise as the base feature map , from which we build 6 pooled feature maps that are of sizes 19\u00d719 , 10\u00d710 , 5\u00d75 , 3\u00d73 , 2\u00d72 , and 1\u00d71 .",
        "aspect": "PPN",
        "sentiment": 0
    },
    {
        "text": "A shared 1\u00d71 depth 512 convolution is applied before the box classifier and location regressor .",
        "aspect": "box classifier",
        "sentiment": 0
    },
    {
        "text": "A shared 1\u00d71 depth 512 convolution is applied before the box classifier and location regressor .",
        "aspect": "location regressor",
        "sentiment": 0
    },
    {
        "text": "We use the same anchor design as SSD , smooth l 1 loss for box regression , and focal loss with \u03b1 = 0.25 and \u03b3 = 2 for box classification .",
        "aspect": "anchor design",
        "sentiment": 0
    },
    {
        "text": "We use the same anchor design as SSD , smooth l 1 loss for box regression , and focal loss with \u03b1 = 0.25 and \u03b3 = 2 for box classification .",
        "aspect": "SSD",
        "sentiment": 0
    },
    {
        "text": "Both SSD and PPN models are initialized using a MobileNet - v1 checkpoint that is pre - trained on ImageNet , and both of them are trained and tested on the splits described in .",
        "aspect": "SSD",
        "sentiment": 0
    },
    {
        "text": "Both SSD and PPN models are initialized using a MobileNet - v1 checkpoint that is pre - trained on ImageNet , and both of them are trained and tested on the splits described in .",
        "aspect": "PPN models",
        "sentiment": 0
    },
    {
        "text": "Both SSD and PPN models are initialized using a MobileNet - v1 checkpoint that is pre - trained on ImageNet , and both of them are trained and tested on the splits described in .",
        "aspect": "MobileNet - v1 checkpoint",
        "sentiment": 0
    },
    {
        "text": "We leverage TPUs",
        "aspect": "TPUs",
        "sentiment": 0
    },
    {
        "text": "Progressive Neural Architecture Search",
        "aspect": "Progressive Neural Architecture Search",
        "sentiment": 0
    },
    {
        "text": "We propose a new method for learning the structure of convolutional neural networks ( CNNs ) that is more efficient than recent state - of - the - art methods based on reinforcement learning and evolutionary algorithms .",
        "aspect": "structure of convolutional neural networks",
        "sentiment": 1
    },
    {
        "text": "We propose a new method for learning the structure of convolutional neural networks ( CNNs ) that is more efficient than recent state - of - the - art methods based on reinforcement learning and evolutionary algorithms .",
        "aspect": "CNNs",
        "sentiment": 1
    },
    {
        "text": "We propose a new method for learning the structure of convolutional neural networks ( CNNs ) that is more efficient than recent state - of - the - art methods based on reinforcement learning and evolutionary algorithms .",
        "aspect": "reinforcement learning",
        "sentiment": 1
    },
    {
        "text": "We propose a new method for learning the structure of convolutional neural networks ( CNNs ) that is more efficient than recent state - of - the - art methods based on reinforcement learning and evolutionary algorithms .",
        "aspect": "evolutionary algorithms",
        "sentiment": 1
    },
    {
        "text": "Our approach uses a sequential model - based optimization ( SMBO ) strategy , in which we search for structures in order of increasing complexity , while simultaneously learning a surrogate model to guide the search through structure space .",
        "aspect": "sequential model - based optimization",
        "sentiment": 1
    },
    {
        "text": "Our approach uses a sequential model - based optimization ( SMBO ) strategy , in which we search for structures in order of increasing complexity , while simultaneously learning a surrogate model to guide the search through structure space .",
        "aspect": "SMBO",
        "sentiment": 1
    },
    {
        "text": "Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of",
        "aspect": "RL method",
        "sentiment": 0
    },
    {
        "text": "There has been a lot of recent interest in automatically learning good neural net architectures .",
        "aspect": "neural net architectures",
        "sentiment": 0
    },
    {
        "text": "Some of this work is summarized in Section 2 , but at a high level , current techniques usually fall into one of two categories : evolutionary algorithms ( see e.g.",
        "aspect": "evolutionary algorithms",
        "sentiment": 0
    },
    {
        "text": "When using evolutionary algorithms ( EA ) , each neural network structure is encoded as a string , and random mutations and recombinations of the strings are performed during the search process ; each string ( model ) is then trained and evaluated on a validation set , and the top performing models generate \" children \" .",
        "aspect": "evolutionary algorithms",
        "sentiment": 0
    },
    {
        "text": "When using evolutionary algorithms ( EA ) , each neural network structure is encoded as a string , and random mutations and recombinations of the strings are performed during the search process ; each string ( model ) is then trained and evaluated on a validation set , and the top performing models generate \" children \" .",
        "aspect": "EA",
        "sentiment": 0
    },
    {
        "text": "When using evolutionary algorithms ( EA ) , each neural network structure is encoded as a string , and random mutations and recombinations of the strings are performed during the search process ; each string ( model ) is then trained and evaluated on a validation set , and the top performing models generate \" children \" .",
        "aspect": "neural network structure",
        "sentiment": 0
    },
    {
        "text": "When using evolutionary algorithms ( EA ) , each neural network structure is encoded as a string , and random mutations and recombinations of the strings are performed during the search process ; each string ( model ) is then trained and evaluated on a validation set , and the top performing models generate \" children \" .",
        "aspect": "search process",
        "sentiment": 0
    },
    {
        "text": "When using reinforcement learning ( RL ) , the agent performs a sequence of actions , which specifies the structure of the model ; this model is then trained and its validation performance is returned as the reward , which is used to update the RNN controller .",
        "aspect": "reinforcement learning",
        "sentiment": 0
    },
    {
        "text": "When using reinforcement learning ( RL ) , the agent performs a sequence of actions , which specifies the structure of the model ; this model is then trained and its validation performance is returned as the reward , which is used to update the RNN controller .",
        "aspect": "RL",
        "sentiment": 0
    },
    {
        "text": "When using reinforcement learning ( RL ) , the agent performs a sequence of actions , which specifies the structure of the model ; this model is then trained and its validation performance is returned as the reward , which is used to update the RNN controller .",
        "aspect": "RNN controller",
        "sentiment": 0
    },
    {
        "text": "Although both EA and RL methods have been able to learn network structures that outperform manually designed architectures , they require significant computational resources .",
        "aspect": "EA",
        "sentiment": 0
    },
    {
        "text": "Although both EA and RL methods have been able to learn network structures that outperform manually designed architectures , they require significant computational resources .",
        "aspect": "RL methods",
        "sentiment": 0
    },
    {
        "text": "Although both EA and RL methods have been able to learn network structures that outperform manually designed architectures , they require significant computational resources .",
        "aspect": "network structures",
        "sentiment": 0
    },
    {
        "text": "For example , the RL method in trains and evaluates 20,000 neural networks across 500 P100 GPUs over 4 days .",
        "aspect": "RL method",
        "sentiment": 0
    },
    {
        "text": "For example , the RL method in trains and evaluates 20,000 neural networks across 500 P100 GPUs over 4 days .",
        "aspect": "neural networks",
        "sentiment": 0
    },
    {
        "text": "In this paper , we describe a method that is able to learn a CNN which matches previous state of the art in terms of accuracy , while requiring 5 times fewer model evaluations during the architecture search .",
        "aspect": "CNN",
        "sentiment": 1
    },
    {
        "text": "[ cs . CV ] 26 Jul 2018 structured search space proposed by , in which the search algorithm is tasked with searching for a good convolutional \" cell \" , as opposed to a full CNN .",
        "aspect": "search algorithm",
        "sentiment": 0
    },
    {
        "text": "[ cs . CV ] 26 Jul 2018 structured search space proposed by , in which the search algorithm is tasked with searching for a good convolutional \" cell \" , as opposed to a full CNN .",
        "aspect": "full CNN",
        "sentiment": 0
    },
    {
        "text": "A cell contains B \" blocks \" , where a block is a combination operator ( such as addition ) applied to two inputs ( tensors ) , each of which can be transformed ( e.g. , using convolution ) before being combined .",
        "aspect": "convolution",
        "sentiment": 0
    },
    {
        "text": "This cell structure is then stacked a certain number of times , depending on the size of the training set , and the desired running time of the final CNN ( see Section 3 for details ) .",
        "aspect": "CNN",
        "sentiment": 0
    },
    {
        "text": "We propose to use heuristic search to search the space of cell structures , starting with simple ( shallow ) models and progressing to complex ones , pruning out unpromising structures as we go .",
        "aspect": "heuristic search",
        "sentiment": 0
    },
    {
        "text": "Since this process is expensive , we also learn a model or surrogate function which can predict the performance of a structure without needing to training it .",
        "aspect": "surrogate function",
        "sentiment": 0
    },
    {
        "text": "We apply our surrogate function to rank all of the K children , pick the top K , and then train and evaluate them .",
        "aspect": "surrogate function",
        "sentiment": 0
    },
    {
        "text": "In Section 5 we show that our approach is 5 times more efficient than the RL method of in terms of number of models evaluated , and 8 times faster in terms of total compute .",
        "aspect": "RL method",
        "sentiment": 3
    },
    {
        "text": "Our paper is based on the \" neural architecture search \" ( NAS ) method proposed in .",
        "aspect": "neural architecture search \"",
        "sentiment": 1
    },
    {
        "text": "Our paper is based on the \" neural architecture search \" ( NAS ) method proposed in .",
        "aspect": "NAS",
        "sentiment": 1
    },
    {
        "text": "In the original paper , they use the REINFORCE algorithm to estimate the parameters of a recurrent neural network ( RNN ) , which represents a policy to generate a sequence of symbols ( actions ) specifying the structure of the CNN ; the reward function is the classification accuracy on the validation set of a CNN generated from this sequence .",
        "aspect": "REINFORCE algorithm",
        "sentiment": 0
    },
    {
        "text": "In the original paper , they use the REINFORCE algorithm to estimate the parameters of a recurrent neural network ( RNN ) , which represents a policy to generate a sequence of symbols ( actions ) specifying the structure of the CNN ; the reward function is the classification accuracy on the validation set of a CNN generated from this sequence .",
        "aspect": "recurrent neural network",
        "sentiment": 0
    },
    {
        "text": "In the original paper , they use the REINFORCE algorithm to estimate the parameters of a recurrent neural network ( RNN ) , which represents a policy to generate a sequence of symbols ( actions ) specifying the structure of the CNN ; the reward function is the classification accuracy on the validation set of a CNN generated from this sequence .",
        "aspect": "RNN",
        "sentiment": 0
    },
    {
        "text": "In the original paper , they use the REINFORCE algorithm to estimate the parameters of a recurrent neural network ( RNN ) , which represents a policy to generate a sequence of symbols ( actions ) specifying the structure of the CNN ; the reward function is the classification accuracy on the validation set of a CNN generated from this sequence .",
        "aspect": "CNN",
        "sentiment": 0
    },
    {
        "text": "In the original paper , they use the REINFORCE algorithm to estimate the parameters of a recurrent neural network ( RNN ) , which represents a policy to generate a sequence of symbols ( actions ) specifying the structure of the CNN ; the reward function is the classification accuracy on the validation set of a CNN generated from this sequence .",
        "aspect": "CNN",
        "sentiment": 0
    },
    {
        "text": "( They also replaced REINFORCE with proximal policy optimization ( PPO ) . )",
        "aspect": "REINFORCE",
        "sentiment": 0
    },
    {
        "text": "( They also replaced REINFORCE with proximal policy optimization ( PPO ) . )",
        "aspect": "proximal policy optimization",
        "sentiment": 0
    },
    {
        "text": "( They also replaced REINFORCE with proximal policy optimization ( PPO ) . )",
        "aspect": "PPO",
        "sentiment": 0
    },
    {
        "text": "This method was able to learn CNNs which outperformed almost all previous methods in terms of accuracy vs speed on image classification ( using CIFAR-10 and ImageNet ) and object detection ( using COCO ) .",
        "aspect": "CNNs",
        "sentiment": 0
    },
    {
        "text": "There are several other papers that use RL to learn network structures .",
        "aspect": "RL",
        "sentiment": 0
    },
    {
        "text": "The same approach , of applying \" network morphisms \" to modify a network , was used in , but in the context of hill climbing search , rather than RL .",
        "aspect": "network morphisms",
        "sentiment": 0
    },
    {
        "text": "The same approach , of applying \" network morphisms \" to modify a network , was used in , but in the context of hill climbing search , rather than RL .",
        "aspect": "hill climbing search",
        "sentiment": 0
    },
    {
        "text": "The same approach , of applying \" network morphisms \" to modify a network , was used in , but in the context of hill climbing search , rather than RL .",
        "aspect": "RL",
        "sentiment": 0
    },
    {
        "text": "An alternative to RL is to use evolutionary algorithms ( EA ; \" neuro - evolution \" ) .",
        "aspect": "RL",
        "sentiment": 0
    },
    {
        "text": "An alternative to RL is to use evolutionary algorithms ( EA ; \" neuro - evolution \" ) .",
        "aspect": "evolutionary algorithms",
        "sentiment": 0
    },
    {
        "text": "An alternative to RL is to use evolutionary algorithms ( EA ; \" neuro - evolution \" ) .",
        "aspect": "EA",
        "sentiment": 0
    },
    {
        "text": "An alternative to RL is to use evolutionary algorithms ( EA ; \" neuro - evolution \" ) .",
        "aspect": "neuro - evolution",
        "sentiment": 0
    },
    {
        "text": "Early work ( e.g. , ) used EA to learn both the structure and the parameters of the network , but more recent methods , such as , just use EA to search the structures , and use SGD to estimate the parameters .",
        "aspect": "EA",
        "sentiment": 0
    },
    {
        "text": "Early work ( e.g. , ) used EA to learn both the structure and the parameters of the network , but more recent methods , such as , just use EA to search the structures , and use SGD to estimate the parameters .",
        "aspect": "EA",
        "sentiment": 0
    },
    {
        "text": "Early work ( e.g. , ) used EA to learn both the structure and the parameters of the network , but more recent methods , such as , just use EA to search the structures , and use SGD to estimate the parameters .",
        "aspect": "SGD",
        "sentiment": 0
    },
    {
        "text": "RL and EA are local search methods that search through the space of fullyspecified graph structures .",
        "aspect": "RL",
        "sentiment": 0
    },
    {
        "text": "RL and EA are local search methods that search through the space of fullyspecified graph structures .",
        "aspect": "EA",
        "sentiment": 0
    },
    {
        "text": "RL and EA are local search methods that search through the space of fullyspecified graph structures .",
        "aspect": "local search methods",
        "sentiment": 0
    },
    {
        "text": "An alternative approach , which we adopt , is to use heuristic search , in which we search through the space of structures in a progressive way , from simple to complex .",
        "aspect": "heuristic search",
        "sentiment": 0
    },
    {
        "text": "Sequential Model Based Optimization ( SMBO ) improves on MCTS by learning a predictive model , which can be used to decide which nodes to expand .",
        "aspect": "Sequential Model Based Optimization",
        "sentiment": 0
    },
    {
        "text": "Sequential Model Based Optimization ( SMBO ) improves on MCTS by learning a predictive model , which can be used to decide which nodes to expand .",
        "aspect": "SMBO",
        "sentiment": 0
    },
    {
        "text": "Sequential Model Based Optimization ( SMBO ) improves on MCTS by learning a predictive model , which can be used to decide which nodes to expand .",
        "aspect": "MCTS",
        "sentiment": 0
    },
    {
        "text": "Sequential Model Based Optimization ( SMBO ) improves on MCTS by learning a predictive model , which can be used to decide which nodes to expand .",
        "aspect": "predictive model",
        "sentiment": 0
    },
    {
        "text": "This technique has been applied to neural net structure search in , but they used a flat CNN search space , rather than our hierarchical cell - based space .",
        "aspect": "hierarchical cell - based space",
        "sentiment": 1
    },
    {
        "text": "This technique has been applied to neural net structure search in , but they used a flat CNN search space , rather than our hierarchical cell - based space .",
        "aspect": "flat CNN search space",
        "sentiment": 3
    },
    {
        "text": "Consequently , their resulting CNNs do not perform very well .",
        "aspect": "CNNs",
        "sentiment": 0
    },
    {
        "text": "Other related works include , who focus on MLP rather than CNNs ; , who used an incremental approach in the context of evolutionary algorithms ; who used a schedule of increasing number of layers ; and who search through the space of latent factor models specified by a grammar .",
        "aspect": "MLP",
        "sentiment": 0
    },
    {
        "text": "Other related works include , who focus on MLP rather than CNNs ; , who used an incremental approach in the context of evolutionary algorithms ; who used a schedule of increasing number of layers ; and who search through the space of latent factor models specified by a grammar .",
        "aspect": "CNNs",
        "sentiment": 0
    },
    {
        "text": "Other related works include , who focus on MLP rather than CNNs ; , who used an incremental approach in the context of evolutionary algorithms ; who used a schedule of increasing number of layers ; and who search through the space of latent factor models specified by a grammar .",
        "aspect": "incremental approach",
        "sentiment": 0
    },
    {
        "text": "Other related works include , who focus on MLP rather than CNNs ; , who used an incremental approach in the context of evolutionary algorithms ; who used a schedule of increasing number of layers ; and who search through the space of latent factor models specified by a grammar .",
        "aspect": "evolutionary algorithms",
        "sentiment": 0
    },
    {
        "text": "Other related works include , who focus on MLP rather than CNNs ; , who used an incremental approach in the context of evolutionary algorithms ; who used a schedule of increasing number of layers ; and who search through the space of latent factor models specified by a grammar .",
        "aspect": "latent factor models",
        "sentiment": 0
    },
    {
        "text": "Finally , grow CNNs sequentially using boosting .",
        "aspect": "CNNs",
        "sentiment": 0
    },
    {
        "text": "Several other papers learn a surrogate function to predict the performance of a candidate structure , either \" zero shot \" ( without training it ) ( see e.g. , ) , or after training it for a small number of epochs and extrapolating the learning curve ( see e.g. , ) .",
        "aspect": "surrogate function",
        "sentiment": 0
    },
    {
        "text": "Several other papers learn a surrogate function to predict the performance of a candidate structure , either \" zero shot \" ( without training it ) ( see e.g. , ) , or after training it for a small number of epochs and extrapolating the learning curve ( see e.g. , ) .",
        "aspect": "zero shot",
        "sentiment": 0
    },
    {
        "text": "However , most of these methods have been applied to fixed sized structures , and would not work with our progressive search approach .",
        "aspect": "progressive search approach",
        "sentiment": 0
    },
    {
        "text": "Random horizontal flip is also used .",
        "aspect": "Random horizontal flip",
        "sentiment": 1
    },
    {
        "text": "For the MLP accuracy predictor , the embedding size is 100 , and we use 2 fully connected layers , each with 100 hidden units .",
        "aspect": "MLP accuracy predictor",
        "sentiment": 0
    },
    {
        "text": "For the MLP accuracy predictor , the embedding size is 100 , and we use 2 fully connected layers , each with 100 hidden units .",
        "aspect": "fully connected layers",
        "sentiment": 0
    },
    {
        "text": "For the RNN accuracy predictor , we use an LSTM , and the hidden state size and embedding size are both 100 .",
        "aspect": "RNN accuracy predictor",
        "sentiment": 0
    },
    {
        "text": "For the RNN accuracy predictor , we use an LSTM , and the hidden state size and embedding size are both 100 .",
        "aspect": "LSTM",
        "sentiment": 0
    },
    {
        "text": "The bias term in the final fully connected layer is initialized to 1.8 ( 0.86 after sigmoid ) to account for the mean observed accuracy of all b = 1 models .",
        "aspect": "fully connected layer",
        "sentiment": 0
    },
    {
        "text": "We use the Adam optimizer with learning rate 0.01 for the b = 1 level and 0.002 for all following levels .",
        "aspect": "Adam optimizer",
        "sentiment": 1
    },
    {
        "text": "Our training procedure for the CNNs follows the one used in .",
        "aspect": "CNNs",
        "sentiment": 0
    },
    {
        "text": "Let PNASNet-5 denote the best CNN we discovered on CIFAR using PNAS , also visualized in Figure ( left ) .",
        "aspect": "PNASNet-5",
        "sentiment": 0
    },
    {
        "text": "Let PNASNet-5 denote the best CNN we discovered on CIFAR using PNAS , also visualized in Figure ( left ) .",
        "aspect": "CNN",
        "sentiment": 0
    },
    {
        "text": "Let PNASNet-5 denote the best CNN we discovered on CIFAR using PNAS , also visualized in Figure ( left ) .",
        "aspect": "PNAS",
        "sentiment": 0
    },
    {
        "text": "We see that PNAS can find a model with the same accuracy as NAS , but using 21 times less compute .",
        "aspect": "PNAS",
        "sentiment": 0
    },
    {
        "text": "We see that PNAS can find a model with the same accuracy as NAS , but using 21 times less compute .",
        "aspect": "NAS",
        "sentiment": 3
    },
    {
        "text": "PNAS also outperforms the Hierarchical EA method of , while using 36 times less compute .",
        "aspect": "PNAS",
        "sentiment": 0
    },
    {
        "text": "PNAS also outperforms the Hierarchical EA method of , while using 36 times less compute .",
        "aspect": "Hierarchical EA method",
        "sentiment": 3
    },
    {
        "text": "Though the the EA method called \" AmoebaNets \" currently give the highest accuracies ( at the time of writing ) , it also requires the most compute , taking 63 times more resources than PNAS .",
        "aspect": "EA method",
        "sentiment": 0
    },
    {
        "text": "Though the the EA method called \" AmoebaNets \" currently give the highest accuracies ( at the time of writing ) , it also requires the most compute , taking 63 times more resources than PNAS .",
        "aspect": "AmoebaNets \"",
        "sentiment": 3
    },
    {
        "text": "Though the the EA method called \" AmoebaNets \" currently give the highest accuracies ( at the time of writing ) , it also requires the most compute , taking 63 times more resources than PNAS .",
        "aspect": "PNAS",
        "sentiment": 0
    },
    {
        "text": "By contrast , in Section 5.3 , we fix the search space for NAS and PNAS , to make the speedup comparison fair .",
        "aspect": "NAS",
        "sentiment": 0
    },
    {
        "text": "By contrast , in Section 5.3 , we fix the search space for NAS and PNAS , to make the speedup comparison fair .",
        "aspect": "PNAS",
        "sentiment": 0
    },
    {
        "text": "To compare the performance of PNASNet-5 to the results in other papers , we conduct experiments under two settings :",
        "aspect": "PNASNet-5",
        "sentiment": 0
    },
    {
        "text": "-Mobile : Here we restrain the representation power of the CNN .",
        "aspect": "CNN",
        "sentiment": 0
    },
    {
        "text": "-Large : Here we compare PNASNet-5 against the state - of - the - art models on ImageNet .",
        "aspect": "PNASNet-5",
        "sentiment": 0
    },
    {
        "text": "In both experiments we use RMSProp optimizer , label smoothing of 0.1 , auxiliary classifier located at 2/3 of the maximum depth weighted by 0.4 , weight decay of 4e-5 , and dropout of 0.5 in the final softmax layer .",
        "aspect": "RMSProp optimizer",
        "sentiment": 1
    },
    {
        "text": "In both experiments we use RMSProp optimizer , label smoothing of 0.1 , auxiliary classifier located at 2/3 of the maximum depth weighted by 0.4 , weight decay of 4e-5 , and dropout of 0.5 in the final softmax layer .",
        "aspect": "softmax layer",
        "sentiment": 0
    },
    {
        "text": "In the Mobile setting , we use distributed synchronous SGD with 50 P100 workers .",
        "aspect": "distributed synchronous SGD",
        "sentiment": 1
    },
    {
        "text": "The results of the Mobile setting are summarized in Table . PNASNet-5 achieves slightly better performance than NASNet - A ( 74.2 % top-1 accuracy for PNAS vs 74.0 % for NASNet - A ) .",
        "aspect": "PNASNet-5",
        "sentiment": 0
    },
    {
        "text": "The results of the Mobile setting are summarized in Table . PNASNet-5 achieves slightly better performance than NASNet - A ( 74.2 % top-1 accuracy for PNAS vs 74.0 % for NASNet - A ) .",
        "aspect": "NASNet - A",
        "sentiment": 0
    },
    {
        "text": "The results of the Mobile setting are summarized in Table . PNASNet-5 achieves slightly better performance than NASNet - A ( 74.2 % top-1 accuracy for PNAS vs 74.0 % for NASNet - A ) .",
        "aspect": "PNAS",
        "sentiment": 0
    },
    {
        "text": "The results of the Mobile setting are summarized in Table . PNASNet-5 achieves slightly better performance than NASNet - A ( 74.2 % top-1 accuracy for PNAS vs 74.0 % for NASNet - A ) .",
        "aspect": "NASNet - A",
        "sentiment": 0
    },
    {
        "text": "Both methods significantly surpass the previous state - of - the - art , which includes the manually designed MobileNet ( 70.6 % ) and ShuffleNet ( 70.9 % ) .",
        "aspect": "MobileNet",
        "sentiment": 0
    },
    {
        "text": "Both methods significantly surpass the previous state - of - the - art , which includes the manually designed MobileNet ( 70.6 % ) and ShuffleNet ( 70.9 % ) .",
        "aspect": "ShuffleNet",
        "sentiment": 0
    },
    {
        "text": "AmoebaNet - C performs the best , but note that this is a different model than their best - performing CIFAR-10 model .",
        "aspect": "AmoebaNet - C",
        "sentiment": 0
    },
    {
        "text": "Table shows that under the Large setting , PNASNet-5 achieves higher performance ( 82.9 % top-1 ; 96.2 % top-5 ) than previous state - of - the - art approaches , including SENet , NASNet - A , and AmoebaNets under the same model capacity .",
        "aspect": "PNASNet-5",
        "sentiment": 0
    },
    {
        "text": "Table shows that under the Large setting , PNASNet-5 achieves higher performance ( 82.9 % top-1 ; 96.2 % top-5 ) than previous state - of - the - art approaches , including SENet , NASNet - A , and AmoebaNets under the same model capacity .",
        "aspect": "SENet",
        "sentiment": 0
    },
    {
        "text": "Table shows that under the Large setting , PNASNet-5 achieves higher performance ( 82.9 % top-1 ; 96.2 % top-5 ) than previous state - of - the - art approaches , including SENet , NASNet - A , and AmoebaNets under the same model capacity .",
        "aspect": "NASNet - A",
        "sentiment": 0
    },
    {
        "text": "Table shows that under the Large setting , PNASNet-5 achieves higher performance ( 82.9 % top-1 ; 96.2 % top-5 ) than previous state - of - the - art approaches , including SENet , NASNet - A , and AmoebaNets under the same model capacity .",
        "aspect": "AmoebaNets",
        "sentiment": 0
    },
    {
        "text": "The main contribution of this work is to show how we can accelerate the search for good CNN structures by using progressive search through the space of increasingly complex graphs , combined with a learned prediction function to efficiently identify the most promising models to explore .",
        "aspect": "CNN structures",
        "sentiment": 0
    },
    {
        "text": "The main contribution of this work is to show how we can accelerate the search for good CNN structures by using progressive search through the space of increasingly complex graphs , combined with a learned prediction function to efficiently identify the most promising models to explore .",
        "aspect": "progressive search",
        "sentiment": 1
    },
    {
        "text": "The main contribution of this work is to show how we can accelerate the search for good CNN structures by using progressive search through the space of increasingly complex graphs , combined with a learned prediction function to efficiently identify the most promising models to explore .",
        "aspect": "prediction function",
        "sentiment": 0
    },
    {
        "text": "There are many possible directions for future work , including : the use of better surrogate predictors , such as Gaussian processes with string kernels ; the use of model - based early stopping , such as , so we can stop the training of \" unpromising \" models before reaching E 1 epochs ; the use of \" warm starting \" , to initialize the training of a larger b + 1 - sized model from its smaller parent ; the use of Bayesian optimization , in which we use an acquisition function , such as expected improvement or upper confidence bound , to rank the candidate models , rather than greedily picking the top K ( see e.g. , ) ; adaptively varying the number of models K evaluated at each step ( e.g. , reducing it over time ) ; the automatic exploration of speed - accuracy tradeoffs ( cf . , ) , etc .",
        "aspect": "surrogate predictors",
        "sentiment": 0
    },
    {
        "text": "There are many possible directions for future work , including : the use of better surrogate predictors , such as Gaussian processes with string kernels ; the use of model - based early stopping , such as , so we can stop the training of \" unpromising \" models before reaching E 1 epochs ; the use of \" warm starting \" , to initialize the training of a larger b + 1 - sized model from its smaller parent ; the use of Bayesian optimization , in which we use an acquisition function , such as expected improvement or upper confidence bound , to rank the candidate models , rather than greedily picking the top K ( see e.g. , ) ; adaptively varying the number of models K evaluated at each step ( e.g. , reducing it over time ) ; the automatic exploration of speed - accuracy tradeoffs ( cf . , ) , etc .",
        "aspect": "Gaussian processes",
        "sentiment": 0
    },
    {
        "text": "There are many possible directions for future work , including : the use of better surrogate predictors , such as Gaussian processes with string kernels ; the use of model - based early stopping , such as , so we can stop the training of \" unpromising \" models before reaching E 1 epochs ; the use of \" warm starting \" , to initialize the training of a larger b + 1 - sized model from its smaller parent ; the use of Bayesian optimization , in which we use an acquisition function , such as expected improvement or upper confidence bound , to rank the candidate models , rather than greedily picking the top K ( see e.g. , ) ; adaptively varying the number of models K evaluated at each step ( e.g. , reducing it over time ) ; the automatic exploration of speed - accuracy tradeoffs ( cf . , ) , etc .",
        "aspect": "string kernels",
        "sentiment": 0
    },
    {
        "text": "There are many possible directions for future work , including : the use of better surrogate predictors , such as Gaussian processes with string kernels ; the use of model - based early stopping , such as , so we can stop the training of \" unpromising \" models before reaching E 1 epochs ; the use of \" warm starting \" , to initialize the training of a larger b + 1 - sized model from its smaller parent ; the use of Bayesian optimization , in which we use an acquisition function , such as expected improvement or upper confidence bound , to rank the candidate models , rather than greedily picking the top K ( see e.g. , ) ; adaptively varying the number of models K evaluated at each step ( e.g. , reducing it over time ) ; the automatic exploration of speed - accuracy tradeoffs ( cf . , ) , etc .",
        "aspect": "model - based early stopping",
        "sentiment": 0
    },
    {
        "text": "There are many possible directions for future work , including : the use of better surrogate predictors , such as Gaussian processes with string kernels ; the use of model - based early stopping , such as , so we can stop the training of \" unpromising \" models before reaching E 1 epochs ; the use of \" warm starting \" , to initialize the training of a larger b + 1 - sized model from its smaller parent ; the use of Bayesian optimization , in which we use an acquisition function , such as expected improvement or upper confidence bound , to rank the candidate models , rather than greedily picking the top K ( see e.g. , ) ; adaptively varying the number of models K evaluated at each step ( e.g. , reducing it over time ) ; the automatic exploration of speed - accuracy tradeoffs ( cf . , ) , etc .",
        "aspect": "Bayesian optimization",
        "sentiment": 0
    },
    {
        "text": "Learning to Reason : End - to - End Module Networks for Visual Question Answering",
        "aspect": "End - to - End Module Networks",
        "sentiment": 0
    },
    {
        "text": "The recently proposed Neural Module Network ( NMN ) architecture",
        "aspect": "Neural Module Network",
        "sentiment": 0
    },
    {
        "text": "The recently proposed Neural Module Network ( NMN ) architecture",
        "aspect": "NMN",
        "sentiment": 0
    },
    {
        "text": "However , existing NMN implementations rely on brittle off - the - shelf parsers , and are restricted to the module configurations proposed by these parsers rather than learning them from data .",
        "aspect": "NMN implementations",
        "sentiment": 0
    },
    {
        "text": "However , existing NMN implementations rely on brittle off - the - shelf parsers , and are restricted to the module configurations proposed by these parsers rather than learning them from data .",
        "aspect": "brittle off - the - shelf parsers",
        "sentiment": 0
    },
    {
        "text": "In this paper , we propose End - to - End Module Networks ( N2NMNs ) , which learn to reason by directly predicting instance - specific network layouts without the aid of a parser .",
        "aspect": "End - to - End Module Networks",
        "sentiment": 2
    },
    {
        "text": "In this paper , we propose End - to - End Module Networks ( N2NMNs ) , which learn to reason by directly predicting instance - specific network layouts without the aid of a parser .",
        "aspect": "N2NMNs",
        "sentiment": 2
    },
    {
        "text": "Experimental results on the new CLEVR dataset targeted at compositional question answering show that N2NMNs achieve an error reduction of nearly 50 % relative to state - of - theart attentional approaches , while discovering interpretable network architectures specialized for each question .",
        "aspect": "N2NMNs",
        "sentiment": 0
    },
    {
        "text": "Experimental results on the new CLEVR dataset targeted at compositional question answering show that N2NMNs achieve an error reduction of nearly 50 % relative to state - of - theart attentional approaches , while discovering interpretable network architectures specialized for each question .",
        "aspect": "attentional approaches",
        "sentiment": 0
    },
    {
        "text": "Experimental results on the new CLEVR dataset targeted at compositional question answering show that N2NMNs achieve an error reduction of nearly 50 % relative to state - of - theart attentional approaches , while discovering interpretable network architectures specialized for each question .",
        "aspect": "interpretable network architectures",
        "sentiment": 0
    },
    {
        "text": "This comprehension often depends on compositional reasoning , for example locating multiple objects in a scene and inspecting their properties or comparing them to one another ( Figure ) .",
        "aspect": "compositional reasoning",
        "sentiment": 0
    },
    {
        "text": "While conventional deep networks have shown promising VQA performance , there is limited evidence that they are capable of explicit compositional reasoning .",
        "aspect": "deep networks",
        "sentiment": 0
    },
    {
        "text": "While conventional deep networks have shown promising VQA performance , there is limited evidence that they are capable of explicit compositional reasoning .",
        "aspect": "compositional reasoning",
        "sentiment": 0
    },
    {
        "text": "Figure : For each instance , our model predicts a computational expression and a sequence of attentive module parameterizations .",
        "aspect": "attentive module parameterizations",
        "sentiment": 0
    },
    {
        "text": "It uses these to assemble a concrete network architecture , and then executes the assembled neural module network to output an answer for visual question answering .",
        "aspect": "network architecture",
        "sentiment": 0
    },
    {
        "text": "It uses these to assemble a concrete network architecture , and then executes the assembled neural module network to output an answer for visual question answering .",
        "aspect": "neural module network",
        "sentiment": 0
    },
    {
        "text": "Additionally , they rely on the same non - modular network structure for all input questions .",
        "aspect": "non - modular network structure",
        "sentiment": 0
    },
    {
        "text": "In this paper , we propose End - to - End Module Networks ( N2NMNs ): a class of models capable of predicting novel modular network architectures directly from textual input and applying them to images in order to solve question answering tasks .",
        "aspect": "End - to - End Module Networks",
        "sentiment": 2
    },
    {
        "text": "In this paper , we propose End - to - End Module Networks ( N2NMNs ): a class of models capable of predicting novel modular network architectures directly from textual input and applying them to images in order to solve question answering tasks .",
        "aspect": "N2NMNs",
        "sentiment": 2
    },
    {
        "text": "In this paper , we propose End - to - End Module Networks ( N2NMNs ): a class of models capable of predicting novel modular network architectures directly from textual input and applying them to images in order to solve question answering tasks .",
        "aspect": "modular network architectures",
        "sentiment": 0
    },
    {
        "text": "The present work synthesizes and extends two recent modular architectures for visual problem solving .",
        "aspect": "modular architectures",
        "sentiment": 0
    },
    {
        "text": "Standard neural module networks ( NMNs ) already provide a technique for constructing dynamic network structures from collections of composable modules .",
        "aspect": "neural module networks",
        "sentiment": 0
    },
    {
        "text": "Standard neural module networks ( NMNs ) already provide a technique for constructing dynamic network structures from collections of composable modules .",
        "aspect": "NMNs",
        "sentiment": 0
    },
    {
        "text": "Meanwhile , the compositional modular network proposed for grounding referring expressions in images does not need a parser , but is restricted to a fixed ( subject , relationship , object ) structure .",
        "aspect": "compositional modular network",
        "sentiment": 0
    },
    {
        "text": "Our contributions are 1 ) a method for learning a layout policy that dynamically predicts a network structure for each instance , without the aid of external linguistic resources at test time and 2 ) a new module parameterization that uses a soft attention over question words rather than hard - coded word assignments .",
        "aspect": "module parameterization",
        "sentiment": 2
    },
    {
        "text": "Our contributions are 1 ) a method for learning a layout policy that dynamically predicts a network structure for each instance , without the aid of external linguistic resources at test time and 2 ) a new module parameterization that uses a soft attention over question words rather than hard - coded word assignments .",
        "aspect": "learning a layout policy",
        "sentiment": 2
    },
    {
        "text": "Our contributions are 1 ) a method for learning a layout policy that dynamically predicts a network structure for each instance , without the aid of external linguistic resources at test time and 2 ) a new module parameterization that uses a soft attention over question words rather than hard - coded word assignments .",
        "aspect": "soft attention over question words",
        "sentiment": 1
    },
    {
        "text": "The recently proposed neural module network ( NMN ) architecture -a general class of recursive neural networks -provides a framework for constructing deep networks with dynamic computational structure .",
        "aspect": "neural module network",
        "sentiment": 0
    },
    {
        "text": "The recently proposed neural module network ( NMN ) architecture -a general class of recursive neural networks -provides a framework for constructing deep networks with dynamic computational structure .",
        "aspect": "NMN",
        "sentiment": 0
    },
    {
        "text": "The recently proposed neural module network ( NMN ) architecture -a general class of recursive neural networks -provides a framework for constructing deep networks with dynamic computational structure .",
        "aspect": "recursive neural networks",
        "sentiment": 0
    },
    {
        "text": "The recently proposed neural module network ( NMN ) architecture -a general class of recursive neural networks -provides a framework for constructing deep networks with dynamic computational structure .",
        "aspect": "deep networks",
        "sentiment": 0
    },
    {
        "text": "The recently proposed neural module network ( NMN ) architecture -a general class of recursive neural networks -provides a framework for constructing deep networks with dynamic computational structure .",
        "aspect": "dynamic computational structure",
        "sentiment": 0
    },
    {
        "text": "In an NMN model , every input is associated with a layout that provides a template for assembling an instancespecific network from a collection of shallow network fragments called modules .",
        "aspect": "NMN model",
        "sentiment": 0
    },
    {
        "text": "In an NMN model , every input is associated with a layout that provides a template for assembling an instancespecific network from a collection of shallow network fragments called modules .",
        "aspect": "shallow network fragments",
        "sentiment": 0
    },
    {
        "text": "The earliest work on NMNs used fixed rule - based layouts generated from dependency parses .",
        "aspect": "NMNs",
        "sentiment": 0
    },
    {
        "text": "The earliest work on NMNs used fixed rule - based layouts generated from dependency parses .",
        "aspect": "rule - based layouts",
        "sentiment": 0
    },
    {
        "text": "Later work on \" dynamic \" module networks ( D - NMNs ) incorporated a limited form of layout prediction by learning to rerank a list of three to ten candidates , again generated by rearranging modules predicted by a dependency parse .",
        "aspect": "dynamic \" module networks",
        "sentiment": 0
    },
    {
        "text": "Later work on \" dynamic \" module networks ( D - NMNs ) incorporated a limited form of layout prediction by learning to rerank a list of three to ten candidates , again generated by rearranging modules predicted by a dependency parse .",
        "aspect": "D - NMNs",
        "sentiment": 0
    },
    {
        "text": "Like D - NMNs , the present work attempts to learn an optimal layout predictor jointly with module behaviors themselves .",
        "aspect": "D - NMNs",
        "sentiment": 3
    },
    {
        "text": "Like D - NMNs , the present work attempts to learn an optimal layout predictor jointly with module behaviors themselves .",
        "aspect": "optimal layout predictor",
        "sentiment": 1
    },
    {
        "text": "We additionally modify the representation of the assembled module networks themselves : where and parameterized individual modules with a fixed embedding supplied by the parser , here we predict these parameters jointly with network structures using a soft attention mechanism .",
        "aspect": "soft attention mechanism",
        "sentiment": 0
    },
    {
        "text": "We additionally modify the representation of the assembled module networks themselves : where and parameterized individual modules with a fixed embedding supplied by the parser , here we predict these parameters jointly with network structures using a soft attention mechanism .",
        "aspect": "assembled module networks",
        "sentiment": 1
    },
    {
        "text": "This parameterization resembles the approach used in the \" compositional modular network \" architecture for grounding referential expressions .",
        "aspect": "compositional modular network \" architecture",
        "sentiment": 0
    },
    {
        "text": "More generally than these dynamic / modular approaches , a long line of research focuses on generic methods for automatically discovering neural network architectures from data .",
        "aspect": "dynamic / modular approaches",
        "sentiment": 0
    },
    {
        "text": "Past work includes techniques for optimizing over the space of architectures using evolutionary algorithms , Bayesian methods , and reinforcement learning .",
        "aspect": "evolutionary algorithms",
        "sentiment": 0
    },
    {
        "text": "Past work includes techniques for optimizing over the space of architectures using evolutionary algorithms , Bayesian methods , and reinforcement learning .",
        "aspect": "Bayesian methods",
        "sentiment": 0
    },
    {
        "text": "Past work includes techniques for optimizing over the space of architectures using evolutionary algorithms , Bayesian methods , and reinforcement learning .",
        "aspect": "reinforcement learning",
        "sentiment": 0
    },
    {
        "text": "The last of these is most closely related to our approach in this paper : both learn a controller RNN to output a network structure , train a neural network with the generated structure , and use the accuracy of the generated network to optimize the controller RNN .",
        "aspect": "controller RNN",
        "sentiment": 0
    },
    {
        "text": "The last of these is most closely related to our approach in this paper : both learn a controller RNN to output a network structure , train a neural network with the generated structure , and use the accuracy of the generated network to optimize the controller RNN .",
        "aspect": "neural network",
        "sentiment": 0
    },
    {
        "text": "The last of these is most closely related to our approach in this paper : both learn a controller RNN to output a network structure , train a neural network with the generated structure , and use the accuracy of the generated network to optimize the controller RNN .",
        "aspect": "controller RNN",
        "sentiment": 0
    },
    {
        "text": "A key difference between and the layout policy optimization in our work is that learns a fixed layout ( network architecture ) that is applied to every instance , while our model learns a layout policy that dynamically predicts a specific layout tailored to each individual input example .",
        "aspect": "layout policy optimization",
        "sentiment": 0
    },
    {
        "text": "A key difference between and the layout policy optimization in our work is that learns a fixed layout ( network architecture ) that is applied to every instance , while our model learns a layout policy that dynamically predicts a specific layout tailored to each individual input example .",
        "aspect": "network architecture",
        "sentiment": 0
    },
    {
        "text": "A key difference between and the layout policy optimization in our work is that learns a fixed layout ( network architecture ) that is applied to every instance , while our model learns a layout policy that dynamically predicts a specific layout tailored to each individual input example .",
        "aspect": "layout policy",
        "sentiment": 0
    },
    {
        "text": "The visual question answering task is generally motivated as a test to measure the capacity of deep models to reason about linguistic and visual inputs jointly .",
        "aspect": "deep models",
        "sentiment": 0
    },
    {
        "text": "Recent years have seen a proliferation of datasets and approaches , including models based on differentiable memory , dynamic prediction of question - specific computations , and core improvements to the implementation of the multi - modal representation and attention mechanism .",
        "aspect": "differentiable memory",
        "sentiment": 0
    },
    {
        "text": "Recent years have seen a proliferation of datasets and approaches , including models based on differentiable memory , dynamic prediction of question - specific computations , and core improvements to the implementation of the multi - modal representation and attention mechanism .",
        "aspect": "multi - modal representation and attention mechanism",
        "sentiment": 0
    },
    {
        "text": "Recent work has found that it is possible to do quite well on many visual QA problems by simply memorizing statistics about question / answer pairs ( suggesting that limited visual reasoning is involved ) , and that models with bag - of - words text representations perform competitively against more sophisticated approaches [ 14 ] ( suggesting that limited linguistic compositionality is involved ) .",
        "aspect": "limited visual reasoning",
        "sentiment": 0
    },
    {
        "text": "Recent work has found that it is possible to do quite well on many visual QA problems by simply memorizing statistics about question / answer pairs ( suggesting that limited visual reasoning is involved ) , and that models with bag - of - words text representations perform competitively against more sophisticated approaches [ 14 ] ( suggesting that limited linguistic compositionality is involved ) .",
        "aspect": "bag - of - words text representations",
        "sentiment": 0
    },
    {
        "text": "Most previous work on this task other than NMN uses a fixed inference structure to answer every question .",
        "aspect": "NMN",
        "sentiment": 0
    },
    {
        "text": "Our model is different from in that we use a set of specialized modules with soft attention mechanism to provide textual parameters for each module , while uses a generic mod- How many other things are of the same size as the green matte ball ?",
        "aspect": "soft attention mechanism",
        "sentiment": 1
    },
    {
        "text": "In this paper , we present the End - to - End Module Networks for visual question answering .",
        "aspect": "End - to - End Module Networks",
        "sentiment": 2
    },
    {
        "text": "Our model uses a set of neural modules to break down complex reasoning problems posed in textual questions into a few sub - tasks connected together , and learns to predict a suitable layout expression for each question using a layout policy implemented with a sequence - to - sequence RNN .",
        "aspect": "neural modules",
        "sentiment": 1
    },
    {
        "text": "Our model uses a set of neural modules to break down complex reasoning problems posed in textual questions into a few sub - tasks connected together , and learns to predict a suitable layout expression for each question using a layout policy implemented with a sequence - to - sequence RNN .",
        "aspect": "layout policy",
        "sentiment": 1
    },
    {
        "text": "Our model uses a set of neural modules to break down complex reasoning problems posed in textual questions into a few sub - tasks connected together , and learns to predict a suitable layout expression for each question using a layout policy implemented with a sequence - to - sequence RNN .",
        "aspect": "sequence - to - sequence RNN",
        "sentiment": 1
    },
    {
        "text": "During training , the model can be first trained with behavioral cloning from an expert layout policy , and further optimized end - to - end using reinforcement learning .",
        "aspect": "behavioral cloning",
        "sentiment": 1
    },
    {
        "text": "During training , the model can be first trained with behavioral cloning from an expert layout policy , and further optimized end - to - end using reinforcement learning .",
        "aspect": "expert layout policy",
        "sentiment": 1
    },
    {
        "text": "During training , the model can be first trained with behavioral cloning from an expert layout policy , and further optimized end - to - end using reinforcement learning .",
        "aspect": "reinforcement learning",
        "sentiment": 1
    },
    {
        "text": "Experimental results demonstrate that our model is capable of handling complicated reasoning problems , and the end - to - end optimization of the neural modules and layout policy can lead to significant further improvement over behavioral cloning from expert layouts .",
        "aspect": "neural modules",
        "sentiment": 0
    },
    {
        "text": "Experimental results demonstrate that our model is capable of handling complicated reasoning problems , and the end - to - end optimization of the neural modules and layout policy can lead to significant further improvement over behavioral cloning from expert layouts .",
        "aspect": "layout policy",
        "sentiment": 0
    },
    {
        "text": "Experimental results demonstrate that our model is capable of handling complicated reasoning problems , and the end - to - end optimization of the neural modules and layout policy can lead to significant further improvement over behavioral cloning from expert layouts .",
        "aspect": "behavioral cloning",
        "sentiment": 0
    },
    {
        "text": "Experimental results demonstrate that our model is capable of handling complicated reasoning problems , and the end - to - end optimization of the neural modules and layout policy can lead to significant further improvement over behavioral cloning from expert layouts .",
        "aspect": "expert layouts",
        "sentiment": 0
    },
    {
        "text": "DENSITY ESTIMATION USING REAL NVP",
        "aspect": "REAL NVP",
        "sentiment": 0
    },
    {
        "text": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning .",
        "aspect": "Unsupervised learning of probabilistic models",
        "sentiment": 0
    },
    {
        "text": "We extend the space of such models using real - valued non - volume preserving ( real NVP ) transformations , a set of powerful , stably invertible , and learnable transformations , resulting in an unsupervised learning algorithm with exact log - likelihood computation , exact and efficient sampling , exact and efficient inference of latent variables , and an interpretable latent space .",
        "aspect": "real - valued non - volume preserving",
        "sentiment": 2
    },
    {
        "text": "We extend the space of such models using real - valued non - volume preserving ( real NVP ) transformations , a set of powerful , stably invertible , and learnable transformations , resulting in an unsupervised learning algorithm with exact log - likelihood computation , exact and efficient sampling , exact and efficient inference of latent variables , and an interpretable latent space .",
        "aspect": "real NVP",
        "sentiment": 2
    },
    {
        "text": "We extend the space of such models using real - valued non - volume preserving ( real NVP ) transformations , a set of powerful , stably invertible , and learnable transformations , resulting in an unsupervised learning algorithm with exact log - likelihood computation , exact and efficient sampling , exact and efficient inference of latent variables , and an interpretable latent space .",
        "aspect": "unsupervised learning algorithm",
        "sentiment": 0
    },
    {
        "text": "We demonstrate its ability to model natural images on four datasets through sampling , log - likelihood evaluation , and latent variable manipulations .",
        "aspect": "sampling",
        "sentiment": 0
    },
    {
        "text": "We demonstrate its ability to model natural images on four datasets through sampling , log - likelihood evaluation , and latent variable manipulations .",
        "aspect": "log - likelihood evaluation",
        "sentiment": 0
    },
    {
        "text": "We demonstrate its ability to model natural images on four datasets through sampling , log - likelihood evaluation , and latent variable manipulations .",
        "aspect": "latent variable manipulations",
        "sentiment": 0
    },
    {
        "text": "The domain of representation learning has undergone tremendous advances due to improved supervised learning techniques .",
        "aspect": "supervised learning techniques",
        "sentiment": 0
    },
    {
        "text": "However , unsupervised learning has the potential to leverage large pools of unlabeled data , and extend these advances to modalities that are otherwise impractical or impossible .",
        "aspect": "unsupervised learning",
        "sentiment": 0
    },
    {
        "text": "One principled approach to unsupervised learning is generative probabilistic modeling .",
        "aspect": "generative probabilistic modeling",
        "sentiment": 0
    },
    {
        "text": "Not only do generative probabilistic models have the ability to create novel content , they also have a wide range of reconstruction related applications including inpainting , denoising , colorization , and super - resolution .",
        "aspect": "generative probabilistic models",
        "sentiment": 0
    },
    {
        "text": "We address this challenge by introducing real - valued non - volume preserving ( real NVP ) transformations , a tractable yet expressive approach to modeling high - dimensional data .",
        "aspect": "real - valued non - volume preserving",
        "sentiment": 2
    },
    {
        "text": "We address this challenge by introducing real - valued non - volume preserving ( real NVP ) transformations , a tractable yet expressive approach to modeling high - dimensional data .",
        "aspect": "real NVP",
        "sentiment": 2
    },
    {
        "text": "Substantial work on probabilistic generative models has focused on training models using maximum likelihood .",
        "aspect": "probabilistic generative models",
        "sentiment": 0
    },
    {
        "text": "Substantial work on probabilistic generative models has focused on training models using maximum likelihood .",
        "aspect": "maximum likelihood",
        "sentiment": 0
    },
    {
        "text": "One class of maximum likelihood models are those described by probabilistic undirected graphs , such as Restricted Boltzmann Machines and Deep Boltzmann Machines .",
        "aspect": "maximum likelihood models",
        "sentiment": 0
    },
    {
        "text": "One class of maximum likelihood models are those described by probabilistic undirected graphs , such as Restricted Boltzmann Machines and Deep Boltzmann Machines .",
        "aspect": "probabilistic undirected graphs",
        "sentiment": 0
    },
    {
        "text": "One class of maximum likelihood models are those described by probabilistic undirected graphs , such as Restricted Boltzmann Machines and Deep Boltzmann Machines .",
        "aspect": "Restricted Boltzmann Machines",
        "sentiment": 0
    },
    {
        "text": "One class of maximum likelihood models are those described by probabilistic undirected graphs , such as Restricted Boltzmann Machines and Deep Boltzmann Machines .",
        "aspect": "Deep Boltzmann Machines",
        "sentiment": 0
    },
    {
        "text": "However , because of the intractability of the associated marginal distribution over latent variables , their training , evaluation , and sampling procedures necessitate the use of approximations like Mean Field inference and Markov Chain Monte Carlo , whose convergence time for such complex models Data space X Latent space Z",
        "aspect": "Mean Field inference",
        "sentiment": 0
    },
    {
        "text": "However , because of the intractability of the associated marginal distribution over latent variables , their training , evaluation , and sampling procedures necessitate the use of approximations like Mean Field inference and Markov Chain Monte Carlo , whose convergence time for such complex models Data space X Latent space Z",
        "aspect": "Markov Chain Monte Carlo",
        "sentiment": 0
    },
    {
        "text": "\u21d0 Figure : Real NVP learns an invertible , stable , mapping between a data distribution pX and a latent distribution p Z ( typically a Gaussian ) .",
        "aspect": "Real NVP",
        "sentiment": 0
    },
    {
        "text": "\u21d0 Figure : Real NVP learns an invertible , stable , mapping between a data distribution pX and a latent distribution p Z ( typically a Gaussian ) .",
        "aspect": "data distribution pX",
        "sentiment": 0
    },
    {
        "text": "\u21d0 Figure : Real NVP learns an invertible , stable , mapping between a data distribution pX and a latent distribution p Z ( typically a Gaussian ) .",
        "aspect": "Gaussian",
        "sentiment": 0
    },
    {
        "text": "This corresponds to exact inference of the latent state given the data .",
        "aspect": "exact inference",
        "sentiment": 0
    },
    {
        "text": "Directed graphical models are instead defined in terms of an ancestral sampling procedure , which is appealing both for its conceptual and computational simplicity .",
        "aspect": "Directed graphical models",
        "sentiment": 0
    },
    {
        "text": "Directed graphical models are instead defined in terms of an ancestral sampling procedure , which is appealing both for its conceptual and computational simplicity .",
        "aspect": "ancestral sampling procedure",
        "sentiment": 0
    },
    {
        "text": "They lack , however , the conditional independence structure of undirected models , making exact and approximate posterior inference on latent variables cumbersome .",
        "aspect": "undirected models",
        "sentiment": 0
    },
    {
        "text": "Recent advances in stochastic variational inference and amortized inference , allowed efficient approximate inference and learning of deep directed graphical models by maximizing a variational lower bound on the log - likelihood .",
        "aspect": "stochastic variational inference",
        "sentiment": 0
    },
    {
        "text": "Recent advances in stochastic variational inference and amortized inference , allowed efficient approximate inference and learning of deep directed graphical models by maximizing a variational lower bound on the log - likelihood .",
        "aspect": "amortized inference",
        "sentiment": 0
    },
    {
        "text": "In particular , the variational autoencoder algorithm simultaneously learns a generative network , that maps gaussian latent variables z to samples x , and a matched approximate inference network that maps samples x to a semantically meaningful latent representation z , by exploiting the reparametrization trick .",
        "aspect": "variational autoencoder algorithm",
        "sentiment": 0
    },
    {
        "text": "In particular , the variational autoencoder algorithm simultaneously learns a generative network , that maps gaussian latent variables z to samples x , and a matched approximate inference network that maps samples x to a semantically meaningful latent representation z , by exploiting the reparametrization trick .",
        "aspect": "generative network",
        "sentiment": 0
    },
    {
        "text": "In particular , the variational autoencoder algorithm simultaneously learns a generative network , that maps gaussian latent variables z to samples x , and a matched approximate inference network that maps samples x to a semantically meaningful latent representation z , by exploiting the reparametrization trick .",
        "aspect": "matched approximate inference network",
        "sentiment": 0
    },
    {
        "text": "In particular , the variational autoencoder algorithm simultaneously learns a generative network , that maps gaussian latent variables z to samples x , and a matched approximate inference network that maps samples x to a semantically meaningful latent representation z , by exploiting the reparametrization trick .",
        "aspect": "reparametrization trick",
        "sentiment": 0
    },
    {
        "text": "Its success in leveraging recent advances in backpropagation in deep neural networks resulted in its adoption for several applications ranging from speech synthesis to language modeling .",
        "aspect": "backpropagation",
        "sentiment": 0
    },
    {
        "text": "Its success in leveraging recent advances in backpropagation in deep neural networks resulted in its adoption for several applications ranging from speech synthesis to language modeling .",
        "aspect": "deep neural networks",
        "sentiment": 0
    },
    {
        "text": "Still , the approximation in the inference process limits its ability to learn high dimensional deep representations , motivating recent work in improving approximate inference .",
        "aspect": "inference process",
        "sentiment": 0
    },
    {
        "text": "Autoregressive models can implement this strategy while typically retaining a great deal of flexibility .",
        "aspect": "Autoregressive models",
        "sentiment": 0
    },
    {
        "text": "This class of algorithms tractably models the joint distribution by decomposing it into a product of conditionals using the probability chain rule according to a fixed ordering over dimensions , simplifying log - likelihood evaluation and sampling .",
        "aspect": "product of conditionals",
        "sentiment": 0
    },
    {
        "text": "This class of algorithms tractably models the joint distribution by decomposing it into a product of conditionals using the probability chain rule according to a fixed ordering over dimensions , simplifying log - likelihood evaluation and sampling .",
        "aspect": "probability chain rule",
        "sentiment": 0
    },
    {
        "text": "This class of algorithms tractably models the joint distribution by decomposing it into a product of conditionals using the probability chain rule according to a fixed ordering over dimensions , simplifying log - likelihood evaluation and sampling .",
        "aspect": "fixed ordering over dimensions",
        "sentiment": 0
    },
    {
        "text": "This class of algorithms tractably models the joint distribution by decomposing it into a product of conditionals using the probability chain rule according to a fixed ordering over dimensions , simplifying log - likelihood evaluation and sampling .",
        "aspect": "simplifying log - likelihood evaluation and sampling",
        "sentiment": 0
    },
    {
        "text": "Recent work in this line of research has taken advantage of recent advances in recurrent networks , in particular long - short term memory , and residual networks in order to learn state - of - the - art generative image models and language models .",
        "aspect": "recurrent networks",
        "sentiment": 0
    },
    {
        "text": "Recent work in this line of research has taken advantage of recent advances in recurrent networks , in particular long - short term memory , and residual networks in order to learn state - of - the - art generative image models and language models .",
        "aspect": "long - short term memory",
        "sentiment": 0
    },
    {
        "text": "Recent work in this line of research has taken advantage of recent advances in recurrent networks , in particular long - short term memory , and residual networks in order to learn state - of - the - art generative image models and language models .",
        "aspect": "residual networks",
        "sentiment": 0
    },
    {
        "text": "Recent work in this line of research has taken advantage of recent advances in recurrent networks , in particular long - short term memory , and residual networks in order to learn state - of - the - art generative image models and language models .",
        "aspect": "generative image models",
        "sentiment": 0
    },
    {
        "text": "Recent work in this line of research has taken advantage of recent advances in recurrent networks , in particular long - short term memory , and residual networks in order to learn state - of - the - art generative image models and language models .",
        "aspect": "language models",
        "sentiment": 0
    },
    {
        "text": "For example , its sampling procedure is sequential and non - parallelizable , which can become cumbersome in applications like speech and music synthesis , or real - time rendering .. Additionally , there is no natural latent representation associated with autoregressive models , and they have not yet been shown to be useful for semi - supervised learning .",
        "aspect": "autoregressive models",
        "sentiment": 0
    },
    {
        "text": "Generative Adversarial Networks ( GANs ) on the other hand can train any differentiable generative network by avoiding the maximum likelihood principle altogether .",
        "aspect": "Generative Adversarial Networks",
        "sentiment": 0
    },
    {
        "text": "Generative Adversarial Networks ( GANs ) on the other hand can train any differentiable generative network by avoiding the maximum likelihood principle altogether .",
        "aspect": "GANs",
        "sentiment": 0
    },
    {
        "text": "Generative Adversarial Networks ( GANs ) on the other hand can train any differentiable generative network by avoiding the maximum likelihood principle altogether .",
        "aspect": "differentiable generative network",
        "sentiment": 0
    },
    {
        "text": "Generative Adversarial Networks ( GANs ) on the other hand can train any differentiable generative network by avoiding the maximum likelihood principle altogether .",
        "aspect": "maximum likelihood principle",
        "sentiment": 0
    },
    {
        "text": "Instead , the generative network is associated with a discriminator network whose task is to distinguish between samples and real data .",
        "aspect": "generative network",
        "sentiment": 0
    },
    {
        "text": "Instead , the generative network is associated with a discriminator network whose task is to distinguish between samples and real data .",
        "aspect": "discriminator network",
        "sentiment": 0
    },
    {
        "text": "Rather than using an intractable log - likelihood , this discriminator network provides the training signal in an adversarial fashion .",
        "aspect": "discriminator network",
        "sentiment": 0
    },
    {
        "text": "Rather than using an intractable log - likelihood , this discriminator network provides the training signal in an adversarial fashion .",
        "aspect": "adversarial fashion",
        "sentiment": 0
    },
    {
        "text": "Successfully trained GAN models can consistently generate sharp and realistically looking samples .",
        "aspect": "GAN models",
        "sentiment": 0
    },
    {
        "text": "Additionally , instability in their training process requires careful hyperparameter tuning to avoid diverging behavior .",
        "aspect": "hyperparameter tuning",
        "sentiment": 0
    },
    {
        "text": "Training such a generative network g that maps latent variable z \u223c p Z to a sample x \u223c p X does not in theory require a discriminator network as in GANs , or approximate inference as in variational autoencoders .",
        "aspect": "generative network g",
        "sentiment": 0
    },
    {
        "text": "Training such a generative network g that maps latent variable z \u223c p Z to a sample x \u223c p X does not in theory require a discriminator network as in GANs , or approximate inference as in variational autoencoders .",
        "aspect": "discriminator network",
        "sentiment": 0
    },
    {
        "text": "Training such a generative network g that maps latent variable z \u223c p Z to a sample x \u223c p X does not in theory require a discriminator network as in GANs , or approximate inference as in variational autoencoders .",
        "aspect": "GANs",
        "sentiment": 0
    },
    {
        "text": "Training such a generative network g that maps latent variable z \u223c p Z to a sample x \u223c p X does not in theory require a discriminator network as in GANs , or approximate inference as in variational autoencoders .",
        "aspect": "approximate inference",
        "sentiment": 0
    },
    {
        "text": "Training such a generative network g that maps latent variable z \u223c p Z to a sample x \u223c p X does not in theory require a discriminator network as in GANs , or approximate inference as in variational autoencoders .",
        "aspect": "variational autoencoders",
        "sentiment": 0
    },
    {
        "text": "Indeed , if g is bijective , it can be trained through maximum likelihood using the change of variable formula :",
        "aspect": "maximum likelihood",
        "sentiment": 0
    },
    {
        "text": "This formula has been discussed in several papers including the maximum likelihood formulation of independent components analysis ( ICA ) , gaussianization and deep density models .",
        "aspect": "maximum likelihood formulation",
        "sentiment": 0
    },
    {
        "text": "This formula has been discussed in several papers including the maximum likelihood formulation of independent components analysis ( ICA ) , gaussianization and deep density models .",
        "aspect": "independent components analysis",
        "sentiment": 0
    },
    {
        "text": "This formula has been discussed in several papers including the maximum likelihood formulation of independent components analysis ( ICA ) , gaussianization and deep density models .",
        "aspect": "ICA",
        "sentiment": 0
    },
    {
        "text": "This formula has been discussed in several papers including the maximum likelihood formulation of independent components analysis ( ICA ) , gaussianization and deep density models .",
        "aspect": "gaussianization",
        "sentiment": 0
    },
    {
        "text": "This formula has been discussed in several papers including the maximum likelihood formulation of independent components analysis ( ICA ) , gaussianization and deep density models .",
        "aspect": "deep density models",
        "sentiment": 0
    },
    {
        "text": "As the existence proof of nonlinear ICA solutions suggests , auto - regressive models can be seen as tractable instance of maximum likelihood nonlinear ICA , where the residual corresponds to the independent components .",
        "aspect": "nonlinear ICA solutions",
        "sentiment": 0
    },
    {
        "text": "As the existence proof of nonlinear ICA solutions suggests , auto - regressive models can be seen as tractable instance of maximum likelihood nonlinear ICA , where the residual corresponds to the independent components .",
        "aspect": "maximum likelihood nonlinear ICA",
        "sentiment": 0
    },
    {
        "text": "As the existence proof of nonlinear ICA solutions suggests , auto - regressive models can be seen as tractable instance of maximum likelihood nonlinear ICA , where the residual corresponds to the independent components .",
        "aspect": "auto - regressive models",
        "sentiment": 0
    },
    {
        "text": "We show in Table that the number of bits per dimension , while not improving over the Pixel RNN baseline , is competitive with other generative methods .",
        "aspect": "Pixel RNN baseline",
        "sentiment": 0
    },
    {
        "text": "We show in Table that the number of bits per dimension , while not improving over the Pixel RNN baseline , is competitive with other generative methods .",
        "aspect": "generative methods",
        "sentiment": 0
    },
    {
        "text": "As mentioned in , maximum likelihood is a principle that values diversity over sample quality in a limited capacity setting .",
        "aspect": "maximum likelihood",
        "sentiment": 0
    },
    {
        "text": "As opposed to variational autoencoders , the samples generated from our model look not only globally coherent but also sharp .",
        "aspect": "variational autoencoders",
        "sentiment": 0
    },
    {
        "text": "Our hypothesis is that as opposed to these models , real NVP does not rely on fixed form reconstruction cost like an L 2 norm which tends to reward capturing low frequency components more heavily than high frequency components .",
        "aspect": "real NVP",
        "sentiment": 0
    },
    {
        "text": "Our hypothesis is that as opposed to these models , real NVP does not rely on fixed form reconstruction cost like an L 2 norm which tends to reward capturing low frequency components more heavily than high frequency components .",
        "aspect": "L 2 norm",
        "sentiment": 0
    },
    {
        "text": "Unlike autoregressive models , sampling from our model is done very efficiently as it is parallelized over input dimensions .",
        "aspect": "autoregressive models",
        "sentiment": 0
    },
    {
        "text": "To further test whether the latent space has a consistent semantic interpretation , we trained a class - conditional model on CelebA , and found that the learned representation had a consistent semantic meaning across class labels ( see Appendix F ) .",
        "aspect": "class - conditional model",
        "sentiment": 0
    },
    {
        "text": "In this paper , we have defined a class of invertible functions with tractable Jacobian determinant , enabling exact and tractable log - likelihood evaluation , inference , and sampling .",
        "aspect": "invertible functions",
        "sentiment": 1
    },
    {
        "text": "We have shown that this class of generative model achieves competitive performances , both in terms of sample quality and log - likelihood .",
        "aspect": "generative model",
        "sentiment": 1
    },
    {
        "text": "Many avenues exist to further improve the functional form of the transformations , for instance by exploiting the latest advances in dilated convolutions and residual networks architectures .",
        "aspect": "dilated convolutions",
        "sentiment": 0
    },
    {
        "text": "Many avenues exist to further improve the functional form of the transformations , for instance by exploiting the latest advances in dilated convolutions and residual networks architectures .",
        "aspect": "residual networks architectures",
        "sentiment": 0
    },
    {
        "text": "This paper presented a technique bridging the gap between auto - regressive models , variational autoencoders , and generative adversarial networks .",
        "aspect": "auto - regressive models",
        "sentiment": 3
    },
    {
        "text": "This paper presented a technique bridging the gap between auto - regressive models , variational autoencoders , and generative adversarial networks .",
        "aspect": "variational autoencoders",
        "sentiment": 3
    },
    {
        "text": "This paper presented a technique bridging the gap between auto - regressive models , variational autoencoders , and generative adversarial networks .",
        "aspect": "generative adversarial networks",
        "sentiment": 3
    },
    {
        "text": "Like auto - regressive models , it allows tractable and exact log - likelihood evaluation for training .",
        "aspect": "auto - regressive models",
        "sentiment": 0
    },
    {
        "text": "It allows however a much more flexible functional form , similar to that in the generative model of variational autoencoders .",
        "aspect": "generative model of variational autoencoders",
        "sentiment": 0
    },
    {
        "text": "Like GANs , and unlike variational autoencoders , our technique does not require the use of a fixed form reconstruction cost , and instead defines a cost in terms of higher level features , generating sharper images .",
        "aspect": "GANs",
        "sentiment": 3
    },
    {
        "text": "Like GANs , and unlike variational autoencoders , our technique does not require the use of a fixed form reconstruction cost , and instead defines a cost in terms of higher level features , generating sharper images .",
        "aspect": "variational autoencoders",
        "sentiment": 3
    },
    {
        "text": "Finally , unlike both variational autoencoders and GANs , our technique is able to learn a semantically meaningful latent space which is as high dimensional as the input space .",
        "aspect": "variational autoencoders",
        "sentiment": 3
    },
    {
        "text": "Finally , unlike both variational autoencoders and GANs , our technique is able to learn a semantically meaningful latent space which is as high dimensional as the input space .",
        "aspect": "GANs",
        "sentiment": 3
    },
    {
        "text": "Real NVP generative models can additionally be conditioned on additional variables ( for instance class labels ) to create a structured output algorithm .",
        "aspect": "Real NVP generative models",
        "sentiment": 0
    },
    {
        "text": "More so , as the resulting class of invertible transformations can be treated as a probability distribution in a modular way , it can also be used to improve upon other probabilistic models like auto - regressive models and variational autoencoders .",
        "aspect": "auto - regressive models",
        "sentiment": 0
    },
    {
        "text": "More so , as the resulting class of invertible transformations can be treated as a probability distribution in a modular way , it can also be used to improve upon other probabilistic models like auto - regressive models and variational autoencoders .",
        "aspect": "variational autoencoders",
        "sentiment": 0
    },
    {
        "text": "For variational autoencoders , these transformations could be used both to enable a more flexible reconstruction cost and a more flexible stochastic inference distribution .",
        "aspect": "variational autoencoders",
        "sentiment": 0
    },
    {
        "text": "For variational autoencoders , these transformations could be used both to enable a more flexible reconstruction cost and a more flexible stochastic inference distribution .",
        "aspect": "stochastic inference distribution",
        "sentiment": 0
    },
    {
        "text": "Probabilistic models in general can also benefit from batch normalization techniques as applied in this paper .",
        "aspect": "Probabilistic models",
        "sentiment": 0
    },
    {
        "text": "Probabilistic models in general can also benefit from batch normalization techniques as applied in this paper .",
        "aspect": "batch normalization techniques",
        "sentiment": 0
    },
    {
        "text": "The definition of powerful and trainable invertible functions can also benefit domains other than generative unsupervised learning .",
        "aspect": "invertible functions",
        "sentiment": 0
    },
    {
        "text": "The definition of powerful and trainable invertible functions can also benefit domains other than generative unsupervised learning .",
        "aspect": "generative unsupervised learning",
        "sentiment": 0
    },
    {
        "text": "For example , in reinforcement learning , these invertible functions can help extend the set of functions for which an argmax operation is tractable for continuous Qlearning or find representation where local linear Gaussian approximations are more appropriate .",
        "aspect": "invertible functions",
        "sentiment": 0
    },
    {
        "text": "For example , in reinforcement learning , these invertible functions can help extend the set of functions for which an argmax operation is tractable for continuous Qlearning or find representation where local linear Gaussian approximations are more appropriate .",
        "aspect": "argmax operation",
        "sentiment": 0
    },
    {
        "text": "For example , in reinforcement learning , these invertible functions can help extend the set of functions for which an argmax operation is tractable for continuous Qlearning or find representation where local linear Gaussian approximations are more appropriate .",
        "aspect": "local linear Gaussian approximations",
        "sentiment": 0
    },
    {
        "text": "The quality of these representations is measured in a word similarity task , and the results are compared to the previously best performing techniques based on different types of neural networks .",
        "aspect": "neural networks",
        "sentiment": 0
    },
    {
        "text": "An example is the popular N - gram model used for statistical language modeling -today , it is possible to train N - grams on virtually all available data ( trillions of words ) .",
        "aspect": "N - gram model",
        "sentiment": 0
    },
    {
        "text": "An example is the popular N - gram model used for statistical language modeling -today , it is possible to train N - grams on virtually all available data ( trillions of words ) .",
        "aspect": "N - grams",
        "sentiment": 0
    },
    {
        "text": "Probably the most successful concept is to use distributed representations of words .",
        "aspect": "distributed representations of words",
        "sentiment": 0
    },
    {
        "text": "For example , neural network based language models significantly outperform N - gram models .",
        "aspect": "neural network based language models",
        "sentiment": 0
    },
    {
        "text": "For example , neural network based language models significantly outperform N - gram models .",
        "aspect": "N - gram models",
        "sentiment": 0
    },
    {
        "text": "A very popular model architecture for estimating neural network language model ( NNLM ) was proposed in , where a feedforward neural network with a linear projection layer and a non - linear hidden layer was used to learn jointly the word vector representation and a statistical language model .",
        "aspect": "neural network language model",
        "sentiment": 0
    },
    {
        "text": "A very popular model architecture for estimating neural network language model ( NNLM ) was proposed in , where a feedforward neural network with a linear projection layer and a non - linear hidden layer was used to learn jointly the word vector representation and a statistical language model .",
        "aspect": "NNLM",
        "sentiment": 0
    },
    {
        "text": "A very popular model architecture for estimating neural network language model ( NNLM ) was proposed in , where a feedforward neural network with a linear projection layer and a non - linear hidden layer was used to learn jointly the word vector representation and a statistical language model .",
        "aspect": "feedforward neural network",
        "sentiment": 0
    },
    {
        "text": "A very popular model architecture for estimating neural network language model ( NNLM ) was proposed in , where a feedforward neural network with a linear projection layer and a non - linear hidden layer was used to learn jointly the word vector representation and a statistical language model .",
        "aspect": "linear projection layer",
        "sentiment": 0
    },
    {
        "text": "A very popular model architecture for estimating neural network language model ( NNLM ) was proposed in , where a feedforward neural network with a linear projection layer and a non - linear hidden layer was used to learn jointly the word vector representation and a statistical language model .",
        "aspect": "non - linear hidden layer",
        "sentiment": 0
    },
    {
        "text": "A very popular model architecture for estimating neural network language model ( NNLM ) was proposed in , where a feedforward neural network with a linear projection layer and a non - linear hidden layer was used to learn jointly the word vector representation and a statistical language model .",
        "aspect": "word vector representation",
        "sentiment": 0
    },
    {
        "text": "A very popular model architecture for estimating neural network language model ( NNLM ) was proposed in , where a feedforward neural network with a linear projection layer and a non - linear hidden layer was used to learn jointly the word vector representation and a statistical language model .",
        "aspect": "statistical language model",
        "sentiment": 0
    },
    {
        "text": "Another interesting architecture of NNLM was presented in , where the word vectors are first learned using neural network with a single hidden layer .",
        "aspect": "NNLM",
        "sentiment": 0
    },
    {
        "text": "Another interesting architecture of NNLM was presented in , where the word vectors are first learned using neural network with a single hidden layer .",
        "aspect": "neural network",
        "sentiment": 0
    },
    {
        "text": "The word vectors are then used to train the NNLM .",
        "aspect": "NNLM",
        "sentiment": 0
    },
    {
        "text": "Thus , the word vectors are learned even without constructing the full NNLM .",
        "aspect": "NNLM",
        "sentiment": 0
    },
    {
        "text": "However , as far as we know , these architectures were significantly more computationally expensive for training than the one proposed in , with the exception of certain version of log - bilinear model where diagonal weight matrices are used .",
        "aspect": "log - bilinear model",
        "sentiment": 0
    },
    {
        "text": "Somewhat surprisingly , these questions can be answered by performing simple algebraic operations with the vector representation of words .",
        "aspect": "algebraic operations",
        "sentiment": 0
    },
    {
        "text": "Somewhat surprisingly , these questions can be answered by performing simple algebraic operations with the vector representation of words .",
        "aspect": "vector representation of words",
        "sentiment": 0
    },
    {
        "text": "To find a word that is similar to small in the same sense as biggest is similar to big , we can simply compute vector X = vector(\"biggest \" ) \u2212 vector(\"big \" ) + vector(\"small \" ) .",
        "aspect": "vector(\"big \"",
        "sentiment": 0
    },
    {
        "text": "Then , we search in the vector space for the word closest to X measured by cosine distance , and use it as the answer to the question ( we discard the input question words during this search ) .",
        "aspect": "cosine distance",
        "sentiment": 0
    },
    {
        "text": "We observed that it is possible to train high quality word vectors using very simple model architectures , compared to the popular neural network models ( both feedforward and recurrent ) .",
        "aspect": "neural network models",
        "sentiment": 3
    },
    {
        "text": "We observed that it is possible to train high quality word vectors using very simple model architectures , compared to the popular neural network models ( both feedforward and recurrent ) .",
        "aspect": "feedforward and recurrent",
        "sentiment": 3
    },
    {
        "text": "Using the DistBelief distributed framework , it should be possible to train the CBOW and Skip - gram models even on corpora with one trillion words , for basically unlimited size of the vocabulary .",
        "aspect": "CBOW and Skip - gram models",
        "sentiment": 0
    },
    {
        "text": "The publicly available RNN vectors were used together with other techniques to achieve over 50 % increase in Spearman 's rank correlation over the previous best result .",
        "aspect": "RNN vectors",
        "sentiment": 0
    },
    {
        "text": "The neural network based word vectors were previously applied to many other NLP tasks , for example sentiment analysis and paraphrase detection .",
        "aspect": "neural network based word vectors",
        "sentiment": 0
    },
    {
        "text": "In the future , it would be also interesting to compare our techniques to Latent Relational Analysis and others .",
        "aspect": "Latent Relational Analysis",
        "sentiment": 0
    },
    {
        "text": "We establish a new connection between value and policy based reinforcement learning ( RL ) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization .",
        "aspect": "RL",
        "sentiment": 0
    },
    {
        "text": "We establish a new connection between value and policy based reinforcement learning ( RL ) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization .",
        "aspect": "entropy regularization",
        "sentiment": 0
    },
    {
        "text": "From this observation , we develop a new RL algorithm , Path Consistency Learning ( PCL ) , that minimizes a notion of soft consistency error along multi - step action sequences extracted from both on - and off - policy traces .",
        "aspect": "RL algorithm",
        "sentiment": 1
    },
    {
        "text": "From this observation , we develop a new RL algorithm , Path Consistency Learning ( PCL ) , that minimizes a notion of soft consistency error along multi - step action sequences extracted from both on - and off - policy traces .",
        "aspect": "Path Consistency Learning",
        "sentiment": 2
    },
    {
        "text": "From this observation , we develop a new RL algorithm , Path Consistency Learning ( PCL ) , that minimizes a notion of soft consistency error along multi - step action sequences extracted from both on - and off - policy traces .",
        "aspect": "PCL",
        "sentiment": 2
    },
    {
        "text": "We examine the behavior of PCL in different scenarios and show that PCL can be interpreted as generalizing both actor - critic and Q - learning algorithms .",
        "aspect": "PCL",
        "sentiment": 1
    },
    {
        "text": "We examine the behavior of PCL in different scenarios and show that PCL can be interpreted as generalizing both actor - critic and Q - learning algorithms .",
        "aspect": "PCL",
        "sentiment": 1
    },
    {
        "text": "We examine the behavior of PCL in different scenarios and show that PCL can be interpreted as generalizing both actor - critic and Q - learning algorithms .",
        "aspect": "actor - critic",
        "sentiment": 0
    },
    {
        "text": "We examine the behavior of PCL in different scenarios and show that PCL can be interpreted as generalizing both actor - critic and Q - learning algorithms .",
        "aspect": "Q - learning algorithms",
        "sentiment": 0
    },
    {
        "text": "The experimental evaluation demonstrates that PCL significantly outperforms strong actor - critic and Q - learning baselines across several benchmarks . 2 1 Work done as a member of the Google Brain Residency program ( g.co/brainresidency ) 2 An implementation of PCL can be found at https://github.com/tensorflow/models/tree/",
        "aspect": "PCL",
        "sentiment": 1
    },
    {
        "text": "The experimental evaluation demonstrates that PCL significantly outperforms strong actor - critic and Q - learning baselines across several benchmarks . 2 1 Work done as a member of the Google Brain Residency program ( g.co/brainresidency ) 2 An implementation of PCL can be found at https://github.com/tensorflow/models/tree/",
        "aspect": "actor - critic",
        "sentiment": 3
    },
    {
        "text": "The experimental evaluation demonstrates that PCL significantly outperforms strong actor - critic and Q - learning baselines across several benchmarks . 2 1 Work done as a member of the Google Brain Residency program ( g.co/brainresidency ) 2 An implementation of PCL can be found at https://github.com/tensorflow/models/tree/",
        "aspect": "Q - learning",
        "sentiment": 3
    },
    {
        "text": "Model - free RL aims to acquire an effective behavior policy through trial and error interaction with a black box environment .",
        "aspect": "Model - free RL",
        "sentiment": 0
    },
    {
        "text": "Model - free RL aims to acquire an effective behavior policy through trial and error interaction with a black box environment .",
        "aspect": "behavior policy",
        "sentiment": 0
    },
    {
        "text": "Model - free RL aims to acquire an effective behavior policy through trial and error interaction with a black box environment .",
        "aspect": "trial and error interaction",
        "sentiment": 0
    },
    {
        "text": "Model - free RL has a myriad of applications in games , robotics , and marketing , to name a few .",
        "aspect": "Model - free RL",
        "sentiment": 0
    },
    {
        "text": "Recently , the impact of model - free RL has been expanded through the use of deep neural networks , which promise to replace manual feature engineering with end - to - end learning of value and policy representations .",
        "aspect": "model - free RL",
        "sentiment": 0
    },
    {
        "text": "Recently , the impact of model - free RL has been expanded through the use of deep neural networks , which promise to replace manual feature engineering with end - to - end learning of value and policy representations .",
        "aspect": "deep neural networks",
        "sentiment": 0
    },
    {
        "text": "Recently , the impact of model - free RL has been expanded through the use of deep neural networks , which promise to replace manual feature engineering with end - to - end learning of value and policy representations .",
        "aspect": "manual feature engineering",
        "sentiment": 0
    },
    {
        "text": "Recently , the impact of model - free RL has been expanded through the use of deep neural networks , which promise to replace manual feature engineering with end - to - end learning of value and policy representations .",
        "aspect": "end - to - end learning of value and policy representations",
        "sentiment": 0
    },
    {
        "text": "Unfortunately , a key challenge remains how best to combine the advantages of value and policy based RL approaches in the presence of deep function approximators , while mitigating their shortcomings .",
        "aspect": "deep function approximators",
        "sentiment": 0
    },
    {
        "text": "The primary advantage of policy based approaches , such as REINFORCE , is that they directly optimize the quantity of interest while remaining stable under function approximation ( given a sufficiently small learning rate ) .",
        "aspect": "policy based approaches",
        "sentiment": 0
    },
    {
        "text": "The primary advantage of policy based approaches , such as REINFORCE , is that they directly optimize the quantity of interest while remaining stable under function approximation ( given a sufficiently small learning rate ) .",
        "aspect": "REINFORCE",
        "sentiment": 0
    },
    {
        "text": "The primary advantage of policy based approaches , such as REINFORCE , is that they directly optimize the quantity of interest while remaining stable under function approximation ( given a sufficiently small learning rate ) .",
        "aspect": "function approximation",
        "sentiment": 0
    },
    {
        "text": "Actor - critic methods have thus become popular , because they use value approximators to replace rollout estimates and reduce variance , at the cost of some bias .",
        "aspect": "Actor - critic methods",
        "sentiment": 0
    },
    {
        "text": "Actor - critic methods have thus become popular , because they use value approximators to replace rollout estimates and reduce variance , at the cost of some bias .",
        "aspect": "value approximators",
        "sentiment": 0
    },
    {
        "text": "Nevertheless , on - policy learning remains inherently sample inefficient ; by estimating quantities defined by the current policy , either on - policy data must be used , or updating must be sufficiently slow to avoid significant bias .",
        "aspect": "on - policy learning",
        "sentiment": 0
    },
    {
        "text": "Naive importance correction is hardly able to overcome these shortcomings in practice .",
        "aspect": "Naive importance correction",
        "sentiment": 0
    },
    {
        "text": "By contrast , value based methods , such as Q - learning , can learn from any trajectory sampled from the same environment .",
        "aspect": "value based methods",
        "sentiment": 0
    },
    {
        "text": "By contrast , value based methods , such as Q - learning , can learn from any trajectory sampled from the same environment .",
        "aspect": "Q - learning",
        "sentiment": 0
    },
    {
        "text": "Such \" off - policy \" methods are able to exploit data from other sources , such as experts , making them inherently more sample efficient than on - policy methods .",
        "aspect": "on - policy methods",
        "sentiment": 0
    },
    {
        "text": "Their key drawback is that off - policy learning does not stably interact with function approximation .",
        "aspect": "off - policy learning",
        "sentiment": 0
    },
    {
        "text": "Their key drawback is that off - policy learning does not stably interact with function approximation .",
        "aspect": "function approximation",
        "sentiment": 0
    },
    {
        "text": "The practical consequence is that extensive hyperparameter tuning can be required to obtain stable behavior .",
        "aspect": "hyperparameter tuning",
        "sentiment": 0
    },
    {
        "text": "Despite practical success , there is also little theoretical understanding of how deep Q - learning might obtain near - optimal objective values .",
        "aspect": "deep Q - learning",
        "sentiment": 0
    },
    {
        "text": "Ideally , one would like to combine the unbiasedness and stability of on - policy training with the data efficiency of off - policy approaches .",
        "aspect": "on - policy training",
        "sentiment": 0
    },
    {
        "text": "Ideally , one would like to combine the unbiasedness and stability of on - policy training with the data efficiency of off - policy approaches .",
        "aspect": "off - policy approaches",
        "sentiment": 0
    },
    {
        "text": "This desire has motivated substantial recent work on off - policy actor - critic methods , where the data efficiency of policy gradient is improved by training an offpolicy critic .",
        "aspect": "off - policy actor - critic methods",
        "sentiment": 0
    },
    {
        "text": "This desire has motivated substantial recent work on off - policy actor - critic methods , where the data efficiency of policy gradient is improved by training an offpolicy critic .",
        "aspect": "offpolicy critic",
        "sentiment": 0
    },
    {
        "text": "Although such methods have demonstrated improvements over on - policy actor - critic approaches , they have not resolved the theoretical difficulty associated with off - policy learning under function approximation .",
        "aspect": "on - policy actor - critic approaches",
        "sentiment": 0
    },
    {
        "text": "Although such methods have demonstrated improvements over on - policy actor - critic approaches , they have not resolved the theoretical difficulty associated with off - policy learning under function approximation .",
        "aspect": "off - policy learning",
        "sentiment": 0
    },
    {
        "text": "Although such methods have demonstrated improvements over on - policy actor - critic approaches , they have not resolved the theoretical difficulty associated with off - policy learning under function approximation .",
        "aspect": "function approximation",
        "sentiment": 0
    },
    {
        "text": "In this paper , we exploit a relationship between policy optimization under entropy regularization and softmax value consistency to obtain a new form of stable off - policy learning .",
        "aspect": "policy optimization",
        "sentiment": 1
    },
    {
        "text": "In this paper , we exploit a relationship between policy optimization under entropy regularization and softmax value consistency to obtain a new form of stable off - policy learning .",
        "aspect": "entropy regularization",
        "sentiment": 1
    },
    {
        "text": "In this paper , we exploit a relationship between policy optimization under entropy regularization and softmax value consistency to obtain a new form of stable off - policy learning .",
        "aspect": "softmax value consistency",
        "sentiment": 1
    },
    {
        "text": "In this paper , we exploit a relationship between policy optimization under entropy regularization and softmax value consistency to obtain a new form of stable off - policy learning .",
        "aspect": "off - policy learning",
        "sentiment": 1
    },
    {
        "text": "Even though entropy regularized policy optimization is a well studied topic in RL -in fact , one that has been attracting renewed interest from concurrent work -we contribute new observations to this study that are essential for the methods we propose : first , we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence ; second , we use this result to formulate a novel optimization objective that allows for a stable form of off - policy actor - critic learning ; finally , we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles .",
        "aspect": "entropy regularization",
        "sentiment": 1
    },
    {
        "text": "Even though entropy regularized policy optimization is a well studied topic in RL -in fact , one that has been attracting renewed interest from concurrent work -we contribute new observations to this study that are essential for the methods we propose : first , we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence ; second , we use this result to formulate a novel optimization objective that allows for a stable form of off - policy actor - critic learning ; finally , we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles .",
        "aspect": "off - policy actor - critic learning",
        "sentiment": 1
    },
    {
        "text": "In some cases entropy regularization is expressed in the form of relative entropy , and in other cases it is the standard entropy .",
        "aspect": "entropy regularization",
        "sentiment": 0
    },
    {
        "text": "Moreover , the algorithms proposed in those works are essentially single - step Q - learning variants , which suffer from the limitation of using single - step backups .",
        "aspect": "single - step Q - learning variants",
        "sentiment": 0
    },
    {
        "text": "Another recent work uses the softmax relationship in the limit of \u03c4 \u2192 0 and proposes to augment an actor - critic algorithm with offline updates that minimize a set of single - step hard - max Bellman errors .",
        "aspect": "actor - critic algorithm",
        "sentiment": 0
    },
    {
        "text": "The proposed PCL and Unified PCL algorithms bear some similarity to multi - step Q - learning , which rather than minimizing one - step hard - max Bellman error , optimizes a Q - value function approximator by unrolling the trajectory for some number of steps before using a hard - max backup .",
        "aspect": "PCL",
        "sentiment": 2
    },
    {
        "text": "The proposed PCL and Unified PCL algorithms bear some similarity to multi - step Q - learning , which rather than minimizing one - step hard - max Bellman error , optimizes a Q - value function approximator by unrolling the trajectory for some number of steps before using a hard - max backup .",
        "aspect": "Unified PCL algorithms",
        "sentiment": 2
    },
    {
        "text": "The proposed PCL and Unified PCL algorithms bear some similarity to multi - step Q - learning , which rather than minimizing one - step hard - max Bellman error , optimizes a Q - value function approximator by unrolling the trajectory for some number of steps before using a hard - max backup .",
        "aspect": "multi - step Q - learning",
        "sentiment": 0
    },
    {
        "text": "The proposed PCL and Unified PCL algorithms bear some similarity to multi - step Q - learning , which rather than minimizing one - step hard - max Bellman error , optimizes a Q - value function approximator by unrolling the trajectory for some number of steps before using a hard - max backup .",
        "aspect": "one - step hard - max Bellman error",
        "sentiment": 0
    },
    {
        "text": "The proposed PCL and Unified PCL algorithms bear some similarity to multi - step Q - learning , which rather than minimizing one - step hard - max Bellman error , optimizes a Q - value function approximator by unrolling the trajectory for some number of steps before using a hard - max backup .",
        "aspect": "Q - value function approximator",
        "sentiment": 0
    },
    {
        "text": "Previous work has used a Boltzmann weighted average operator .",
        "aspect": "Boltzmann weighted average operator",
        "sentiment": 0
    },
    {
        "text": "In particular , this operator has been used by to propose an iterative algorithm converging to the optimal maximum reward policy inspired by the work of .",
        "aspect": "iterative algorithm",
        "sentiment": 0
    },
    {
        "text": "In particular , this operator has been used by to propose an iterative algorithm converging to the optimal maximum reward policy inspired by the work of .",
        "aspect": "maximum reward policy",
        "sentiment": 0
    },
    {
        "text": "While they use the Boltzmann weighted average , they briefly mention that a softmax ( log - sum - exp ) operator would have similar theoretical properties .",
        "aspect": "Boltzmann weighted average",
        "sentiment": 0
    },
    {
        "text": "While they use the Boltzmann weighted average , they briefly mention that a softmax ( log - sum - exp ) operator would have similar theoretical properties .",
        "aspect": "softmax ( log - sum - exp ) operator",
        "sentiment": 0
    },
    {
        "text": "More recently proposed a mellowmax operator , defined as log - average - exp .",
        "aspect": "mellowmax operator",
        "sentiment": 0
    },
    {
        "text": "These log - averageexp operators share a similar non - expansion property , and the proofs of non - expansion are related .",
        "aspect": "log - averageexp operators",
        "sentiment": 0
    },
    {
        "text": "PCL exhibits comparable performance to A3C in some tasks , but clearly outperforms A3C on the more challenging tasks .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "PCL exhibits comparable performance to A3C in some tasks , but clearly outperforms A3C on the more challenging tasks .",
        "aspect": "A3C",
        "sentiment": 3
    },
    {
        "text": "Across all tasks , the performance of DQN is worse than PCL .",
        "aspect": "DQN",
        "sentiment": 3
    },
    {
        "text": "Across all tasks , the performance of DQN is worse than PCL .",
        "aspect": "PCL",
        "sentiment": 2
    },
    {
        "text": "Additionally it is possible to show that when restricted to an infinite horizon setting , the fixed point of the mellowmax operator is a constant shift of the Q * investigated here .",
        "aspect": "mellowmax operator",
        "sentiment": 0
    },
    {
        "text": "In all these cases , the suggested training algorithm optimizes a single - step consistency unlike PCL and Unified PCL , which optimizes a multi - step consistency .",
        "aspect": "single - step consistency",
        "sentiment": 0
    },
    {
        "text": "In all these cases , the suggested training algorithm optimizes a single - step consistency unlike PCL and Unified PCL , which optimizes a multi - step consistency .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "In all these cases , the suggested training algorithm optimizes a single - step consistency unlike PCL and Unified PCL , which optimizes a multi - step consistency .",
        "aspect": "Unified PCL",
        "sentiment": 0
    },
    {
        "text": "Finally , there has been a considerable amount of work in reinforcement learning using off - policy data to design more sample efficient algorithms .",
        "aspect": "off - policy data",
        "sentiment": 0
    },
    {
        "text": "Finally , there has been a considerable amount of work in reinforcement learning using off - policy data to design more sample efficient algorithms .",
        "aspect": "sample efficient algorithms",
        "sentiment": 0
    },
    {
        "text": "Previous work that has considered multi - step off - policy learning has typically used a correction ( e.g. , via importance - sampling or truncated importance sampling with bias correction , or eligibility traces ) .",
        "aspect": "multi - step off - policy learning",
        "sentiment": 0
    },
    {
        "text": "Previous work that has considered multi - step off - policy learning has typically used a correction ( e.g. , via importance - sampling or truncated importance sampling with bias correction , or eligibility traces ) .",
        "aspect": "importance - sampling",
        "sentiment": 0
    },
    {
        "text": "Previous work that has considered multi - step off - policy learning has typically used a correction ( e.g. , via importance - sampling or truncated importance sampling with bias correction , or eligibility traces ) .",
        "aspect": "truncated importance sampling",
        "sentiment": 0
    },
    {
        "text": "Previous work that has considered multi - step off - policy learning has typically used a correction ( e.g. , via importance - sampling or truncated importance sampling with bias correction , or eligibility traces ) .",
        "aspect": "bias correction",
        "sentiment": 0
    },
    {
        "text": "We evaluate the proposed algorithms , namely PCL & Unified PCL , across several different tasks and compare them to an A3C implementation , based on , and an implementation of double Q - learning with prioritized experience replay , based on .",
        "aspect": "PCL",
        "sentiment": 2
    },
    {
        "text": "We evaluate the proposed algorithms , namely PCL & Unified PCL , across several different tasks and compare them to an A3C implementation , based on , and an implementation of double Q - learning with prioritized experience replay , based on .",
        "aspect": "Unified PCL",
        "sentiment": 2
    },
    {
        "text": "We evaluate the proposed algorithms , namely PCL & Unified PCL , across several different tasks and compare them to an A3C implementation , based on , and an implementation of double Q - learning with prioritized experience replay , based on .",
        "aspect": "A3C",
        "sentiment": 3
    },
    {
        "text": "We evaluate the proposed algorithms , namely PCL & Unified PCL , across several different tasks and compare them to an A3C implementation , based on , and an implementation of double Q - learning with prioritized experience replay , based on .",
        "aspect": "double Q - learning",
        "sentiment": 3
    },
    {
        "text": "We find that PCL can consistently match or beat the performance of these baselines .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "We also provide a comparison between PCL and Unified PCL and find that the use of a single unified model for both values and policy can be competitive with PCL .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "We also provide a comparison between PCL and Unified PCL and find that the use of a single unified model for both values and policy can be competitive with PCL .",
        "aspect": "Unified PCL",
        "sentiment": 0
    },
    {
        "text": "We also provide a comparison between PCL and Unified PCL and find that the use of a single unified model for both values and policy can be competitive with PCL .",
        "aspect": "unified model",
        "sentiment": 0
    },
    {
        "text": "We also provide a comparison between PCL and Unified PCL and find that the use of a single unified model for both values and policy can be competitive with PCL .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "We present the results of each of the variants PCL , A3C , and DQN in Figure .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "We present the results of each of the variants PCL , A3C , and DQN in Figure .",
        "aspect": "DQN",
        "sentiment": 0
    },
    {
        "text": "We present the results of each of the variants PCL , A3C , and DQN in Figure .",
        "aspect": "A3C",
        "sentiment": 0
    },
    {
        "text": "PCL augmented with a small number of expert trajectories on the hardest algorithmic tasks .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "The gap between PCL and A3C is hard to discern in some of the more simple tasks such as Copy , Reverse , and RepeatCopy .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "The gap between PCL and A3C is hard to discern in some of the more simple tasks such as Copy , Reverse , and RepeatCopy .",
        "aspect": "A3C",
        "sentiment": 0
    },
    {
        "text": "Across all of the experiments , it is clear that the prioritized DQN performs worse than PCL .",
        "aspect": "prioritized DQN",
        "sentiment": 2
    },
    {
        "text": "These results suggest that PCL is a competitive RL algorithm , which in some cases significantly outperforms strong baselines .",
        "aspect": "PCL",
        "sentiment": 2
    },
    {
        "text": "We compare PCL to Unified PCL in Figure . The same protocol is performed to find the best hyperparameters and plot the average reward over several training iterations .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "We compare PCL to Unified PCL in Figure . The same protocol is performed to find the best hyperparameters and plot the average reward over several training iterations .",
        "aspect": "Unified PCL",
        "sentiment": 0
    },
    {
        "text": "We find that using a single model for both values and policy in Unified PCL is slightly detrimental on the simpler tasks , but on the more difficult tasks Unified PCL is competitive or even better than PCL .",
        "aspect": "Unified PCL",
        "sentiment": 0
    },
    {
        "text": "We find that using a single model for both values and policy in Unified PCL is slightly detrimental on the simpler tasks , but on the more difficult tasks Unified PCL is competitive or even better than PCL .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "We find that using a single model for both values and policy in Unified PCL is slightly detrimental on the simpler tasks , but on the more difficult tasks Unified PCL is competitive or even better than PCL .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "We present the results of PCL along with PCL augmented with expert trajectories in Figure .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "We present the results of PCL along with PCL augmented with expert trajectories in Figure .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "We performed similar experiments with Unified PCL and observed a similar lift from using expert trajectories .",
        "aspect": "Unified PCL",
        "sentiment": 0
    },
    {
        "text": "Incorporating expert trajectories in PCL is relatively trivial compared to the specialized methods developed for other policy based algorithms .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "Incorporating expert trajectories in PCL is relatively trivial compared to the specialized methods developed for other policy based algorithms .",
        "aspect": "policy based algorithms",
        "sentiment": 0
    },
    {
        "text": "Importantly , the ability of PCL to incorporate expert trajectories without requiring adjustment or correction is a desirable property in real - world applications such as robotics .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "We study the characteristics of the optimal policy and state values for a maximum expected reward objective in the presence of discounted entropy regularization .",
        "aspect": "discounted entropy regularization",
        "sentiment": 0
    },
    {
        "text": "The introduction of an entropy regularizer induces an interesting softmax consistency between the optimal policy and optimal state values , which may be expressed as either a single - step or multi - step consistency .",
        "aspect": "entropy regularizer",
        "sentiment": 0
    },
    {
        "text": "This softmax consistency leads to the development of Path Consistency Learning ( PCL ) , an RL algorithm that resembles actor - critic in that it maintains and jointly learns a model of the state values and a model of the policy , and is similar to Q - learning in that it minimizes a measure of temporal consistency error .",
        "aspect": "Path Consistency Learning",
        "sentiment": 0
    },
    {
        "text": "This softmax consistency leads to the development of Path Consistency Learning ( PCL ) , an RL algorithm that resembles actor - critic in that it maintains and jointly learns a model of the state values and a model of the policy , and is similar to Q - learning in that it minimizes a measure of temporal consistency error .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "This softmax consistency leads to the development of Path Consistency Learning ( PCL ) , an RL algorithm that resembles actor - critic in that it maintains and jointly learns a model of the state values and a model of the policy , and is similar to Q - learning in that it minimizes a measure of temporal consistency error .",
        "aspect": "RL algorithm",
        "sentiment": 0
    },
    {
        "text": "This softmax consistency leads to the development of Path Consistency Learning ( PCL ) , an RL algorithm that resembles actor - critic in that it maintains and jointly learns a model of the state values and a model of the policy , and is similar to Q - learning in that it minimizes a measure of temporal consistency error .",
        "aspect": "actor - critic",
        "sentiment": 0
    },
    {
        "text": "This softmax consistency leads to the development of Path Consistency Learning ( PCL ) , an RL algorithm that resembles actor - critic in that it maintains and jointly learns a model of the state values and a model of the policy , and is similar to Q - learning in that it minimizes a measure of temporal consistency error .",
        "aspect": "Q - learning",
        "sentiment": 0
    },
    {
        "text": "We also propose the variant Unified PCL which maintains a single model for both the policy and the values , thus upending the actor - critic paradigm of separating the actor from the critic .",
        "aspect": "Unified PCL",
        "sentiment": 2
    },
    {
        "text": "Unlike standard policy based RL algorithms , PCL and Unified PCL apply to both on - policy and off - policy trajectory samples .",
        "aspect": "policy based RL algorithms",
        "sentiment": 0
    },
    {
        "text": "Unlike standard policy based RL algorithms , PCL and Unified PCL apply to both on - policy and off - policy trajectory samples .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "Unlike standard policy based RL algorithms , PCL and Unified PCL apply to both on - policy and off - policy trajectory samples .",
        "aspect": "Unified PCL",
        "sentiment": 0
    },
    {
        "text": "Further , unlike value based RL algorithms , PCL and Unified PCL can take advantage of multi - step consistencies .",
        "aspect": "value based RL algorithms",
        "sentiment": 0
    },
    {
        "text": "Further , unlike value based RL algorithms , PCL and Unified PCL can take advantage of multi - step consistencies .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "Further , unlike value based RL algorithms , PCL and Unified PCL can take advantage of multi - step consistencies .",
        "aspect": "Unified PCL",
        "sentiment": 0
    },
    {
        "text": "Empirically , PCL and Unified PCL exhibit a significant improvement over baseline methods across several algorithmic benchmarks .",
        "aspect": "PCL",
        "sentiment": 0
    },
    {
        "text": "Empirically , PCL and Unified PCL exhibit a significant improvement over baseline methods across several algorithmic benchmarks .",
        "aspect": "Unified PCL",
        "sentiment": 0
    },
    {
        "text": "Deep Exploration via Bootstrapped DQN",
        "aspect": "Bootstrapped DQN",
        "sentiment": 0
    },
    {
        "text": "Common dithering strategies for exploration , such as -greedy , do not carry out temporally - extended ( or deep ) exploration ; this can lead to exponentially larger data requirements .",
        "aspect": "dithering strategies",
        "sentiment": 0
    },
    {
        "text": "Common dithering strategies for exploration , such as -greedy , do not carry out temporally - extended ( or deep ) exploration ; this can lead to exponentially larger data requirements .",
        "aspect": "-greedy",
        "sentiment": 0
    },
    {
        "text": "Randomized value functions offer a promising approach to efficient exploration with generalization , but existing algorithms are not compatible with nonlinearly parameterized value functions .",
        "aspect": "Randomized value functions",
        "sentiment": 0
    },
    {
        "text": "As a first step towards addressing such contexts we develop bootstrapped DQN .",
        "aspect": "bootstrapped DQN",
        "sentiment": 2
    },
    {
        "text": "We demonstrate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy .",
        "aspect": "bootstrapped DQN",
        "sentiment": 2
    },
    {
        "text": "We demonstrate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy .",
        "aspect": "deep exploration",
        "sentiment": 1
    },
    {
        "text": "We demonstrate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy .",
        "aspect": "deep neural networks",
        "sentiment": 1
    },
    {
        "text": "We demonstrate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy .",
        "aspect": "dithering strategy",
        "sentiment": 0
    },
    {
        "text": "In the Arcade Learning Environment bootstrapped DQN substantially improves learning speed and cumulative performance across most games .",
        "aspect": "bootstrapped DQN",
        "sentiment": 2
    },
    {
        "text": "Without this sort of temporally extended ( deep ) exploration , learning times can worsen by an exponential factor .",
        "aspect": "temporally extended ( deep ) exploration",
        "sentiment": 0
    },
    {
        "text": "However , most of these are designed for Markov decision processes ( MDPs ) with small finite state spaces , while others require solving computationally intractable planning tasks .",
        "aspect": "Markov decision processes",
        "sentiment": 0
    },
    {
        "text": "However , most of these are designed for Markov decision processes ( MDPs ) with small finite state spaces , while others require solving computationally intractable planning tasks .",
        "aspect": "MDPs",
        "sentiment": 0
    },
    {
        "text": "For this reason , large - scale applications of RL have relied upon statistically inefficient strategies for exploration or even no exploration at all .",
        "aspect": "RL",
        "sentiment": 0
    },
    {
        "text": "For this reason , large - scale applications of RL have relied upon statistically inefficient strategies for exploration or even no exploration at all .",
        "aspect": "statistically inefficient strategies",
        "sentiment": 0
    },
    {
        "text": "Common dithering strategies , such as -greedy , approximate the value of an action by a single number .",
        "aspect": "dithering strategies",
        "sentiment": 0
    },
    {
        "text": "Common dithering strategies , such as -greedy , approximate the value of an action by a single number .",
        "aspect": "-greedy",
        "sentiment": 0
    },
    {
        "text": "In this paper , we consider an alternative approach to efficient exploration inspired by Thompson sampling .",
        "aspect": "Thompson sampling",
        "sentiment": 1
    },
    {
        "text": "Recent work has shown that randomized value functions can implement something similar to Thompson sampling without the need for an intractable exact posterior update .",
        "aspect": "randomized value functions",
        "sentiment": 0
    },
    {
        "text": "Recent work has shown that randomized value functions can implement something similar to Thompson sampling without the need for an intractable exact posterior update .",
        "aspect": "Thompson sampling",
        "sentiment": 0
    },
    {
        "text": "We present a natural extension of this approach that enables use of complex non - linear generalization methods such as deep neural networks .",
        "aspect": "non - linear generalization methods",
        "sentiment": 1
    },
    {
        "text": "We present a natural extension of this approach that enables use of complex non - linear generalization methods such as deep neural networks .",
        "aspect": "deep neural networks",
        "sentiment": 1
    },
    {
        "text": "We show that the bootstrap with random initialization can produce reasonable uncertainty estimates for neural networks at low computational cost .",
        "aspect": "bootstrap with random initialization",
        "sentiment": 1
    },
    {
        "text": "Bootstrapped DQN leverages these uncertainty estimates for efficient ( and deep ) exploration .",
        "aspect": "Bootstrapped DQN",
        "sentiment": 2
    },
    {
        "text": "Bootstrapped DQN substantially reduces learning times and improves performance across most games .",
        "aspect": "Bootstrapped DQN",
        "sentiment": 0
    },
    {
        "text": "This algorithm is computationally efficient and parallelizable ; on a single machine our implementation runs roughly 20 % slower than DQN .",
        "aspect": "DQN",
        "sentiment": 3
    },
    {
        "text": "For any prior distribution over MDPs , the optimal exploration strategy is available through dynamic programming in the Bayesian belief state space .",
        "aspect": "MDPs",
        "sentiment": 0
    },
    {
        "text": "For any prior distribution over MDPs , the optimal exploration strategy is available through dynamic programming in the Bayesian belief state space .",
        "aspect": "dynamic programming",
        "sentiment": 0
    },
    {
        "text": "Many exploration strategies are guided by the principle of \" optimism in the face of uncertainty \" ( OFU ) .",
        "aspect": "optimism in the face of uncertainty",
        "sentiment": 0
    },
    {
        "text": "Many exploration strategies are guided by the principle of \" optimism in the face of uncertainty \" ( OFU ) .",
        "aspect": "OFU",
        "sentiment": 0
    },
    {
        "text": "Except for particular deterministic contexts , OFU methods that lead to efficient RL in complex domains have been computationally intractable .",
        "aspect": "OFU methods",
        "sentiment": 0
    },
    {
        "text": "The work of aims to add an effective bonus through a variation of DQN .",
        "aspect": "DQN",
        "sentiment": 1
    },
    {
        "text": "We compare our results on Atari to theirs in Appendix D and find that bootstrapped DQN offers a significant improvement over previous methods .",
        "aspect": "bootstrapped DQN",
        "sentiment": 2
    },
    {
        "text": "Perhaps the oldest heuristic for balancing exploration with exploitation is given by Thompson sampling .",
        "aspect": "Thompson sampling",
        "sentiment": 0
    },
    {
        "text": "This bandit algorithm takes a single sample from the posterior at every time step and chooses the action which is optimal for that time step .",
        "aspect": "bandit algorithm",
        "sentiment": 0
    },
    {
        "text": "To apply the Thompson sampling principle to RL , an agent should sample a value function from its posterior .",
        "aspect": "Thompson sampling principle",
        "sentiment": 0
    },
    {
        "text": "Naive applications of Thompson sampling to RL which resample every timestep can be extremely inefficient .",
        "aspect": "Thompson sampling",
        "sentiment": 0
    },
    {
        "text": "The algorithm PSRL does exactly this , with state of the art guarantees .",
        "aspect": "PSRL",
        "sentiment": 0
    },
    {
        "text": "However , this algorithm still requires solving a single known MDP , which will usually be intractable for large systems .",
        "aspect": "MDP",
        "sentiment": 0
    },
    {
        "text": "Our new algorithm , bootstrapped DQN , approximates this approach to exploration via randomized value functions sampled from an approximate posterior .",
        "aspect": "bootstrapped DQN",
        "sentiment": 2
    },
    {
        "text": "Our new algorithm , bootstrapped DQN , approximates this approach to exploration via randomized value functions sampled from an approximate posterior .",
        "aspect": "randomized value functions",
        "sentiment": 1
    },
    {
        "text": "Recently , authors have proposed the RLSVI algorithm which accomplishes this for linearly parameterized value functions .",
        "aspect": "RLSVI algorithm",
        "sentiment": 0
    },
    {
        "text": "Surprisingly , RLSVI recovers state of the art guarantees in the setting with tabular basis functions , but its performance is crucially dependent upon a suitable linear representation of the value function .",
        "aspect": "RLSVI",
        "sentiment": 0
    },
    {
        "text": "Our method is simple , general and compatible with almost all advances in deep RL at low computational cost and with few tuning parameters .",
        "aspect": "deep RL",
        "sentiment": 0
    },
    {
        "text": "We introduce \" talking - heads attention \" -a variation on multi - head attention which includes linear projections across the attention - heads dimension , immediately before and after the softmax operation .",
        "aspect": "talking - heads attention",
        "sentiment": 1
    },
    {
        "text": "* Noam Shazeer devised the talking - heads architecture , ran the T5 experiments and wrote most of the paper .",
        "aspect": "talking - heads architecture",
        "sentiment": 0
    },
    {
        "text": "Zhenzhong Lan had the initial idea of talking - heads attention , designed and coordinated part of the experiments .",
        "aspect": "talking - heads attention",
        "sentiment": 0
    },
    {
        "text": "Youlong Cheng reproduced BERT in MeshTensorFlow and run all the talking heads experiments for MeshTensorFlow BERT .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Youlong Cheng reproduced BERT in MeshTensorFlow and run all the talking heads experiments for MeshTensorFlow BERT .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Nan Ding ran the ALBERT experiments .",
        "aspect": "ALBERT",
        "sentiment": 0
    },
    {
        "text": "Neural Attention was introduced by as a way of extracting information from variablelength representations .",
        "aspect": "Neural Attention",
        "sentiment": 0
    },
    {
        "text": "The Transformer model uses \" multi - head \" attention , consisting of multiple attention layers ( \" heads \" ) in parallel , each with different projections on its inputs and outputs .",
        "aspect": "Transformer model",
        "sentiment": 0
    },
    {
        "text": "The Transformer model uses \" multi - head \" attention , consisting of multiple attention layers ( \" heads \" ) in parallel , each with different projections on its inputs and outputs .",
        "aspect": "attention layers",
        "sentiment": 0
    },
    {
        "text": "The Transformer model uses \" multi - head \" attention , consisting of multiple attention layers ( \" heads \" ) in parallel , each with different projections on its inputs and outputs .",
        "aspect": "uses \" multi - head \" attention",
        "sentiment": 0
    },
    {
        "text": "By using a dimensionality reduction in the input projections , the computational cost is kept similar to that of basic attention .",
        "aspect": "dimensionality reduction",
        "sentiment": 0
    },
    {
        "text": "In this paper , we introduce a new variant , \" talking - heads attention \" , that addresses this problem by inserting a learned linear projection across the attention - heads dimension of the attention - logits tensor .",
        "aspect": "talking - heads attention",
        "sentiment": 2
    },
    {
        "text": "We also insert a second such projection immediately following the softmax .",
        "aspect": "softmax",
        "sentiment": 1
    },
    {
        "text": "We have proposed talking - heads attention and shown some promising results .",
        "aspect": "talking - heads attention",
        "sentiment": 2
    },
    {
        "text": "Another potential approach is to decrease the number of memory - positions considered for each query - position -for example , by using the local - attention and memory - compressedattention approaches described in .",
        "aspect": "local - attention and memory - compressedattention approaches",
        "sentiment": 0
    },
    {
        "text": "Superglue : A stickier benchmark for general - purpose language understanding systems .",
        "aspect": "Superglue",
        "sentiment": 2
    },
    {
        "text": "Full Resolution Image Compression with Recurrent Neural Networks",
        "aspect": "Recurrent Neural Networks",
        "sentiment": 1
    },
    {
        "text": "This paper presents a set of full - resolution lossy image compression methods based on neural networks .",
        "aspect": "full - resolution lossy image compression methods",
        "sentiment": 0
    },
    {
        "text": "This paper presents a set of full - resolution lossy image compression methods based on neural networks .",
        "aspect": "neural networks",
        "sentiment": 1
    },
    {
        "text": "All of our architectures consist of a recurrent neural network ( RNN)-based encoder and decoder , a binarizer , and a neural network for entropy coding .",
        "aspect": "recurrent neural network ( RNN)-based encoder and decoder",
        "sentiment": 1
    },
    {
        "text": "All of our architectures consist of a recurrent neural network ( RNN)-based encoder and decoder , a binarizer , and a neural network for entropy coding .",
        "aspect": "binarizer",
        "sentiment": 1
    },
    {
        "text": "All of our architectures consist of a recurrent neural network ( RNN)-based encoder and decoder , a binarizer , and a neural network for entropy coding .",
        "aspect": "neural network",
        "sentiment": 1
    },
    {
        "text": "We compare RNN types ( LSTM , associative LSTM ) and introduce a new hybrid of GRU and ResNet .",
        "aspect": "LSTM",
        "sentiment": 0
    },
    {
        "text": "We compare RNN types ( LSTM , associative LSTM ) and introduce a new hybrid of GRU and ResNet .",
        "aspect": "associative LSTM",
        "sentiment": 0
    },
    {
        "text": "We compare RNN types ( LSTM , associative LSTM ) and introduce a new hybrid of GRU and ResNet .",
        "aspect": "ResNet",
        "sentiment": 1
    },
    {
        "text": "We compare RNN types ( LSTM , associative LSTM ) and introduce a new hybrid of GRU and ResNet .",
        "aspect": "GRU",
        "sentiment": 1
    },
    {
        "text": "We compare RNN types ( LSTM , associative LSTM ) and introduce a new hybrid of GRU and ResNet .",
        "aspect": "RNN",
        "sentiment": 0
    },
    {
        "text": "We also study \" one - shot \" versus additive reconstruction architectures and introduce a new scaled - additive framework .",
        "aspect": "scaled - additive framework",
        "sentiment": 0
    },
    {
        "text": "We also study \" one - shot \" versus additive reconstruction architectures and introduce a new scaled - additive framework .",
        "aspect": "one - shot",
        "sentiment": 0
    },
    {
        "text": "As far as we know , this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate - distortion curve on the Kodak dataset images , with and without the aid of entropy coding .",
        "aspect": "neural network architecture",
        "sentiment": 0
    },
    {
        "text": "As far as we know , this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate - distortion curve on the Kodak dataset images , with and without the aid of entropy coding .",
        "aspect": "JPEG",
        "sentiment": 0
    },
    {
        "text": "As far as we know , this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate - distortion curve on the Kodak dataset images , with and without the aid of entropy coding .",
        "aspect": "entropy coding",
        "sentiment": 1
    },
    {
        "text": "Image compression has traditionally been one of the tasks which neural networks were suspected to be good at , but there was little evidence that it would be possible to train a single neural network that would be competitive across compression rates and image sizes .",
        "aspect": "neural networks",
        "sentiment": 0
    },
    {
        "text": "Image compression has traditionally been one of the tasks which neural networks were suspected to be good at , but there was little evidence that it would be possible to train a single neural network that would be competitive across compression rates and image sizes .",
        "aspect": "neural network",
        "sentiment": 0
    },
    {
        "text": "Our goal is to provide a neural network which is competitive across compression rates on images of arbitrary size .",
        "aspect": "neural network",
        "sentiment": 1
    },
    {
        "text": "There are two possible ways to achieve this : 1 ) design a stronger patch - based residual encoder ; and 2 ) design an entropy coder that is able to capture long - term dependencies between patches in the image .",
        "aspect": "patch - based residual encoder",
        "sentiment": 0
    },
    {
        "text": "There are two possible ways to achieve this : 1 ) design a stronger patch - based residual encoder ; and 2 ) design an entropy coder that is able to capture long - term dependencies between patches in the image .",
        "aspect": "entropy coder",
        "sentiment": 0
    },
    {
        "text": "This idea was exploited in lossy image compression methods such as JPEG .",
        "aspect": "lossy image compression methods",
        "sentiment": 0
    },
    {
        "text": "This idea was exploited in lossy image compression methods such as JPEG .",
        "aspect": "JPEG",
        "sentiment": 0
    },
    {
        "text": "Autoencoders have been used to reduce the dimensionality of images , convert images to compressed binary codes for retrieval , and to extract compact visual representations that can be used in other applications .",
        "aspect": "Autoencoders",
        "sentiment": 0
    },
    {
        "text": "Autoencoders have been used to reduce the dimensionality of images , convert images to compressed binary codes for retrieval , and to extract compact visual representations that can be used in other applications .",
        "aspect": "compact visual representations",
        "sentiment": 0
    },
    {
        "text": "More recently , variational ( recurrent ) autoencoders have been directly applied to the problem of compression ( with results on images of size up to 64\u00d764 pixels ) , while non - variational recurrent neural networks were used to implement variablerate encoding .",
        "aspect": "variational ( recurrent ) autoencoders",
        "sentiment": 0
    },
    {
        "text": "More recently , variational ( recurrent ) autoencoders have been directly applied to the problem of compression ( with results on images of size up to 64\u00d764 pixels ) , while non - variational recurrent neural networks were used to implement variablerate encoding .",
        "aspect": "non - variational recurrent neural networks",
        "sentiment": 0
    },
    {
        "text": "More recently , variational ( recurrent ) autoencoders have been directly applied to the problem of compression ( with results on images of size up to 64\u00d764 pixels ) , while non - variational recurrent neural networks were used to implement variablerate encoding .",
        "aspect": "variablerate encoding",
        "sentiment": 0
    },
    {
        "text": "Most image compression neural networks use a fixed compression rate based on the size of a bottleneck layer .",
        "aspect": "bottleneck layer",
        "sentiment": 0
    },
    {
        "text": "This work extends previous methods by supporting variable rate compression while maintaining high compression rates beyond thumbnail - sized images .",
        "aspect": "variable rate compression",
        "sentiment": 1
    },
    {
        "text": "Training Setup : In order to evaluate the recurrent models we described , we used two sets of training data .",
        "aspect": "recurrent models",
        "sentiment": 0
    },
    {
        "text": "The second dataset takes a random sample of 6 million 1280\u00d7720 images on the web , decomposes the images into nonoverlapping 32\u00d732 tiles and samples 100 tiles that have the worst compression ratio when using the PNG compression algorithm .",
        "aspect": "PNG compression algorithm",
        "sentiment": 0
    },
    {
        "text": "By selecting the patches that compress the least under PNG , we intend to create a dataset with \" hardto - compress \" data .",
        "aspect": "PNG",
        "sentiment": 0
    },
    {
        "text": "The hypothesis is that training on such patches should yield a better compression model .",
        "aspect": "compression model",
        "sentiment": 0
    },
    {
        "text": "All network architectures were trained using the Tensorflow API , with the Adam optimizer .",
        "aspect": "network architectures",
        "sentiment": 0
    },
    {
        "text": "All network architectures were trained using the Tensorflow API , with the Adam optimizer .",
        "aspect": "Adam optimizer",
        "sentiment": 1
    },
    {
        "text": "We use Multi - Scale Structural Similarity ( MS - SSIM ) , a well - established metric for comparing lossy image compression algorithms , and the more recent Peak Signal to Noise Ratio -Human Visual System ( PSNR - HVS ) .",
        "aspect": "lossy image compression algorithms",
        "sentiment": 0
    },
    {
        "text": "The dataset consists of 24 768\u00d7512 PNG images ( landscape / portrait ) which were never compressed with a lossy algorithm .",
        "aspect": "lossy algorithm",
        "sentiment": 0
    },
    {
        "text": "Architectures : We ran experiments consisting of { GRU , Residual GRU , LSTM , Associative LSTM } \u00d7 { One Shot Reconstruction , Additive Reconstruction , Additive Rescaled Residual } and report the results for the best performing models after 1 million training steps .",
        "aspect": "GRU",
        "sentiment": 1
    },
    {
        "text": "Architectures : We ran experiments consisting of { GRU , Residual GRU , LSTM , Associative LSTM } \u00d7 { One Shot Reconstruction , Additive Reconstruction , Additive Rescaled Residual } and report the results for the best performing models after 1 million training steps .",
        "aspect": "Residual GRU",
        "sentiment": 1
    },
    {
        "text": "Architectures : We ran experiments consisting of { GRU , Residual GRU , LSTM , Associative LSTM } \u00d7 { One Shot Reconstruction , Additive Reconstruction , Additive Rescaled Residual } and report the results for the best performing models after 1 million training steps .",
        "aspect": "LSTM",
        "sentiment": 1
    },
    {
        "text": "Architectures : We ran experiments consisting of { GRU , Residual GRU , LSTM , Associative LSTM } \u00d7 { One Shot Reconstruction , Additive Reconstruction , Additive Rescaled Residual } and report the results for the best performing models after 1 million training steps .",
        "aspect": "Associative LSTM",
        "sentiment": 1
    },
    {
        "text": "Architectures : We ran experiments consisting of { GRU , Residual GRU , LSTM , Associative LSTM } \u00d7 { One Shot Reconstruction , Additive Reconstruction , Additive Rescaled Residual } and report the results for the best performing models after 1 million training steps .",
        "aspect": "One Shot Reconstruction",
        "sentiment": 1
    },
    {
        "text": "Architectures : We ran experiments consisting of { GRU , Residual GRU , LSTM , Associative LSTM } \u00d7 { One Shot Reconstruction , Additive Reconstruction , Additive Rescaled Residual } and report the results for the best performing models after 1 million training steps .",
        "aspect": "Additive Reconstruction",
        "sentiment": 1
    },
    {
        "text": "Architectures : We ran experiments consisting of { GRU , Residual GRU , LSTM , Associative LSTM } \u00d7 { One Shot Reconstruction , Additive Reconstruction , Additive Rescaled Residual } and report the results for the best performing models after 1 million training steps .",
        "aspect": "Additive Rescaled Residual",
        "sentiment": 1
    },
    {
        "text": "When using the 32\u00d732 training data , GRU ( One Shot ) had the highest performance in both metrics .",
        "aspect": "GRU",
        "sentiment": 0
    },
    {
        "text": "The LSTM model with Residual Scaling had the second highest MS - SSIM , while the Residual GRU had the second highest PSNR - HVS .",
        "aspect": "LSTM model",
        "sentiment": 0
    },
    {
        "text": "The LSTM model with Residual Scaling had the second highest MS - SSIM , while the Residual GRU had the second highest PSNR - HVS .",
        "aspect": "Residual Scaling",
        "sentiment": 0
    },
    {
        "text": "The LSTM model with Residual Scaling had the second highest MS - SSIM , while the Residual GRU had the second highest PSNR - HVS .",
        "aspect": "Residual GRU",
        "sentiment": 0
    },
    {
        "text": "When training on the High Entropy dataset , The One Shot version of LSTM had the highest MS - SSIM , but the worst PSNR - HVS .",
        "aspect": "One Shot version",
        "sentiment": 0
    },
    {
        "text": "When training on the High Entropy dataset , The One Shot version of LSTM had the highest MS - SSIM , but the worst PSNR - HVS .",
        "aspect": "LSTM",
        "sentiment": 0
    },
    {
        "text": "The GRU with \" one shot \" reconstruction ranked 2nd highest in both metrics , while the Residual GRU with \" one shot \" reconstruction had the highest PSNR - HVS .",
        "aspect": "GRU with \" one shot \" reconstruction",
        "sentiment": 0
    },
    {
        "text": "The GRU with \" one shot \" reconstruction ranked 2nd highest in both metrics , while the Residual GRU with \" one shot \" reconstruction had the highest PSNR - HVS .",
        "aspect": "Residual GRU with \" one shot \" reconstruction",
        "sentiment": 0
    },
    {
        "text": "Entropy Coding : The progressive entropy coder is trained for a specific image encoder , and we compare a subset of our models .",
        "aspect": "Entropy Coding",
        "sentiment": 0
    },
    {
        "text": "Entropy Coding : The progressive entropy coder is trained for a specific image encoder , and we compare a subset of our models .",
        "aspect": "image encoder",
        "sentiment": 0
    },
    {
        "text": "For training , we use a set of 1280\u00d7720 images that are encoded using one of the previous image encoders ( resulting in a 80\u00d745\u00d732 bitmap or 1 /8 bits per pixel per RNN iteration ) .",
        "aspect": "image encoders",
        "sentiment": 0
    },
    {
        "text": "Figure and Figure show that all models benefit from this additional entropy coding layer .",
        "aspect": "entropy coding layer",
        "sentiment": 0
    },
    {
        "text": "We apply the entropy coding model to the Baseline LSTM model , and the bit - rate saving ranges from 25 % at 2 bpp to 57 % at 0.25 bpp .",
        "aspect": "entropy coding model",
        "sentiment": 1
    },
    {
        "text": "We apply the entropy coding model to the Baseline LSTM model , and the bit - rate saving ranges from 25 % at 2 bpp to 57 % at 0.25 bpp .",
        "aspect": "LSTM model",
        "sentiment": 0
    },
    {
        "text": "We presented a general architecture for compressing with RNNs , content - based residual scaling , and a new variation of GRU , which provided the highest PSNR - HVS out of the models trained on the high entropy dataset .",
        "aspect": "RNNs",
        "sentiment": 1
    },
    {
        "text": "We presented a general architecture for compressing with RNNs , content - based residual scaling , and a new variation of GRU , which provided the highest PSNR - HVS out of the models trained on the high entropy dataset .",
        "aspect": "content - based residual scaling",
        "sentiment": 1
    },
    {
        "text": "We presented a general architecture for compressing with RNNs , content - based residual scaling , and a new variation of GRU , which provided the highest PSNR - HVS out of the models trained on the high entropy dataset .",
        "aspect": "GRU",
        "sentiment": 1
    },
    {
        "text": "However , we provided a set of models which perform well according to these metrics , and on average we achieve better than JPEG performance on both MS - SSIM AUC and PSNR - HVS AUC , both with and without entropy coding .",
        "aspect": "entropy coding",
        "sentiment": 1
    },
    {
        "text": "With that said , our models do benefit from the additional step of entropy coding due to the fact that in the early iterations the recurrent encoder models produce spatially correlated codes .",
        "aspect": "entropy coding",
        "sentiment": 1
    },
    {
        "text": "With that said , our models do benefit from the additional step of entropy coding due to the fact that in the early iterations the recurrent encoder models produce spatially correlated codes .",
        "aspect": "recurrent encoder models",
        "sentiment": 0
    },
    {
        "text": "Additionally , we are open sourcing our best Residual GRU model and our Entropy Coder training and evaluation in https://github.com/tensorflow/models/tree/master/comp",
        "aspect": "Residual GRU model",
        "sentiment": 0
    },
    {
        "text": "Additionally , we are open sourcing our best Residual GRU model and our Entropy Coder training and evaluation in https://github.com/tensorflow/models/tree/master/comp",
        "aspect": "Entropy Coder training",
        "sentiment": 0
    },
    {
        "text": "The next challenge will be besting compression methods derived from video compression codecs , such as WebP ( which was derived from VP8 video codec ) , on large images since they employ tricks such as reusing patches that were already decoded .",
        "aspect": "compression methods",
        "sentiment": 0
    },
    {
        "text": "Additionally training the entropy coder ( BinaryRNN ) and the patch - based encoder jointly and on larger patches should allow us to choose a trade - off between the efficiency of the patch - based encoder and the predictive power of the entropy coder .",
        "aspect": "entropy coder",
        "sentiment": 0
    },
    {
        "text": "Additionally training the entropy coder ( BinaryRNN ) and the patch - based encoder jointly and on larger patches should allow us to choose a trade - off between the efficiency of the patch - based encoder and the predictive power of the entropy coder .",
        "aspect": "BinaryRNN",
        "sentiment": 0
    },
    {
        "text": "Additionally training the entropy coder ( BinaryRNN ) and the patch - based encoder jointly and on larger patches should allow us to choose a trade - off between the efficiency of the patch - based encoder and the predictive power of the entropy coder .",
        "aspect": "patch - based encoder",
        "sentiment": 0
    },
    {
        "text": "Additionally training the entropy coder ( BinaryRNN ) and the patch - based encoder jointly and on larger patches should allow us to choose a trade - off between the efficiency of the patch - based encoder and the predictive power of the entropy coder .",
        "aspect": "patch - based encoder",
        "sentiment": 0
    },
    {
        "text": "Additionally training the entropy coder ( BinaryRNN ) and the patch - based encoder jointly and on larger patches should allow us to choose a trade - off between the efficiency of the patch - based encoder and the predictive power of the entropy coder .",
        "aspect": "entropy coder",
        "sentiment": 0
    },
    {
        "text": "No entropy coding was used .",
        "aspect": "entropy coding",
        "sentiment": 0
    },
    {
        "text": "After entropy coding , the AUC will be higher for the network - based approaches .",
        "aspect": "entropy coding",
        "sentiment": 0
    },
    {
        "text": "After entropy coding , the AUC will be higher for the network - based approaches .",
        "aspect": "network - based approaches",
        "sentiment": 0
    },
    {
        "text": "DRAGNN : A Transition - based Framework for Dynamically Connected Neural Networks",
        "aspect": "DRAGNN",
        "sentiment": 0
    },
    {
        "text": "DRAGNN : A Transition - based Framework for Dynamically Connected Neural Networks",
        "aspect": "Transition - based Framework",
        "sentiment": 0
    },
    {
        "text": "DRAGNN : A Transition - based Framework for Dynamically Connected Neural Networks",
        "aspect": "Dynamically Connected Neural Networks",
        "sentiment": 0
    },
    {
        "text": "In this work , we present a compact , modular framework for constructing novel recurrent neural architectures .",
        "aspect": "modular framework",
        "sentiment": 0
    },
    {
        "text": "In this work , we present a compact , modular framework for constructing novel recurrent neural architectures .",
        "aspect": "recurrent neural architectures",
        "sentiment": 0
    },
    {
        "text": "Our basic module is a new generic unit , the Transition Based Recurrent Unit ( TBRU ) .",
        "aspect": "Transition Based Recurrent Unit",
        "sentiment": 2
    },
    {
        "text": "Our basic module is a new generic unit , the Transition Based Recurrent Unit ( TBRU ) .",
        "aspect": "TBRU",
        "sentiment": 2
    },
    {
        "text": "In addition to hidden layer activations , TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations .",
        "aspect": "TBRUs",
        "sentiment": 0
    },
    {
        "text": "By connecting multiple TBRUs , we can extend and combine commonly used architectures such as sequence - tosequence , attention mechanisms , and recursive tree - structured models .",
        "aspect": "TBRUs",
        "sentiment": 0
    },
    {
        "text": "By connecting multiple TBRUs , we can extend and combine commonly used architectures such as sequence - tosequence , attention mechanisms , and recursive tree - structured models .",
        "aspect": "sequence - tosequence",
        "sentiment": 0
    },
    {
        "text": "By connecting multiple TBRUs , we can extend and combine commonly used architectures such as sequence - tosequence , attention mechanisms , and recursive tree - structured models .",
        "aspect": "attention mechanisms",
        "sentiment": 0
    },
    {
        "text": "By connecting multiple TBRUs , we can extend and combine commonly used architectures such as sequence - tosequence , attention mechanisms , and recursive tree - structured models .",
        "aspect": "recursive tree - structured models",
        "sentiment": 0
    },
    {
        "text": "A TBRU can also serve as both an encoder for downstream tasks and as a decoder for its own task simultaneously , resulting in more accurate multi - task learning .",
        "aspect": "TBRU",
        "sentiment": 0
    },
    {
        "text": "A TBRU can also serve as both an encoder for downstream tasks and as a decoder for its own task simultaneously , resulting in more accurate multi - task learning .",
        "aspect": "encoder",
        "sentiment": 0
    },
    {
        "text": "A TBRU can also serve as both an encoder for downstream tasks and as a decoder for its own task simultaneously , resulting in more accurate multi - task learning .",
        "aspect": "decoder",
        "sentiment": 0
    },
    {
        "text": "We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks , or DRAGNN .",
        "aspect": "Dynamic Recurrent Acyclic Graphical Neural Networks",
        "sentiment": 2
    },
    {
        "text": "We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks , or DRAGNN .",
        "aspect": "DRAGNN",
        "sentiment": 2
    },
    {
        "text": "We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multitask learning for extractive summarization tasks .",
        "aspect": "DRAGNN",
        "sentiment": 0
    },
    {
        "text": "We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multitask learning for extractive summarization tasks .",
        "aspect": "seq2seq",
        "sentiment": 0
    },
    {
        "text": "We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multitask learning for extractive summarization tasks .",
        "aspect": "attention",
        "sentiment": 0
    },
    {
        "text": "We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multitask learning for extractive summarization tasks .",
        "aspect": "multitask learning",
        "sentiment": 0
    },
    {
        "text": "To apply deep learning models to structured prediction , machine learning practitioners must address two primary issues : ( 1 ) how to represent the input , and ( 2 ) how to represent the output .",
        "aspect": "deep learning models",
        "sentiment": 0
    },
    {
        "text": "The seq2seq encoder / decoder framework proposes solving these generically .",
        "aspect": "seq2seq encoder / decoder framework",
        "sentiment": 0
    },
    {
        "text": "In its simplest form , the encoder network produces a fixed - length vector representation of an input , while the decoder network pro - duces a linearization of the target output structure as a sequence of output symbols .",
        "aspect": "encoder network",
        "sentiment": 0
    },
    {
        "text": "In its simplest form , the encoder network produces a fixed - length vector representation of an input , while the decoder network pro - duces a linearization of the target output structure as a sequence of output symbols .",
        "aspect": "fixed - length vector representation",
        "sentiment": 0
    },
    {
        "text": "In its simplest form , the encoder network produces a fixed - length vector representation of an input , while the decoder network pro - duces a linearization of the target output structure as a sequence of output symbols .",
        "aspect": "decoder network",
        "sentiment": 0
    },
    {
        "text": "Encoder / decoder is state of the art for several key tasks in natural language processing , such as machine translation .",
        "aspect": "Encoder / decoder",
        "sentiment": 0
    },
    {
        "text": "However , fixed - size encodings become less competitive when the input structure can be explicitly mapped to the output .",
        "aspect": "fixed - size encodings",
        "sentiment": 0
    },
    {
        "text": "In the simple case of predicting tags for individual tokens in a sentence , state - of - the - art taggers learn vector representations for each input token and predict output tags from those .",
        "aspect": "taggers",
        "sentiment": 0
    },
    {
        "text": "When the input or output is a syntactic parse tree , networks that explicitly operate over the compositional structure of the network typically outperform generic representations .",
        "aspect": "generic representations",
        "sentiment": 0
    },
    {
        "text": "Implictly learned mappings via attention mechanisms can significantly improve the performance of sequence - to - sequence , but require runtime that 's quadratic in the input size .",
        "aspect": "Implictly learned mappings",
        "sentiment": 0
    },
    {
        "text": "Implictly learned mappings via attention mechanisms can significantly improve the performance of sequence - to - sequence , but require runtime that 's quadratic in the input size .",
        "aspect": "attention mechanisms",
        "sentiment": 0
    },
    {
        "text": "In this work , we propose a modular neural architecture that generalizes the encoder / decoder concept to include explicit structure .",
        "aspect": "encoder / decoder concept",
        "sentiment": 0
    },
    {
        "text": "In this work , we propose a modular neural architecture that generalizes the encoder / decoder concept to include explicit structure .",
        "aspect": "modular neural architecture",
        "sentiment": 0
    },
    {
        "text": "Our framework can represent sequence - to - sequence learning as well as models with explicit structure like bi - directional tagging models and compositional , tree - structured models .",
        "aspect": "bi - directional tagging models",
        "sentiment": 0
    },
    {
        "text": "Our framework can represent sequence - to - sequence learning as well as models with explicit structure like bi - directional tagging models and compositional , tree - structured models .",
        "aspect": "compositional , tree - structured models",
        "sentiment": 0
    },
    {
        "text": "Transition - based neural networks have recently been applied to a wide variety of arXiv:1703.04474v1",
        "aspect": "Transition - based neural networks",
        "sentiment": 0
    },
    {
        "text": "[ cs . CL ] 13 Mar 2017 We generalize these approaches with a new basic module , the Transition - Based Recurrent Unit ( TBRU ) , which produces a vector representation for every transition state in the output linearization ( Figure ) .",
        "aspect": "Transition - Based Recurrent Unit",
        "sentiment": 2
    },
    {
        "text": "[ cs . CL ] 13 Mar 2017 We generalize these approaches with a new basic module , the Transition - Based Recurrent Unit ( TBRU ) , which produces a vector representation for every transition state in the output linearization ( Figure ) .",
        "aspect": "TBRU",
        "sentiment": 2
    },
    {
        "text": "[ cs . CL ] 13 Mar 2017 We generalize these approaches with a new basic module , the Transition - Based Recurrent Unit ( TBRU ) , which produces a vector representation for every transition state in the output linearization ( Figure ) .",
        "aspect": "vector representation",
        "sentiment": 0
    },
    {
        "text": "For example , a TBRU that attaches two sub - trees while building a syntactic parse tree will also produce the hidden layer activations to serve as an encoding for the newly constructed phrase .",
        "aspect": "TBRU",
        "sentiment": 0
    },
    {
        "text": "Multiple TBRUs can be connected and learned jointly to add explicit structure to multi - task learning setups and share representations between tasks with different input or output spaces ( Figure ) .",
        "aspect": "TBRUs",
        "sentiment": 0
    },
    {
        "text": "This inference procedure will construct an acyclic compute graph representing the network architecture , where recurrent connections are dynamically added as the network unfolds .",
        "aspect": "inference procedure",
        "sentiment": 0
    },
    {
        "text": "This inference procedure will construct an acyclic compute graph representing the network architecture , where recurrent connections are dynamically added as the network unfolds .",
        "aspect": "acyclic compute graph",
        "sentiment": 0
    },
    {
        "text": "We therefore call our approach Dynamic Recurrent Acyclic Graphical Neural Networks , or DRAGNN .",
        "aspect": "Dynamic Recurrent Acyclic Graphical Neural Networks",
        "sentiment": 2
    },
    {
        "text": "We therefore call our approach Dynamic Recurrent Acyclic Graphical Neural Networks , or DRAGNN .",
        "aspect": "DRAGNN",
        "sentiment": 2
    },
    {
        "text": "DRAGNN has several distinct modeling advantages over traditional fixed neural architectures .",
        "aspect": "DRAGNN",
        "sentiment": 2
    },
    {
        "text": "DRAGNN has several distinct modeling advantages over traditional fixed neural architectures .",
        "aspect": "fixed neural architectures",
        "sentiment": 0
    },
    {
        "text": "Unlike generic seq2seq , DRAGNN supports variable sized input representations that may contain explicit structure .",
        "aspect": "seq2seq",
        "sentiment": 0
    },
    {
        "text": "Unlike generic seq2seq , DRAGNN supports variable sized input representations that may contain explicit structure .",
        "aspect": "DRAGNN",
        "sentiment": 0
    },
    {
        "text": "Unlike purely sequential RNNs , the dynamic connections in a DRAGNN can span arbitrary distances in the input space .",
        "aspect": "sequential RNNs",
        "sentiment": 0
    },
    {
        "text": "Unlike purely sequential RNNs , the dynamic connections in a DRAGNN can span arbitrary distances in the input space .",
        "aspect": "DRAGNN",
        "sentiment": 0
    },
    {
        "text": "Crucially , inference remains linear in the size of the input , in contrast to quadratic - time attention mechanisms .",
        "aspect": "quadratic - time attention mechanisms",
        "sentiment": 0
    },
    {
        "text": "Dynamic connections thus establish a compromise between pure seq2seq and pure attention architec - tures by providing a finite set of long - range inputs that ' attend ' to relevant portions of the input space .",
        "aspect": "attention architec - tures",
        "sentiment": 0
    },
    {
        "text": "Unlike recursive neural networks DRAGNN can both predict intermediate structures ( such as parse trees ) and utilize those structures in a single deep model , backpropagating downstream task errors through the intermediate structures .",
        "aspect": "recursive neural networks",
        "sentiment": 0
    },
    {
        "text": "Unlike recursive neural networks DRAGNN can both predict intermediate structures ( such as parse trees ) and utilize those structures in a single deep model , backpropagating downstream task errors through the intermediate structures .",
        "aspect": "DRAGNN",
        "sentiment": 0
    },
    {
        "text": "Unlike recursive neural networks DRAGNN can both predict intermediate structures ( such as parse trees ) and utilize those structures in a single deep model , backpropagating downstream task errors through the intermediate structures .",
        "aspect": "deep model",
        "sentiment": 0
    },
    {
        "text": "Compared to models such as Stack - LSTM and , TBRUs are a more general formulation that allows incorporating dynamically structured multi - task learning and more varied network architectures .",
        "aspect": "Stack - LSTM",
        "sentiment": 0
    },
    {
        "text": "Compared to models such as Stack - LSTM and , TBRUs are a more general formulation that allows incorporating dynamically structured multi - task learning and more varied network architectures .",
        "aspect": "TBRUs",
        "sentiment": 0
    },
    {
        "text": "Compared to models such as Stack - LSTM and , TBRUs are a more general formulation that allows incorporating dynamically structured multi - task learning and more varied network architectures .",
        "aspect": "network architectures",
        "sentiment": 0
    },
    {
        "text": "In sum , DRAGNN is not a particular neural architecture , but rather a formulation for describing neural architectures compactly .",
        "aspect": "DRAGNN",
        "sentiment": 0
    },
    {
        "text": "In sum , DRAGNN is not a particular neural architecture , but rather a formulation for describing neural architectures compactly .",
        "aspect": "neural architecture",
        "sentiment": 0
    },
    {
        "text": "In sum , DRAGNN is not a particular neural architecture , but rather a formulation for describing neural architectures compactly .",
        "aspect": "neural architectures",
        "sentiment": 0
    },
    {
        "text": "The key to this compact description is a new recurrent unit - the TBRU - which allows connections between nodes in an unrolled compute graph to be specified dynamically in a generic fashion .",
        "aspect": "recurrent unit",
        "sentiment": 0
    },
    {
        "text": "The key to this compact description is a new recurrent unit - the TBRU - which allows connections between nodes in an unrolled compute graph to be specified dynamically in a generic fashion .",
        "aspect": "TBRU",
        "sentiment": 0
    },
    {
        "text": "We utilize transition systems to provide succinct , discrete representations via linearizations of both the input and the output for structured prediction .",
        "aspect": "transition systems",
        "sentiment": 0
    },
    {
        "text": "We utilize transition systems to provide succinct , discrete representations via linearizations of both the input and the output for structured prediction .",
        "aspect": "discrete representations",
        "sentiment": 0
    },
    {
        "text": "We demonstrate the effectiveness of DRAGNN on two NLP tasks that benefit from explicit structure : dependency parsing and extractive sentence summarization .",
        "aspect": "DRAGNN",
        "sentiment": 0
    },
    {
        "text": "First , we show how to use TBRUs to incrementally add structure to the input and output of a \" vanilla \" seq2seq dependency parsing model , dramatically boosting accuracy over seq2seq with no additional .",
        "aspect": "TBRUs",
        "sentiment": 0
    },
    {
        "text": "First , we show how to use TBRUs to incrementally add structure to the input and output of a \" vanilla \" seq2seq dependency parsing model , dramatically boosting accuracy over seq2seq with no additional .",
        "aspect": "seq2seq",
        "sentiment": 0
    },
    {
        "text": "Bottom left : Extending the \" stack - propagation \" idea to included dependency parse trees as intermediate representations .",
        "aspect": "intermediate representations",
        "sentiment": 0
    },
    {
        "text": "Bottom left : Extending the \" stack - propagation \" idea to included dependency parse trees as intermediate representations .",
        "aspect": "stack - propagation",
        "sentiment": 0
    },
    {
        "text": "Right : Unrolled TBRUs for each setup for a input fragment \" Uniformed man laughed \" , utilizing the transition systems described in Section 4 .",
        "aspect": "transition systems",
        "sentiment": 0
    },
    {
        "text": "Second , we demonstrate how the same TBRUs can be used to provide structured intermediate syntactic representations for extractive sentence summarization .",
        "aspect": "TBRUs",
        "sentiment": 0
    },
    {
        "text": "Second , we demonstrate how the same TBRUs can be used to provide structured intermediate syntactic representations for extractive sentence summarization .",
        "aspect": "structured intermediate syntactic representations",
        "sentiment": 0
    },
    {
        "text": "This yields better accuracy than is possible with the generic multi - task seq2seq approach .",
        "aspect": "seq2seq",
        "sentiment": 0
    },
    {
        "text": "Finally , we show how multiple TBRUs for the same dependency parsing task can be stacked together to produce a single state - of - the - art dependency parsing model .",
        "aspect": "TBRUs",
        "sentiment": 0
    },
    {
        "text": "Finally , we show how multiple TBRUs for the same dependency parsing task can be stacked together to produce a single state - of - the - art dependency parsing model .",
        "aspect": "dependency parsing model",
        "sentiment": 0
    },
    {
        "text": "We implement our method in Tensor - Flow , using mini - batches of size 4 and following the averaged momentum training and hyperparameter tuning procedure of .",
        "aspect": "averaged momentum training",
        "sentiment": 1
    },
    {
        "text": "We implement our method in Tensor - Flow , using mini - batches of size 4 and following the averaged momentum training and hyperparameter tuning procedure of .",
        "aspect": "hyperparameter tuning procedure",
        "sentiment": 1
    },
    {
        "text": "Using explicit structure improves encoder / decoder We explore the impact of different types of recurrences on dependency parsing in Table .",
        "aspect": "encoder / decoder",
        "sentiment": 0
    },
    {
        "text": "In this setup , we used relatively small models : single - layer LSTMs with 256 hidden units , taking 32 - dimensional word or output symbol embeddings as input to each cell .",
        "aspect": "single - layer LSTMs",
        "sentiment": 1
    },
    {
        "text": "In each case , the parsing TBRU takes input from a right - to - left shift - only TBRU .",
        "aspect": "parsing TBRU",
        "sentiment": 0
    },
    {
        "text": "In each case , the parsing TBRU takes input from a right - to - left shift - only TBRU .",
        "aspect": "TBRU",
        "sentiment": 0
    },
    {
        "text": "Under these settings , the pure encoder / decoder seq2seq model simply does not have the capacity to parse newswire text with any degree of accuracy , but the TBRU - based approach is nearly state - of - the - art at the same exact computational cost .",
        "aspect": "seq2seq",
        "sentiment": 0
    },
    {
        "text": "Under these settings , the pure encoder / decoder seq2seq model simply does not have the capacity to parse newswire text with any degree of accuracy , but the TBRU - based approach is nearly state - of - the - art at the same exact computational cost .",
        "aspect": "TBRU",
        "sentiment": 0
    },
    {
        "text": "As a point of comparison and an alternative to using input pointers , we also implemented an attention mechanism within DRAGNN .",
        "aspect": "attention mechanism",
        "sentiment": 1
    },
    {
        "text": "As a point of comparison and an alternative to using input pointers , we also implemented an attention mechanism within DRAGNN .",
        "aspect": "DRAGNN",
        "sentiment": 0
    },
    {
        "text": "We used the dot - product formulation from Parikh et al . ( ) , where r(s i ) in the parser takes in all of the shift - only TBRU 's hidden states and RNN aggregates over them .",
        "aspect": "RNN",
        "sentiment": 0
    },
    {
        "text": "We used the dot - product formulation from Parikh et al . ( ) , where r(s i ) in the parser takes in all of the shift - only TBRU 's hidden states and RNN aggregates over them .",
        "aspect": "TBRU",
        "sentiment": 0
    },
    {
        "text": "Utilizing parse representations improves summarization We evaluate our approach on the summarization task in Table .",
        "aspect": "parse representations",
        "sentiment": 0
    },
    {
        "text": "We compare two single - task LSTM tagging baselines against two multi - task approaches : an adaptation of and the stack - propagation idea of .",
        "aspect": "single - task LSTM tagging baselines",
        "sentiment": 0
    },
    {
        "text": "We compare two single - task LSTM tagging baselines against two multi - task approaches : an adaptation of and the stack - propagation idea of .",
        "aspect": "multi - task approaches",
        "sentiment": 0
    },
    {
        "text": "We compare two single - task LSTM tagging baselines against two multi - task approaches : an adaptation of and the stack - propagation idea of .",
        "aspect": "stack - propagation idea",
        "sentiment": 0
    },
    {
        "text": "In both multi - task setups , we use a right - to - left shift - only TBRU to encode the input , and connect it to both our compositional arc - standard dependency parser and the Keep / Drop summarization tagging model .",
        "aspect": "compositional arc - standard dependency parser",
        "sentiment": 0
    },
    {
        "text": "In both multi - task setups , we use a right - to - left shift - only TBRU to encode the input , and connect it to both our compositional arc - standard dependency parser and the Keep / Drop summarization tagging model .",
        "aspect": "Keep / Drop summarization tagging model",
        "sentiment": 0
    },
    {
        "text": "In both multi - task setups , we use a right - to - left shift - only TBRU to encode the input , and connect it to both our compositional arc - standard dependency parser and the Keep / Drop summarization tagging model .",
        "aspect": "TBRU",
        "sentiment": 0
    },
    {
        "text": "In both setups we do not follow seq2seq , but utilize the INPUT function to connect output decisions directly to input token representations .",
        "aspect": "token representations",
        "sentiment": 0
    },
    {
        "text": "However , in the stack - prop case , we use the SUBTREE function to connect the tagging TBRU to the parser TBRU 's phrase representations directly ( Figure ) .",
        "aspect": "SUBTREE function",
        "sentiment": 0
    },
    {
        "text": "However , in the stack - prop case , we use the SUBTREE function to connect the tagging TBRU to the parser TBRU 's phrase representations directly ( Figure ) .",
        "aspect": "tagging TBRU",
        "sentiment": 1
    },
    {
        "text": "However , in the stack - prop case , we use the SUBTREE function to connect the tagging TBRU to the parser TBRU 's phrase representations directly ( Figure ) .",
        "aspect": "parser TBRU",
        "sentiment": 1
    },
    {
        "text": "We find that allowing the compressor to directly use the parser 's phrase representations significantly improves the outcome of the multi - task learning setup .",
        "aspect": "compressor",
        "sentiment": 0
    },
    {
        "text": "We find that allowing the compressor to directly use the parser 's phrase representations significantly improves the outcome of the multi - task learning setup .",
        "aspect": "parser 's phrase representations",
        "sentiment": 0
    },
    {
        "text": "Deep stacked bi - directional parsing Here we propose a continuous version of the bi - directional parsing model of : first , the sentence is parsed in the left - to - right order as usual ; then a right - to - left transition system analyzes the sentence in reverse order using addition features extracted from the left - to - right parser .",
        "aspect": "bi - directional parsing model",
        "sentiment": 0
    },
    {
        "text": "Deep stacked bi - directional parsing Here we propose a continuous version of the bi - directional parsing model of : first , the sentence is parsed in the left - to - right order as usual ; then a right - to - left transition system analyzes the sentence in reverse order using addition features extracted from the left - to - right parser .",
        "aspect": "right - to - left transition system",
        "sentiment": 0
    },
    {
        "text": "Deep stacked bi - directional parsing Here we propose a continuous version of the bi - directional parsing model of : first , the sentence is parsed in the left - to - right order as usual ; then a right - to - left transition system analyzes the sentence in reverse order using addition features extracted from the left - to - right parser .",
        "aspect": "left - to - right parser",
        "sentiment": 0
    },
    {
        "text": "In our version , we connect the right - to - left parsing TBRU directly to the phrase representations of the left - to - right parsing TBRU , again using the SUBTREE function .",
        "aspect": "right - to - left parsing TBRU",
        "sentiment": 0
    },
    {
        "text": "In our version , we connect the right - to - left parsing TBRU directly to the phrase representations of the left - to - right parsing TBRU , again using the SUBTREE function .",
        "aspect": "phrase representations",
        "sentiment": 0
    },
    {
        "text": "In our version , we connect the right - to - left parsing TBRU directly to the phrase representations of the left - to - right parsing TBRU , again using the SUBTREE function .",
        "aspect": "left - to - right parsing TBRU",
        "sentiment": 0
    },
    {
        "text": "In our version , we connect the right - to - left parsing TBRU directly to the phrase representations of the left - to - right parsing TBRU , again using the SUBTREE function .",
        "aspect": "SUBTREE function",
        "sentiment": 0
    },
    {
        "text": "Our final model uses 5 TBRU units .",
        "aspect": "TBRU units",
        "sentiment": 1
    },
    {
        "text": "Inspired by , a left - to - right POS tagging TBRU provides the first layer of representations .",
        "aspect": "left - to - right POS tagging TBRU",
        "sentiment": 0
    },
    {
        "text": "Next , two shift - only TBRUs , one in each direction , provide representations to the parsers .",
        "aspect": "shift - only TBRUs",
        "sentiment": 0
    },
    {
        "text": "Finally , we connect the left - to - right parser to the right - to - left parser using links defined via the SUBTREE function .",
        "aspect": "left - to - right parser",
        "sentiment": 0
    },
    {
        "text": "Finally , we connect the left - to - right parser to the right - to - left parser using links defined via the SUBTREE function .",
        "aspect": "SUBTREE function",
        "sentiment": 0
    },
    {
        "text": "The result ( Table ) is a state - of - the - art dependency parser , yielding the highest published accuracy on the Treebank Union setup for both part of speech tagging and parsing .",
        "aspect": "dependency parser",
        "sentiment": 0
    },
    {
        "text": "We presented a compact , modular framework for describing recurrent neural architectures .",
        "aspect": "recurrent neural architectures",
        "sentiment": 0
    },
    {
        "text": "We evaluated our dynamically structured model and found it to be significantly more efficient and accurate than attention mechanisms for dependency parsing and extractive sentence summarization in both single - and multi - task setups .",
        "aspect": "dynamically structured model",
        "sentiment": 0
    },
    {
        "text": "We evaluated our dynamically structured model and found it to be significantly more efficient and accurate than attention mechanisms for dependency parsing and extractive sentence summarization in both single - and multi - task setups .",
        "aspect": "attention mechanisms",
        "sentiment": 0
    },
    {
        "text": "Under review as a conference paper at ICLR 2016 UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
        "aspect": "UNSUPERVISED REPRESENTATION LEARNING",
        "sentiment": 0
    },
    {
        "text": "Under review as a conference paper at ICLR 2016 UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
        "aspect": "DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS",
        "sentiment": 0
    },
    {
        "text": "In recent years , supervised learning with convolutional networks ( CNNs ) has seen huge adoption in computer vision applications .",
        "aspect": "supervised learning",
        "sentiment": 0
    },
    {
        "text": "In recent years , supervised learning with convolutional networks ( CNNs ) has seen huge adoption in computer vision applications .",
        "aspect": "convolutional networks",
        "sentiment": 0
    },
    {
        "text": "In recent years , supervised learning with convolutional networks ( CNNs ) has seen huge adoption in computer vision applications .",
        "aspect": "CNNs",
        "sentiment": 0
    },
    {
        "text": "In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning .",
        "aspect": "CNNs",
        "sentiment": 0
    },
    {
        "text": "We introduce a class of CNNs called deep convolutional generative adversarial networks ( DCGANs ) , that have certain architectural constraints , and demonstrate that they are a strong candidate for unsupervised learning .",
        "aspect": "CNNs",
        "sentiment": 1
    },
    {
        "text": "We introduce a class of CNNs called deep convolutional generative adversarial networks ( DCGANs ) , that have certain architectural constraints , and demonstrate that they are a strong candidate for unsupervised learning .",
        "aspect": "deep convolutional generative adversarial networks",
        "sentiment": 2
    },
    {
        "text": "We introduce a class of CNNs called deep convolutional generative adversarial networks ( DCGANs ) , that have certain architectural constraints , and demonstrate that they are a strong candidate for unsupervised learning .",
        "aspect": "DCGANs",
        "sentiment": 2
    },
    {
        "text": "Training on various image datasets , we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator .",
        "aspect": "deep convolutional adversarial pair",
        "sentiment": 0
    },
    {
        "text": "Training on various image datasets , we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator .",
        "aspect": "generator and discriminator",
        "sentiment": 0
    },
    {
        "text": "Additionally , we use the learned features for novel tasks -demonstrating their applicability as general image representations .",
        "aspect": "general image representations",
        "sentiment": 0
    },
    {
        "text": "In the context of computer vision , one can leverage the practically unlimited amount of unlabeled images and videos to learn good intermediate representations , which can then be used on a variety of supervised learning tasks such as image classification .",
        "aspect": "intermediate representations",
        "sentiment": 0
    },
    {
        "text": "We propose that one way to build good image representations is by training Generative Adversarial Networks ( GANs ) , and later reusing parts of the generator and discriminator networks as feature extractors for supervised tasks .",
        "aspect": "image representations",
        "sentiment": 0
    },
    {
        "text": "We propose that one way to build good image representations is by training Generative Adversarial Networks ( GANs ) , and later reusing parts of the generator and discriminator networks as feature extractors for supervised tasks .",
        "aspect": "Generative Adversarial Networks",
        "sentiment": 0
    },
    {
        "text": "We propose that one way to build good image representations is by training Generative Adversarial Networks ( GANs ) , and later reusing parts of the generator and discriminator networks as feature extractors for supervised tasks .",
        "aspect": "GANs",
        "sentiment": 0
    },
    {
        "text": "We propose that one way to build good image representations is by training Generative Adversarial Networks ( GANs ) , and later reusing parts of the generator and discriminator networks as feature extractors for supervised tasks .",
        "aspect": "generator and discriminator networks",
        "sentiment": 0
    },
    {
        "text": "We propose that one way to build good image representations is by training Generative Adversarial Networks ( GANs ) , and later reusing parts of the generator and discriminator networks as feature extractors for supervised tasks .",
        "aspect": "feature extractors",
        "sentiment": 0
    },
    {
        "text": "GANs provide an attractive alternative to maximum likelihood techniques .",
        "aspect": "GANs",
        "sentiment": 0
    },
    {
        "text": "GANs provide an attractive alternative to maximum likelihood techniques .",
        "aspect": "maximum likelihood techniques",
        "sentiment": 0
    },
    {
        "text": "One can additionally argue that their learning process and the lack of a heuristic cost function ( such as pixel - wise independent mean - square error ) are attractive to representation learning .",
        "aspect": "learning process",
        "sentiment": 0
    },
    {
        "text": "GANs have been known to be unstable to train , often resulting in generators that produce nonsensical outputs .",
        "aspect": "GANs",
        "sentiment": 0
    },
    {
        "text": "There has been very limited published research in trying to understand and visualize what GANs learn , and the intermediate representations of multi - layer GANs .",
        "aspect": "GANs",
        "sentiment": 0
    },
    {
        "text": "There has been very limited published research in trying to understand and visualize what GANs learn , and the intermediate representations of multi - layer GANs .",
        "aspect": "intermediate representations of multi - layer GANs",
        "sentiment": 0
    },
    {
        "text": "\u2022 We propose and evaluate a set of constraints on the architectural topology of Convolutional GANs that make them stable to train in most settings .",
        "aspect": "Convolutional GANs",
        "sentiment": 0
    },
    {
        "text": "We name this class of architectures Deep Convolutional GANs ( DCGAN ) \u2022 We use the trained discriminators for image classification tasks , showing competitive performance with other unsupervised algorithms .",
        "aspect": "architectures Deep Convolutional GANs",
        "sentiment": 2
    },
    {
        "text": "We name this class of architectures Deep Convolutional GANs ( DCGAN ) \u2022 We use the trained discriminators for image classification tasks , showing competitive performance with other unsupervised algorithms .",
        "aspect": "DCGAN",
        "sentiment": 2
    },
    {
        "text": "We name this class of architectures Deep Convolutional GANs ( DCGAN ) \u2022 We use the trained discriminators for image classification tasks , showing competitive performance with other unsupervised algorithms .",
        "aspect": "discriminators",
        "sentiment": 0
    },
    {
        "text": "We name this class of architectures Deep Convolutional GANs ( DCGAN ) \u2022 We use the trained discriminators for image classification tasks , showing competitive performance with other unsupervised algorithms .",
        "aspect": "unsupervised algorithms",
        "sentiment": 0
    },
    {
        "text": "\u2022 We visualize the filters learnt by GANs and empirically show that specific filters have learned to draw specific objects .",
        "aspect": "GANs",
        "sentiment": 0
    },
    {
        "text": "A classic approach to unsupervised representation learning is to do clustering on the data ( for example using K - means ) , and leverage the clusters for improved classification scores .",
        "aspect": "clustering",
        "sentiment": 0
    },
    {
        "text": "A classic approach to unsupervised representation learning is to do clustering on the data ( for example using K - means ) , and leverage the clusters for improved classification scores .",
        "aspect": "K - means",
        "sentiment": 0
    },
    {
        "text": "In the context of images , one can do hierarchical clustering of image patches to learn powerful image representations .",
        "aspect": "hierarchical clustering of image patches",
        "sentiment": 0
    },
    {
        "text": "In the context of images , one can do hierarchical clustering of image patches to learn powerful image representations .",
        "aspect": "image representations",
        "sentiment": 0
    },
    {
        "text": "Another popular method is to train auto - encoders ( convolutionally , stacked , separating the what and where components of the code , ladder structures ) that encode an image into a compact code , and decode the code to reconstruct the image as accurately as possible .",
        "aspect": "auto - encoders",
        "sentiment": 0
    },
    {
        "text": "Another popular method is to train auto - encoders ( convolutionally , stacked , separating the what and where components of the code , ladder structures ) that encode an image into a compact code , and decode the code to reconstruct the image as accurately as possible .",
        "aspect": "convolutionally",
        "sentiment": 0
    },
    {
        "text": "These methods have also been shown to learn good feature representations from image pixels .",
        "aspect": "feature representations",
        "sentiment": 0
    },
    {
        "text": "Deep belief networks have also been shown to work well in learning hierarchical representations .",
        "aspect": "Deep belief networks",
        "sentiment": 0
    },
    {
        "text": "We propose a more stable set of architectures for training generative adversarial networks and we give evidence that adversarial networks learn good representations of images for supervised learning and generative modeling .",
        "aspect": "generative adversarial networks",
        "sentiment": 1
    },
    {
        "text": "We propose a more stable set of architectures for training generative adversarial networks and we give evidence that adversarial networks learn good representations of images for supervised learning and generative modeling .",
        "aspect": "adversarial networks",
        "sentiment": 0
    },
    {
        "text": "Photo - Realistic Single Image Super - Resolution Using a Generative Adversarial Network",
        "aspect": "Generative Adversarial Network",
        "sentiment": 0
    },
    {
        "text": "GANs provide a powerful framework for generating plausible - looking natural images with high perceptual quality .",
        "aspect": "GANs",
        "sentiment": 0
    },
    {
        "text": "The GAN procedure encourages the reconstructions to move towards regions of the search space with high probability of containing photo - realistic images and thus closer to the natural image manifold as shown in Figure .",
        "aspect": "GAN procedure",
        "sentiment": 0
    },
    {
        "text": "In this paper we describe the first very deep ResNet architecture using the concept of GANs to form a perceptual loss function for photo - realistic SISR .",
        "aspect": "deep ResNet architecture",
        "sentiment": 0
    },
    {
        "text": "In this paper we describe the first very deep ResNet architecture using the concept of GANs to form a perceptual loss function for photo - realistic SISR .",
        "aspect": "GANs",
        "sentiment": 1
    },
    {
        "text": "In this paper we describe the first very deep ResNet architecture using the concept of GANs to form a perceptual loss function for photo - realistic SISR .",
        "aspect": "perceptual loss function",
        "sentiment": 1
    },
    {
        "text": "Our main contributions are : \u2022 We propose SRGAN which is a GAN - based network optimized for a new perceptual loss .",
        "aspect": "SRGAN",
        "sentiment": 2
    },
    {
        "text": "Here we replace the MSE - based content loss with a loss calculated on feature maps of the VGG network , which are more invariant to changes in pixel space .",
        "aspect": "MSE - based content loss",
        "sentiment": 0
    },
    {
        "text": "Here we replace the MSE - based content loss with a loss calculated on feature maps of the VGG network , which are more invariant to changes in pixel space .",
        "aspect": "VGG network",
        "sentiment": 0
    },
    {
        "text": "\u2022 We confirm with an extensive mean opinion score ( MOS ) test on images from three public benchmark datasets that SRGAN is the new state of the art , by a large margin , for the estimation of photo - realistic SR images with high upscaling factors ( 4\u00d7 ) .",
        "aspect": "SRGAN",
        "sentiment": 0
    },
    {
        "text": "We describe the network architecture and the perceptual loss in Section 2 . A quantitative evaluation on public benchmark datasets as well as visual illustrations are provided in Section 3 . The paper concludes with a discussion in Section 4 and concluding remarks in Section 5 .",
        "aspect": "network architecture",
        "sentiment": 0
    },
    {
        "text": "We confirmed the superior perceptual performance of SRGAN using MOS testing .",
        "aspect": "SRGAN",
        "sentiment": 0
    },
    {
        "text": "We confirmed the superior perceptual performance of SRGAN using MOS testing .",
        "aspect": "MOS testing",
        "sentiment": 1
    },
    {
        "text": "We have further shown that standard quantitative measures such as PSNR and SSIM fail to capture and accurately assess image quality with respect to the human visual system .",
        "aspect": "human visual system",
        "sentiment": 0
    },
    {
        "text": "However , preliminary experiments on the network architecture suggest that shallower networks have the potential to provide very efficient alternatives at a small reduction of qualitative performance .",
        "aspect": "network architecture",
        "sentiment": 0
    },
    {
        "text": "However , preliminary experiments on the network architecture suggest that shallower networks have the potential to provide very efficient alternatives at a small reduction of qualitative performance .",
        "aspect": "shallower networks",
        "sentiment": 0
    },
    {
        "text": "In contrast to Dong et al . , we found deeper network architectures to be beneficial .",
        "aspect": "deeper network architectures",
        "sentiment": 0
    },
    {
        "text": "We speculate that the ResNet design has a substantial impact on the performance of deeper networks .",
        "aspect": "ResNet design",
        "sentiment": 0
    },
    {
        "text": "We speculate that the ResNet design has a substantial impact on the performance of deeper networks .",
        "aspect": "deeper networks",
        "sentiment": 0
    },
    {
        "text": "We found that even deeper networks ( B > 16 ) can further increase the performance of SRResNet , however , come at the cost of longer training and testing times ( c.f .",
        "aspect": "SRResNet",
        "sentiment": 0
    },
    {
        "text": "We further found SRGAN variants of deeper networks are increasingly difficult to train due to the appearance of high - frequency artifacts .",
        "aspect": "SRGAN",
        "sentiment": 0
    },
    {
        "text": "We further found SRGAN variants of deeper networks are increasingly difficult to train due to the appearance of high - frequency artifacts .",
        "aspect": "deeper networks",
        "sentiment": 0
    },
    {
        "text": "We speculate that feature maps of these deeper layers focus purely on the content while leaving the adversarial loss focusing on texture details which are the main difference between the super - resolved images without the adversarial loss and photo - realistic images .",
        "aspect": "adversarial loss",
        "sentiment": 0
    },
    {
        "text": "The development of content loss functions that describe image spatial content , but more invariant to changes in pixel space will further improve photo - realistic image SR results .",
        "aspect": "content loss functions",
        "sentiment": 0
    },
    {
        "text": "We have described a deep residual network SRRes - Net that sets a new state of the art on public benchmark datasets when evaluated with the widely used PSNR measure .",
        "aspect": "deep residual network",
        "sentiment": 0
    },
    {
        "text": "We have described a deep residual network SRRes - Net that sets a new state of the art on public benchmark datasets when evaluated with the widely used PSNR measure .",
        "aspect": "SRRes - Net",
        "sentiment": 0
    },
    {
        "text": "We have highlighted some limitations of this PSNR - focused image super - resolution and introduced SRGAN , which augments the content loss function with an adversarial loss by training a GAN .",
        "aspect": "SRGAN",
        "sentiment": 2
    },
    {
        "text": "We have highlighted some limitations of this PSNR - focused image super - resolution and introduced SRGAN , which augments the content loss function with an adversarial loss by training a GAN .",
        "aspect": "GAN",
        "sentiment": 1
    },
    {
        "text": "Using extensive MOS testing , we have confirmed that SRGAN reconstructions for large upscaling factors ( 4\u00d7 ) are , by a considerable margin , more photo - realistic than reconstructions obtained with state - ofthe - art reference methods .",
        "aspect": "SRGAN reconstructions",
        "sentiment": 0
    },
    {
        "text": "I - BERT : Integer - only BERT Quantization",
        "aspect": "I - BERT",
        "sentiment": 0
    },
    {
        "text": "I - BERT : Integer - only BERT Quantization",
        "aspect": "Integer - only BERT",
        "sentiment": 0
    },
    {
        "text": "Transformer based models , like BERT and RoBERTa , have achieved state - of - the - art results in many Natural Language Processing tasks .",
        "aspect": "Transformer based models",
        "sentiment": 0
    },
    {
        "text": "Transformer based models , like BERT and RoBERTa , have achieved state - of - the - art results in many Natural Language Processing tasks .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Transformer based models , like BERT and RoBERTa , have achieved state - of - the - art results in many Natural Language Processing tasks .",
        "aspect": "RoBERTa",
        "sentiment": 0
    },
    {
        "text": "While quantization can be a viable solution for this , previous work on quantizing Transformer based models use floating - point arithmetic during inference , which can not efficiently utilize integer - only logical units such as the recent Turing Tensor Cores , or traditional integer - only ARM processors .",
        "aspect": "quantization",
        "sentiment": 0
    },
    {
        "text": "While quantization can be a viable solution for this , previous work on quantizing Transformer based models use floating - point arithmetic during inference , which can not efficiently utilize integer - only logical units such as the recent Turing Tensor Cores , or traditional integer - only ARM processors .",
        "aspect": "quantizing Transformer based models",
        "sentiment": 0
    },
    {
        "text": "While quantization can be a viable solution for this , previous work on quantizing Transformer based models use floating - point arithmetic during inference , which can not efficiently utilize integer - only logical units such as the recent Turing Tensor Cores , or traditional integer - only ARM processors .",
        "aspect": "integer - only logical units",
        "sentiment": 0
    },
    {
        "text": "In this work , we propose I - BERT , a novel quantization scheme for Transformer based models that quantizes the entire inference with integer - only arithmetic .",
        "aspect": "I - BERT",
        "sentiment": 2
    },
    {
        "text": "In this work , we propose I - BERT , a novel quantization scheme for Transformer based models that quantizes the entire inference with integer - only arithmetic .",
        "aspect": "quantization scheme",
        "sentiment": 0
    },
    {
        "text": "In this work , we propose I - BERT , a novel quantization scheme for Transformer based models that quantizes the entire inference with integer - only arithmetic .",
        "aspect": "Transformer based models",
        "sentiment": 0
    },
    {
        "text": "In this work , we propose I - BERT , a novel quantization scheme for Transformer based models that quantizes the entire inference with integer - only arithmetic .",
        "aspect": "integer - only arithmetic",
        "sentiment": 0
    },
    {
        "text": "Based on lightweight integer - only approximation methods for nonlinear operations , e.g. , GELU , Softmax , and Layer Normalization , I - BERT performs an end - to - end integer - only BERT inference without any floating point calculation .",
        "aspect": "integer - only approximation methods",
        "sentiment": 1
    },
    {
        "text": "Based on lightweight integer - only approximation methods for nonlinear operations , e.g. , GELU , Softmax , and Layer Normalization , I - BERT performs an end - to - end integer - only BERT inference without any floating point calculation .",
        "aspect": "GELU",
        "sentiment": 1
    },
    {
        "text": "Based on lightweight integer - only approximation methods for nonlinear operations , e.g. , GELU , Softmax , and Layer Normalization , I - BERT performs an end - to - end integer - only BERT inference without any floating point calculation .",
        "aspect": "Softmax",
        "sentiment": 1
    },
    {
        "text": "Based on lightweight integer - only approximation methods for nonlinear operations , e.g. , GELU , Softmax , and Layer Normalization , I - BERT performs an end - to - end integer - only BERT inference without any floating point calculation .",
        "aspect": "Layer Normalization",
        "sentiment": 1
    },
    {
        "text": "Based on lightweight integer - only approximation methods for nonlinear operations , e.g. , GELU , Softmax , and Layer Normalization , I - BERT performs an end - to - end integer - only BERT inference without any floating point calculation .",
        "aspect": "I - BERT",
        "sentiment": 0
    },
    {
        "text": "We evaluate our approach on GLUE downstream tasks using RoBERTa - Base / Large .",
        "aspect": "RoBERTa",
        "sentiment": 0
    },
    {
        "text": "We show that for both cases , I - BERT achieves similar ( and slightly higher ) accuracy as compared to the full - precision baseline .",
        "aspect": "I - BERT",
        "sentiment": 0
    },
    {
        "text": "We show that for both cases , I - BERT achieves similar ( and slightly higher ) accuracy as compared to the full - precision baseline .",
        "aspect": "full - precision baseline",
        "sentiment": 0
    },
    {
        "text": "Furthermore , our preliminary implementation of I - BERT shows a speedup of 2.4 \u2212 4.0\u00d7 for INT8 inference on a T4 GPU system as compared to FP32 inference .",
        "aspect": "I - BERT",
        "sentiment": 0
    },
    {
        "text": "The recent Transformer based Neural Network ( NN ) models , pre - trained from large unlabeled data ( e.g. , BERT , RoBERTa Proceedings of the 38 th International Conference on Machine Learning , PMLR 139 , 2021 .",
        "aspect": "Transformer based Neural Network ( NN ) models",
        "sentiment": 0
    },
    {
        "text": "2019 ) , and the GPT family ) , have achieved a significant accuracy improvement when fine - tuned on a wide range of Natural Language Processing ( NLP ) tasks such as sentence classification and question answering .",
        "aspect": "GPT family",
        "sentiment": 0
    },
    {
        "text": "Despite the state - of - the - art results in various NLP tasks , pre - trained Transformer models are generally orders of magnitude larger than prior models .",
        "aspect": "Transformer models",
        "sentiment": 0
    },
    {
        "text": "For example , the BERT - Large model contains 340 M parameters .",
        "aspect": "BERT - Large model",
        "sentiment": 0
    },
    {
        "text": "Much larger Transformer models have been introduced in the past few years , with even more parameters .",
        "aspect": "Transformer models",
        "sentiment": 0
    },
    {
        "text": "One promising method to tackle this challenge is quantization , a procedure which compresses NN models into smaller size by representing parameters and/or activations with low bit precision , e.g. , 8 - bit integer ( INT8 ) instead of 32 - bit floating point ( FP32 ) .",
        "aspect": "NN models",
        "sentiment": 0
    },
    {
        "text": "With the recent integer - only quantization methods , one can also benefit from faster inference speed by using low precision integer multiplication and accumulation , instead of floating point arithmetic .",
        "aspect": "integer - only quantization methods",
        "sentiment": 0
    },
    {
        "text": "With the recent integer - only quantization methods , one can also benefit from faster inference speed by using low precision integer multiplication and accumulation , instead of floating point arithmetic .",
        "aspect": "floating point arithmetic",
        "sentiment": 0
    },
    {
        "text": "However , previous quantization schemes for Transformer based models use simulated quantization ( aka fake quantization ) , where all or part of operations in the inference ( e.g. , GELU , Softmax , and Layer Normalization ) are carried out with floating point arithmetic .",
        "aspect": "quantization schemes",
        "sentiment": 0
    },
    {
        "text": "However , previous quantization schemes for Transformer based models use simulated quantization ( aka fake quantization ) , where all or part of operations in the inference ( e.g. , GELU , Softmax , and Layer Normalization ) are carried out with floating point arithmetic .",
        "aspect": "Transformer based models",
        "sentiment": 0
    },
    {
        "text": "However , previous quantization schemes for Transformer based models use simulated quantization ( aka fake quantization ) , where all or part of operations in the inference ( e.g. , GELU , Softmax , and Layer Normalization ) are carried out with floating point arithmetic .",
        "aspect": "simulated quantization",
        "sentiment": 0
    },
    {
        "text": "However , previous quantization schemes for Transformer based models use simulated quantization ( aka fake quantization ) , where all or part of operations in the inference ( e.g. , GELU , Softmax , and Layer Normalization ) are carried out with floating point arithmetic .",
        "aspect": "GELU",
        "sentiment": 0
    },
    {
        "text": "However , previous quantization schemes for Transformer based models use simulated quantization ( aka fake quantization ) , where all or part of operations in the inference ( e.g. , GELU , Softmax , and Layer Normalization ) are carried out with floating point arithmetic .",
        "aspect": "Softmax",
        "sentiment": 0
    },
    {
        "text": "However , previous quantization schemes for Transformer based models use simulated quantization ( aka fake quantization ) , where all or part of operations in the inference ( e.g. , GELU , Softmax , and Layer Normalization ) are carried out with floating point arithmetic .",
        "aspect": "Layer Normalization",
        "sentiment": 0
    },
    {
        "text": "However , previous quantization schemes for Transformer based models use simulated quantization ( aka fake quantization ) , where all or part of operations in the inference ( e.g. , GELU , Softmax , and Layer Normalization ) are carried out with floating point arithmetic .",
        "aspect": "floating point arithmetic",
        "sentiment": 0
    },
    {
        "text": "Most importantly , the resulting NN models can not be deployed on neural accelerators or popular edge processors that do not support floating point arithmetic .",
        "aspect": "NN models",
        "sentiment": 0
    },
    {
        "text": "Most importantly , the resulting NN models can not be deployed on neural accelerators or popular edge processors that do not support floating point arithmetic .",
        "aspect": "neural accelerators",
        "sentiment": 0
    },
    {
        "text": "Most importantly , the resulting NN models can not be deployed on neural accelerators or popular edge processors that do not support floating point arithmetic .",
        "aspect": "edge processors",
        "sentiment": 0
    },
    {
        "text": "Most importantly , the resulting NN models can not be deployed on neural accelerators or popular edge processors that do not support floating point arithmetic .",
        "aspect": "floating point arithmetic",
        "sentiment": 0
    },
    {
        "text": "For instance , the recent server class of Turing Tensor Cores have added high throughput integer logic that are faster than single / half - precision .",
        "aspect": "Turing Tensor Cores",
        "sentiment": 0
    },
    {
        "text": "Similarly , some of the edge pro - cessor cores in ARM Cortex - M ( ARM , 2020 ) family for embedded systems only contain integer arithmetic units , and they can only support NN deployment with the integer - only kernels .",
        "aspect": "NN deployment",
        "sentiment": 0
    },
    {
        "text": "Similarly , some of the edge pro - cessor cores in ARM Cortex - M ( ARM , 2020 ) family for embedded systems only contain integer arithmetic units , and they can only support NN deployment with the integer - only kernels .",
        "aspect": "integer - only kernels",
        "sentiment": 0
    },
    {
        "text": "Moreover , one has to consider that compared to the integer - only inference , the approaches that use floating point arithmetic are inferior in latency and power efficiency .",
        "aspect": "integer - only inference",
        "sentiment": 0
    },
    {
        "text": "Moreover , one has to consider that compared to the integer - only inference , the approaches that use floating point arithmetic are inferior in latency and power efficiency .",
        "aspect": "floating point arithmetic",
        "sentiment": 0
    },
    {
        "text": "For chip designers wishing to support BERT - like models , adding floating point arithmetic logic occupies larger die area on a chip , as compared to integer arithmetic logic .",
        "aspect": "BERT - like models",
        "sentiment": 0
    },
    {
        "text": "For chip designers wishing to support BERT - like models , adding floating point arithmetic logic occupies larger die area on a chip , as compared to integer arithmetic logic .",
        "aspect": "floating point arithmetic logic",
        "sentiment": 0
    },
    {
        "text": "While prior work has shown the feasibility of integer - only inference , these approaches have only focused on models in computer vision with simple CNN layers , Batch Normalization ( Batch - Norm ) , and ReLU activations .",
        "aspect": "CNN layers",
        "sentiment": 0
    },
    {
        "text": "While prior work has shown the feasibility of integer - only inference , these approaches have only focused on models in computer vision with simple CNN layers , Batch Normalization ( Batch - Norm ) , and ReLU activations .",
        "aspect": "Batch Normalization",
        "sentiment": 0
    },
    {
        "text": "While prior work has shown the feasibility of integer - only inference , these approaches have only focused on models in computer vision with simple CNN layers , Batch Normalization ( Batch - Norm ) , and ReLU activations .",
        "aspect": "Batch - Norm",
        "sentiment": 0
    },
    {
        "text": "While prior work has shown the feasibility of integer - only inference , these approaches have only focused on models in computer vision with simple CNN layers , Batch Normalization ( Batch - Norm ) , and ReLU activations .",
        "aspect": "ReLU activations",
        "sentiment": 0
    },
    {
        "text": "These are all linear or piece - wise linear operators .",
        "aspect": "piece - wise linear operators",
        "sentiment": 0
    },
    {
        "text": "Due to the non - linear operations used in Transformer architecture , e.g. , GELU , Softmax , and Layer Normalization ( LayerNorm ) , these methods can not be applied to Transformer based models .",
        "aspect": "Transformer architecture",
        "sentiment": 0
    },
    {
        "text": "Due to the non - linear operations used in Transformer architecture , e.g. , GELU , Softmax , and Layer Normalization ( LayerNorm ) , these methods can not be applied to Transformer based models .",
        "aspect": "GELU",
        "sentiment": 0
    },
    {
        "text": "Due to the non - linear operations used in Transformer architecture , e.g. , GELU , Softmax , and Layer Normalization ( LayerNorm ) , these methods can not be applied to Transformer based models .",
        "aspect": "Softmax",
        "sentiment": 0
    },
    {
        "text": "Due to the non - linear operations used in Transformer architecture , e.g. , GELU , Softmax , and Layer Normalization ( LayerNorm ) , these methods can not be applied to Transformer based models .",
        "aspect": "Layer Normalization",
        "sentiment": 0
    },
    {
        "text": "Due to the non - linear operations used in Transformer architecture , e.g. , GELU , Softmax , and Layer Normalization ( LayerNorm ) , these methods can not be applied to Transformer based models .",
        "aspect": "LayerNorm",
        "sentiment": 0
    },
    {
        "text": "Due to the non - linear operations used in Transformer architecture , e.g. , GELU , Softmax , and Layer Normalization ( LayerNorm ) , these methods can not be applied to Transformer based models .",
        "aspect": "Transformer based models",
        "sentiment": 0
    },
    {
        "text": "Unlike ReLU , computing GELU and Softmax with integer - only arithmetic is not straightforward , due to their non - linearity .",
        "aspect": "ReLU",
        "sentiment": 0
    },
    {
        "text": "Unlike ReLU , computing GELU and Softmax with integer - only arithmetic is not straightforward , due to their non - linearity .",
        "aspect": "GELU",
        "sentiment": 0
    },
    {
        "text": "Unlike ReLU , computing GELU and Softmax with integer - only arithmetic is not straightforward , due to their non - linearity .",
        "aspect": "Softmax",
        "sentiment": 0
    },
    {
        "text": "Unlike ReLU , computing GELU and Softmax with integer - only arithmetic is not straightforward , due to their non - linearity .",
        "aspect": "integer - only arithmetic",
        "sentiment": 0
    },
    {
        "text": "Furthermore , unlike BatchNorm whose parameters / statistics can be fused into the previous convolutional layer in inference , LayerNorm requires the dynamic computation of the square root of the variance for each input .",
        "aspect": "BatchNorm",
        "sentiment": 0
    },
    {
        "text": "Furthermore , unlike BatchNorm whose parameters / statistics can be fused into the previous convolutional layer in inference , LayerNorm requires the dynamic computation of the square root of the variance for each input .",
        "aspect": "convolutional layer",
        "sentiment": 0
    },
    {
        "text": "Furthermore , unlike BatchNorm whose parameters / statistics can be fused into the previous convolutional layer in inference , LayerNorm requires the dynamic computation of the square root of the variance for each input .",
        "aspect": "LayerNorm",
        "sentiment": 0
    },
    {
        "text": "This can not be na\u00efvely computed with integer - only arithmetic .",
        "aspect": "integer - only arithmetic",
        "sentiment": 0
    },
    {
        "text": "Another challenge is that processing GELU , Softmax , and LayerNorm with low precision can result in signifciant accuracy degradation .",
        "aspect": "GELU",
        "sentiment": 0
    },
    {
        "text": "Another challenge is that processing GELU , Softmax , and LayerNorm with low precision can result in signifciant accuracy degradation .",
        "aspect": "Softmax",
        "sentiment": 0
    },
    {
        "text": "Another challenge is that processing GELU , Softmax , and LayerNorm with low precision can result in signifciant accuracy degradation .",
        "aspect": "LayerNorm",
        "sentiment": 0
    },
    {
        "text": "For these reasons , other quantization methods such as keep these operations in FP32 precision .",
        "aspect": "quantization methods",
        "sentiment": 0
    },
    {
        "text": "In this work , we propose I - BERT to address these challenges .",
        "aspect": "I - BERT",
        "sentiment": 2
    },
    {
        "text": "I - BERT incorporates a series of novel integer - only quantization scheme for Transformer based models .",
        "aspect": "I - BERT",
        "sentiment": 0
    },
    {
        "text": "I - BERT incorporates a series of novel integer - only quantization scheme for Transformer based models .",
        "aspect": "integer - only quantization scheme",
        "sentiment": 1
    },
    {
        "text": "I - BERT incorporates a series of novel integer - only quantization scheme for Transformer based models .",
        "aspect": "Transformer based models",
        "sentiment": 0
    },
    {
        "text": "\u2022 We propose new kernels for the efficient and accurate integer - only computation of GELU and Softmax .",
        "aspect": "kernels",
        "sentiment": 2
    },
    {
        "text": "In particular , we approximate GELU and Softmax with lightweight second - order polynomials , which can be evaluated with integer - only arithmetic .",
        "aspect": "GELU",
        "sentiment": 0
    },
    {
        "text": "In particular , we approximate GELU and Softmax with lightweight second - order polynomials , which can be evaluated with integer - only arithmetic .",
        "aspect": "Softmax",
        "sentiment": 0
    },
    {
        "text": "In particular , we approximate GELU and Softmax with lightweight second - order polynomials , which can be evaluated with integer - only arithmetic .",
        "aspect": "second - order polynomials",
        "sentiment": 1
    },
    {
        "text": "In particular , we approximate GELU and Softmax with lightweight second - order polynomials , which can be evaluated with integer - only arithmetic .",
        "aspect": "integer - only arithmetic",
        "sentiment": 1
    },
    {
        "text": "We utilize different techniques to improve the approximation error , and achieve a maximum error of 1.8 \u00d7 10 \u22122 for GELU , and 1.9 \u00d7 10 \u22123 for Softmax .",
        "aspect": "GELU",
        "sentiment": 0
    },
    {
        "text": "We utilize different techniques to improve the approximation error , and achieve a maximum error of 1.8 \u00d7 10 \u22122 for GELU , and 1.9 \u00d7 10 \u22123 for Softmax .",
        "aspect": "Softmax",
        "sentiment": 0
    },
    {
        "text": "\u2022 For LayerNorm , we perform integer - only computation by leveraging a known algorithm for integer calculation of square root .",
        "aspect": "LayerNorm",
        "sentiment": 0
    },
    {
        "text": "\u2022 For LayerNorm , we perform integer - only computation by leveraging a known algorithm for integer calculation of square root .",
        "aspect": "integer - only computation",
        "sentiment": 1
    },
    {
        "text": "\u2022 For LayerNorm , we perform integer - only computation by leveraging a known algorithm for integer calculation of square root .",
        "aspect": "integer calculation of square root",
        "sentiment": 1
    },
    {
        "text": "\u2022 We use these approximations of GELU , Softmax , and LayerNorm to design integer - only quantization for Trans - former based models .",
        "aspect": "approximations of GELU",
        "sentiment": 0
    },
    {
        "text": "\u2022 We use these approximations of GELU , Softmax , and LayerNorm to design integer - only quantization for Trans - former based models .",
        "aspect": "Softmax",
        "sentiment": 0
    },
    {
        "text": "\u2022 We use these approximations of GELU , Softmax , and LayerNorm to design integer - only quantization for Trans - former based models .",
        "aspect": "LayerNorm",
        "sentiment": 0
    },
    {
        "text": "\u2022 We use these approximations of GELU , Softmax , and LayerNorm to design integer - only quantization for Trans - former based models .",
        "aspect": "Trans - former based models",
        "sentiment": 0
    },
    {
        "text": "Specifically , we process Embedding and matrix multiplication ( MatMul ) with INT8 multiplication and INT32 accumulation .",
        "aspect": "Embedding and matrix multiplication",
        "sentiment": 0
    },
    {
        "text": "Specifically , we process Embedding and matrix multiplication ( MatMul ) with INT8 multiplication and INT32 accumulation .",
        "aspect": "INT8 multiplication",
        "sentiment": 1
    },
    {
        "text": "Specifically , we process Embedding and matrix multiplication ( MatMul ) with INT8 multiplication and INT32 accumulation .",
        "aspect": "INT32 accumulation",
        "sentiment": 1
    },
    {
        "text": "Specifically , we process Embedding and matrix multiplication ( MatMul ) with INT8 multiplication and INT32 accumulation .",
        "aspect": "MatMul",
        "sentiment": 0
    },
    {
        "text": "The following non - linear operations ( GELU , Softmax , and LayerNorm ) are then calculated on the INT32 accumulated result and then requantized back to INT8 .",
        "aspect": "GELU",
        "sentiment": 0
    },
    {
        "text": "The following non - linear operations ( GELU , Softmax , and LayerNorm ) are then calculated on the INT32 accumulated result and then requantized back to INT8 .",
        "aspect": "Softmax",
        "sentiment": 0
    },
    {
        "text": "The following non - linear operations ( GELU , Softmax , and LayerNorm ) are then calculated on the INT32 accumulated result and then requantized back to INT8 .",
        "aspect": "LayerNorm",
        "sentiment": 0
    },
    {
        "text": "The following non - linear operations ( GELU , Softmax , and LayerNorm ) are then calculated on the INT32 accumulated result and then requantized back to INT8 .",
        "aspect": "INT32",
        "sentiment": 1
    },
    {
        "text": "The following non - linear operations ( GELU , Softmax , and LayerNorm ) are then calculated on the INT32 accumulated result and then requantized back to INT8 .",
        "aspect": "INT8",
        "sentiment": 1
    },
    {
        "text": "\u2022 We apply I - BERT to RoBERTa - Base / Large , and we evaluate their accuracy on the GLUE downstream tasks .",
        "aspect": "I - BERT",
        "sentiment": 0
    },
    {
        "text": "\u2022 We apply I - BERT to RoBERTa - Base / Large , and we evaluate their accuracy on the GLUE downstream tasks .",
        "aspect": "RoBERTa - Base / Large",
        "sentiment": 3
    },
    {
        "text": "I - BERT achieves similar results as compared to full - precision baseline .",
        "aspect": "I - BERT",
        "sentiment": 0
    },
    {
        "text": "I - BERT achieves similar results as compared to full - precision baseline .",
        "aspect": "full - precision baseline",
        "sentiment": 0
    },
    {
        "text": "Specifically , I - BERT outperforms the baseline by 0.3 and 0.5 on the GLUE downstream tasks for RoBERTa - Base and RoBERTa - Large , respectively .",
        "aspect": "I - BERT",
        "sentiment": 0
    },
    {
        "text": "Specifically , I - BERT outperforms the baseline by 0.3 and 0.5 on the GLUE downstream tasks for RoBERTa - Base and RoBERTa - Large , respectively .",
        "aspect": "RoBERTa - Base",
        "sentiment": 3
    },
    {
        "text": "Specifically , I - BERT outperforms the baseline by 0.3 and 0.5 on the GLUE downstream tasks for RoBERTa - Base and RoBERTa - Large , respectively .",
        "aspect": "RoBERTa - Large",
        "sentiment": 3
    },
    {
        "text": "\u2022 We deploy INT8 BERT models with the integer - only kernels for non - linear operations on a T4 GPU using Ten - sorRT .",
        "aspect": "INT8 BERT",
        "sentiment": 0
    },
    {
        "text": "\u2022 We deploy INT8 BERT models with the integer - only kernels for non - linear operations on a T4 GPU using Ten - sorRT .",
        "aspect": "integer - only kernels",
        "sentiment": 0
    },
    {
        "text": "\u2022 We deploy INT8 BERT models with the integer - only kernels for non - linear operations on a T4 GPU using Ten - sorRT .",
        "aspect": "T4 GPU",
        "sentiment": 0
    },
    {
        "text": "We show that INT8 inference achieves up to 4\u00d7 speedup as compared to FP32 inference .",
        "aspect": "INT8 inference",
        "sentiment": 0
    },
    {
        "text": "We show that INT8 inference achieves up to 4\u00d7 speedup as compared to FP32 inference .",
        "aspect": "FP32 inference",
        "sentiment": 0
    },
    {
        "text": "Efficient Neural Network .",
        "aspect": "Efficient Neural Network",
        "sentiment": 0
    },
    {
        "text": "There are several different approaches to reduce the memory footprint , latency , and power of modern NN architectures .",
        "aspect": "NN architectures",
        "sentiment": 0
    },
    {
        "text": "While this line of research mostly focuses on CNN models , there have been recent attempts to introduce quantization techniques into Transformer based models as well .",
        "aspect": "CNN models",
        "sentiment": 0
    },
    {
        "text": "While this line of research mostly focuses on CNN models , there have been recent attempts to introduce quantization techniques into Transformer based models as well .",
        "aspect": "quantization techniques",
        "sentiment": 0
    },
    {
        "text": "While this line of research mostly focuses on CNN models , there have been recent attempts to introduce quantization techniques into Transformer based models as well .",
        "aspect": "Transformer based models",
        "sentiment": 0
    },
    {
        "text": "For example , and propose an 8 - bit quantization scheme for Transformer based models and compress the model size up to 25 % of the original size .",
        "aspect": "8 - bit quantization scheme",
        "sentiment": 0
    },
    {
        "text": "For example , and propose an 8 - bit quantization scheme for Transformer based models and compress the model size up to 25 % of the original size .",
        "aspect": "Transformer based models",
        "sentiment": 0
    },
    {
        "text": "Another work applies uniform and mixed - precision to quantize BERT model , where a second - order sensitivity method is used for the mixedprecision setting .",
        "aspect": "BERT model",
        "sentiment": 0
    },
    {
        "text": "Another work applies uniform and mixed - precision to quantize BERT model , where a second - order sensitivity method is used for the mixedprecision setting .",
        "aspect": "second - order sensitivity method",
        "sentiment": 0
    },
    {
        "text": "However , to the best of our knowledge , all of the prior quantization work on Transformer based models use simulated quantization ( aka fake quantization ) , where all or part of operations are performed with floating point arithmetic .",
        "aspect": "Transformer based models",
        "sentiment": 0
    },
    {
        "text": "However , to the best of our knowledge , all of the prior quantization work on Transformer based models use simulated quantization ( aka fake quantization ) , where all or part of operations are performed with floating point arithmetic .",
        "aspect": "simulated quantization",
        "sentiment": 0
    },
    {
        "text": "While attempt to process Embedding and MatMul efficiently with integer arithmetic , they keep the remaining operations ( i.e. , GELU , Softmax , and LayerNorm ) in FP32 , as illustrated in Fig . ( middle ) .",
        "aspect": "MatMul",
        "sentiment": 0
    },
    {
        "text": "While attempt to process Embedding and MatMul efficiently with integer arithmetic , they keep the remaining operations ( i.e. , GELU , Softmax , and LayerNorm ) in FP32 , as illustrated in Fig . ( middle ) .",
        "aspect": "Softmax",
        "sentiment": 0
    },
    {
        "text": "While attempt to process Embedding and MatMul efficiently with integer arithmetic , they keep the remaining operations ( i.e. , GELU , Softmax , and LayerNorm ) in FP32 , as illustrated in Fig . ( middle ) .",
        "aspect": "LayerNorm",
        "sentiment": 0
    },
    {
        "text": "While attempt to process Embedding and MatMul efficiently with integer arithmetic , they keep the remaining operations ( i.e. , GELU , Softmax , and LayerNorm ) in FP32 , as illustrated in Fig . ( middle ) .",
        "aspect": "FP32",
        "sentiment": 0
    },
    {
        "text": "While attempt to process Embedding and MatMul efficiently with integer arithmetic , they keep the remaining operations ( i.e. , GELU , Softmax , and LayerNorm ) in FP32 , as illustrated in Fig . ( middle ) .",
        "aspect": "GELU",
        "sentiment": 0
    },
    {
        "text": "However , our method I - BERT uses integer - only quantization for the entire inference process - i.e. , without any floating point arithmetic and without any dequantization during the entire inference .",
        "aspect": "I - BERT",
        "sentiment": 2
    },
    {
        "text": "However , our method I - BERT uses integer - only quantization for the entire inference process - i.e. , without any floating point arithmetic and without any dequantization during the entire inference .",
        "aspect": "integer - only quantization",
        "sentiment": 1
    },
    {
        "text": "This allows more efficient hardware deployment on specialized accelerators or integer - only processors ( ARM , 2020 ) as well as faster and less energy consuming inference .",
        "aspect": "integer - only processors",
        "sentiment": 0
    },
    {
        "text": "While we focus on uniform quantization , our method is complementary to other mixed and/or low - precision methods , and can be deployed for those settings as well .",
        "aspect": "mixed and/or low - precision methods",
        "sentiment": 0
    },
    {
        "text": "To briefly discuss , there are also several quantization works for computer vision .",
        "aspect": "quantization",
        "sentiment": 0
    },
    {
        "text": "Similarly , the recent work of extends this approach to low precision and mixed precision dyadic quantization , which is an extension of integer - only quantization where no integer division is used .",
        "aspect": "integer - only quantization",
        "sentiment": 0
    },
    {
        "text": "Similarly , the recent work of extends this approach to low precision and mixed precision dyadic quantization , which is an extension of integer - only quantization where no integer division is used .",
        "aspect": "integer division",
        "sentiment": 0
    },
    {
        "text": "However , both of these works are limited to CNN models that only contain linear and piece - wise linear operators , and they can not be applied to Transformer based models with non - linear operators , e.g. , GELU , Softmax , and LayerNorm .",
        "aspect": "CNN models",
        "sentiment": 0
    },
    {
        "text": "However , both of these works are limited to CNN models that only contain linear and piece - wise linear operators , and they can not be applied to Transformer based models with non - linear operators , e.g. , GELU , Softmax , and LayerNorm .",
        "aspect": "Transformer based models",
        "sentiment": 0
    },
    {
        "text": "However , both of these works are limited to CNN models that only contain linear and piece - wise linear operators , and they can not be applied to Transformer based models with non - linear operators , e.g. , GELU , Softmax , and LayerNorm .",
        "aspect": "GELU",
        "sentiment": 0
    },
    {
        "text": "However , both of these works are limited to CNN models that only contain linear and piece - wise linear operators , and they can not be applied to Transformer based models with non - linear operators , e.g. , GELU , Softmax , and LayerNorm .",
        "aspect": "Softmax",
        "sentiment": 0
    },
    {
        "text": "However , both of these works are limited to CNN models that only contain linear and piece - wise linear operators , and they can not be applied to Transformer based models with non - linear operators , e.g. , GELU , Softmax , and LayerNorm .",
        "aspect": "LayerNorm",
        "sentiment": 0
    },
    {
        "text": "Our work aims to address this limitation by extending the integer - only scheme to the Transformer based models without accuracy drop .",
        "aspect": "integer - only scheme",
        "sentiment": 1
    },
    {
        "text": "Our work aims to address this limitation by extending the integer - only scheme to the Transformer based models without accuracy drop .",
        "aspect": "Transformer based models",
        "sentiment": 1
    },
    {
        "text": "We have proposed I - BERT , a novel integer - only quantization scheme for Transformers , where the entire inference is performed with pure integer arithmetic .",
        "aspect": "I - BERT",
        "sentiment": 2
    },
    {
        "text": "We have proposed I - BERT , a novel integer - only quantization scheme for Transformers , where the entire inference is performed with pure integer arithmetic .",
        "aspect": "integer - only quantization scheme",
        "sentiment": 1
    },
    {
        "text": "We have proposed I - BERT , a novel integer - only quantization scheme for Transformers , where the entire inference is performed with pure integer arithmetic .",
        "aspect": "integer arithmetic",
        "sentiment": 1
    },
    {
        "text": "Key elements of I - BERT are approximation methods for nonlinear operations such as GELU , Softmax , and LayerNorm , which enable their approximation with integer computation .",
        "aspect": "I - BERT",
        "sentiment": 0
    },
    {
        "text": "Key elements of I - BERT are approximation methods for nonlinear operations such as GELU , Softmax , and LayerNorm , which enable their approximation with integer computation .",
        "aspect": "approximation methods",
        "sentiment": 1
    },
    {
        "text": "Key elements of I - BERT are approximation methods for nonlinear operations such as GELU , Softmax , and LayerNorm , which enable their approximation with integer computation .",
        "aspect": "GELU",
        "sentiment": 0
    },
    {
        "text": "Key elements of I - BERT are approximation methods for nonlinear operations such as GELU , Softmax , and LayerNorm , which enable their approximation with integer computation .",
        "aspect": "Softmax",
        "sentiment": 0
    },
    {
        "text": "Key elements of I - BERT are approximation methods for nonlinear operations such as GELU , Softmax , and LayerNorm , which enable their approximation with integer computation .",
        "aspect": "LayerNorm",
        "sentiment": 0
    },
    {
        "text": "Key elements of I - BERT are approximation methods for nonlinear operations such as GELU , Softmax , and LayerNorm , which enable their approximation with integer computation .",
        "aspect": "integer computation",
        "sentiment": 1
    },
    {
        "text": "We empirically evaluated I - BERT on RoBERTa - Base / Large models , where our quantization method improves the average GLUE score by 0.3/0.5 points as comapred to baseline .",
        "aspect": "I - BERT",
        "sentiment": 0
    },
    {
        "text": "We empirically evaluated I - BERT on RoBERTa - Base / Large models , where our quantization method improves the average GLUE score by 0.3/0.5 points as comapred to baseline .",
        "aspect": "RoBERTa - Base / Large models",
        "sentiment": 0
    },
    {
        "text": "We empirically evaluated I - BERT on RoBERTa - Base / Large models , where our quantization method improves the average GLUE score by 0.3/0.5 points as comapred to baseline .",
        "aspect": "quantization method",
        "sentiment": 0
    },
    {
        "text": "Furthermore , we directly deployed the quantized models and measured the end - to - end inference latency , showing that I - BERT can achieve up to 4.00\u00d7 speedup on a Tesla T4 GPU as compared to floating point baseline .",
        "aspect": "quantized models",
        "sentiment": 0
    },
    {
        "text": "Furthermore , we directly deployed the quantized models and measured the end - to - end inference latency , showing that I - BERT can achieve up to 4.00\u00d7 speedup on a Tesla T4 GPU as compared to floating point baseline .",
        "aspect": "I - BERT",
        "sentiment": 0
    },
    {
        "text": "Furthermore , we directly deployed the quantized models and measured the end - to - end inference latency , showing that I - BERT can achieve up to 4.00\u00d7 speedup on a Tesla T4 GPU as compared to floating point baseline .",
        "aspect": "floating point baseline",
        "sentiment": 0
    },
    {
        "text": "For instance , one could consider replacing GELU with i - GELU during training .",
        "aspect": "GELU",
        "sentiment": 0
    },
    {
        "text": "For instance , one could consider replacing GELU with i - GELU during training .",
        "aspect": "i - GELU",
        "sentiment": 0
    },
    {
        "text": "Also , further studies are needed to evaluate the performance benefit of i - GELU as compared to GELU .",
        "aspect": "i - GELU",
        "sentiment": 0
    },
    {
        "text": "Also , further studies are needed to evaluate the performance benefit of i - GELU as compared to GELU .",
        "aspect": "GELU",
        "sentiment": 0
    },
    {
        "text": "For this reason , we only use symmetric quantization in this work .",
        "aspect": "symmetric quantization",
        "sentiment": 0
    },
    {
        "text": "DeeBERT : Dynamic Early Exiting for Accelerating BERT Inference",
        "aspect": "DeeBERT",
        "sentiment": 0
    },
    {
        "text": "DeeBERT : Dynamic Early Exiting for Accelerating BERT Inference",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Large - scale pre - trained language models such as BERT have brought significant improvements to NLP applications .",
        "aspect": "language models",
        "sentiment": 0
    },
    {
        "text": "Large - scale pre - trained language models such as BERT have brought significant improvements to NLP applications .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "We propose a simple but effective method , DeeBERT , to accelerate BERT inference .",
        "aspect": "DeeBERT",
        "sentiment": 2
    },
    {
        "text": "We propose a simple but effective method , DeeBERT , to accelerate BERT inference .",
        "aspect": "inference",
        "sentiment": 0
    },
    {
        "text": "Experiments show that DeeBERT is able to save up to \u223c40 % inference time with minimal degradation in model quality .",
        "aspect": "DeeBERT",
        "sentiment": 0
    },
    {
        "text": "Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Our work provides new ideas to efficiently apply deep transformer - based models to downstream tasks .",
        "aspect": "deep transformer - based models",
        "sentiment": 0
    },
    {
        "text": "Large - scale pre - trained language models such as ELMo , GPT , BERT , XLNet , and RoBERTa have brought significant improvements to natural language processing ( NLP ) applications .",
        "aspect": "language models",
        "sentiment": 0
    },
    {
        "text": "Large - scale pre - trained language models such as ELMo , GPT , BERT , XLNet , and RoBERTa have brought significant improvements to natural language processing ( NLP ) applications .",
        "aspect": "ELMo",
        "sentiment": 0
    },
    {
        "text": "Large - scale pre - trained language models such as ELMo , GPT , BERT , XLNet , and RoBERTa have brought significant improvements to natural language processing ( NLP ) applications .",
        "aspect": "GPT",
        "sentiment": 0
    },
    {
        "text": "Large - scale pre - trained language models such as ELMo , GPT , BERT , XLNet , and RoBERTa have brought significant improvements to natural language processing ( NLP ) applications .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Large - scale pre - trained language models such as ELMo , GPT , BERT , XLNet , and RoBERTa have brought significant improvements to natural language processing ( NLP ) applications .",
        "aspect": "XLNet",
        "sentiment": 0
    },
    {
        "text": "Large - scale pre - trained language models such as ELMo , GPT , BERT , XLNet , and RoBERTa have brought significant improvements to natural language processing ( NLP ) applications .",
        "aspect": "RoBERTa",
        "sentiment": 0
    },
    {
        "text": "To accelerate inference for BERT , we propose DeeBERT : Dynamic early exiting for BERT .",
        "aspect": "BERT",
        "sentiment": 1
    },
    {
        "text": "To accelerate inference for BERT , we propose DeeBERT : Dynamic early exiting for BERT .",
        "aspect": "DeeBERT",
        "sentiment": 2
    },
    {
        "text": "To accelerate inference for BERT , we propose DeeBERT : Dynamic early exiting for BERT .",
        "aspect": "BERT",
        "sentiment": 1
    },
    {
        "text": "The inspiration comes from a well - known observation in the computer vision community : in deep convolutional neural networks , higher layers typically produce more detailed and finer - grained features .",
        "aspect": "deep convolutional neural networks",
        "sentiment": 0
    },
    {
        "text": "Therefore , we hypothesize that , for BERT , features provided by the intermediate transformer layers may suffice to classify some input samples .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Therefore , we hypothesize that , for BERT , features provided by the intermediate transformer layers may suffice to classify some input samples .",
        "aspect": "transformer layers",
        "sentiment": 0
    },
    {
        "text": "DeeBERT accelerates BERT inference by inserting extra classification layers ( which we refer to as off - ramps ) between each transformer layer of BERT ( Figure ) .",
        "aspect": "DeeBERT",
        "sentiment": 0
    },
    {
        "text": "DeeBERT accelerates BERT inference by inserting extra classification layers ( which we refer to as off - ramps ) between each transformer layer of BERT ( Figure ) .",
        "aspect": "classification layers",
        "sentiment": 0
    },
    {
        "text": "DeeBERT accelerates BERT inference by inserting extra classification layers ( which we refer to as off - ramps ) between each transformer layer of BERT ( Figure ) .",
        "aspect": "transformer layer",
        "sentiment": 0
    },
    {
        "text": "DeeBERT accelerates BERT inference by inserting extra classification layers ( which we refer to as off - ramps ) between each transformer layer of BERT ( Figure ) .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "DeeBERT accelerates BERT inference by inserting extra classification layers ( which we refer to as off - ramps ) between each transformer layer of BERT ( Figure ) .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "DeeBERT accelerates BERT inference by inserting extra classification layers ( which we refer to as off - ramps ) between each transformer layer of BERT ( Figure ) .",
        "aspect": "off - ramps",
        "sentiment": 0
    },
    {
        "text": "All transformer layers and offramps are jointly fine - tuned on a given downstream dataset .",
        "aspect": "transformer layers",
        "sentiment": 0
    },
    {
        "text": "All transformer layers and offramps are jointly fine - tuned on a given downstream dataset .",
        "aspect": "offramps",
        "sentiment": 0
    },
    {
        "text": "At inference time , after a sample goes through a transformer layer , it is passed to the following off - ramp .",
        "aspect": "transformer layer",
        "sentiment": 0
    },
    {
        "text": "At inference time , after a sample goes through a transformer layer , it is passed to the following off - ramp .",
        "aspect": "off - ramp",
        "sentiment": 0
    },
    {
        "text": "If the off - ramp is confident of the prediction , the result is returned ; otherwise , the sample is sent to the next transformer layer .",
        "aspect": "transformer layer",
        "sentiment": 0
    },
    {
        "text": "If the off - ramp is confident of the prediction , the result is returned ; otherwise , the sample is sent to the next transformer layer .",
        "aspect": "off - ramp",
        "sentiment": 0
    },
    {
        "text": "In this paper , we conduct experiments on BERT and RoBERTa with six GLUE datasets , showing that DeeBERT is capable of accelerating model inference by up to \u223c40 % with minimal model quality degradation on downstream tasks .",
        "aspect": "BERT",
        "sentiment": 1
    },
    {
        "text": "In this paper , we conduct experiments on BERT and RoBERTa with six GLUE datasets , showing that DeeBERT is capable of accelerating model inference by up to \u223c40 % with minimal model quality degradation on downstream tasks .",
        "aspect": "RoBERTa",
        "sentiment": 1
    },
    {
        "text": "In this paper , we conduct experiments on BERT and RoBERTa with six GLUE datasets , showing that DeeBERT is capable of accelerating model inference by up to \u223c40 % with minimal model quality degradation on downstream tasks .",
        "aspect": "DeeBERT",
        "sentiment": 0
    },
    {
        "text": "Further analyses reveal interesting patterns in the models ' transformer layers , as well as redundancy in both BERT and RoBERTa .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Further analyses reveal interesting patterns in the models ' transformer layers , as well as redundancy in both BERT and RoBERTa .",
        "aspect": "RoBERTa",
        "sentiment": 0
    },
    {
        "text": "BERT and RoBERTa are large - scale pre - trained language models based on transformers .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "BERT and RoBERTa are large - scale pre - trained language models based on transformers .",
        "aspect": "RoBERTa",
        "sentiment": 0
    },
    {
        "text": "BERT and RoBERTa are large - scale pre - trained language models based on transformers .",
        "aspect": "language models",
        "sentiment": 0
    },
    {
        "text": "BERT and RoBERTa are large - scale pre - trained language models based on transformers .",
        "aspect": "transformers",
        "sentiment": 0
    },
    {
        "text": "Q - BERT uses quantization to compress BERT , and Layer - Drop uses group regularization to enable structured pruning at inference time .",
        "aspect": "Q - BERT",
        "sentiment": 0
    },
    {
        "text": "Q - BERT uses quantization to compress BERT , and Layer - Drop uses group regularization to enable structured pruning at inference time .",
        "aspect": "quantization",
        "sentiment": 0
    },
    {
        "text": "Q - BERT uses quantization to compress BERT , and Layer - Drop uses group regularization to enable structured pruning at inference time .",
        "aspect": "Layer - Drop",
        "sentiment": 0
    },
    {
        "text": "Q - BERT uses quantization to compress BERT , and Layer - Drop uses group regularization to enable structured pruning at inference time .",
        "aspect": "group regularization",
        "sentiment": 0
    },
    {
        "text": "On the knowledge distillation side , TinyBERT and DistilBERT both distill BERT into a smaller transformer - based model , and distill BERT into even smaller non - transformer - based models .",
        "aspect": "DistilBERT",
        "sentiment": 0
    },
    {
        "text": "On the knowledge distillation side , TinyBERT and DistilBERT both distill BERT into a smaller transformer - based model , and distill BERT into even smaller non - transformer - based models .",
        "aspect": "transformer - based model",
        "sentiment": 0
    },
    {
        "text": "On the knowledge distillation side , TinyBERT and DistilBERT both distill BERT into a smaller transformer - based model , and distill BERT into even smaller non - transformer - based models .",
        "aspect": "non - transformer - based models",
        "sentiment": 0
    },
    {
        "text": "On the knowledge distillation side , TinyBERT and DistilBERT both distill BERT into a smaller transformer - based model , and distill BERT into even smaller non - transformer - based models .",
        "aspect": "TinyBERT",
        "sentiment": 0
    },
    {
        "text": "For each run of Dee - BERT on a dataset , we choose three entropy thresholds S based on quality - efficiency trade - offs on the development set , aiming to demonstrate two cases :",
        "aspect": "Dee - BERT",
        "sentiment": 0
    },
    {
        "text": "The turning point typically comes earlier for BERT than for RoBERTa , but after the turning point , the performance of RoBERTa drops faster than for BERT .",
        "aspect": "RoBERTa",
        "sentiment": 0
    },
    {
        "text": "The turning point typically comes earlier for BERT than for RoBERTa , but after the turning point , the performance of RoBERTa drops faster than for BERT .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "\u2022 Occasionally , we observe spikes in the curves , e.g. , RoBERTa in SST-2 , and both BERT and RoBERTa in RTE .",
        "aspect": "SST-2",
        "sentiment": 0
    },
    {
        "text": "\u2022 Occasionally , we observe spikes in the curves , e.g. , RoBERTa in SST-2 , and both BERT and RoBERTa in RTE .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "\u2022 Occasionally , we observe spikes in the curves , e.g. , RoBERTa in SST-2 , and both BERT and RoBERTa in RTE .",
        "aspect": "RTE",
        "sentiment": 0
    },
    {
        "text": "\u2022 Occasionally , we observe spikes in the curves , e.g. , RoBERTa in SST-2 , and both BERT and RoBERTa in RTE .",
        "aspect": "RoBERTa",
        "sentiment": 0
    },
    {
        "text": "\u2022 Occasionally , we observe spikes in the curves , e.g. , RoBERTa in SST-2 , and both BERT and RoBERTa in RTE .",
        "aspect": "RoBERTa",
        "sentiment": 0
    },
    {
        "text": "Compared with other BERT acceleration methods , DeeBERT has the following two advantages :",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Compared with other BERT acceleration methods , DeeBERT has the following two advantages :",
        "aspect": "DeeBERT",
        "sentiment": 0
    },
    {
        "text": "\u2022 Instead of producing a fixed - size smaller model like DistilBERT , Dee - BERT produces a series of options for faster inference , which users have the flexibility to choose from , according to their demands .",
        "aspect": "fixed - size smaller model",
        "sentiment": 0
    },
    {
        "text": "\u2022 Instead of producing a fixed - size smaller model like DistilBERT , Dee - BERT produces a series of options for faster inference , which users have the flexibility to choose from , according to their demands .",
        "aspect": "DistilBERT",
        "sentiment": 0
    },
    {
        "text": "\u2022 Instead of producing a fixed - size smaller model like DistilBERT , Dee - BERT produces a series of options for faster inference , which users have the flexibility to choose from , according to their demands .",
        "aspect": "Dee - BERT",
        "sentiment": 0
    },
    {
        "text": "\u2022 Unlike DistilBERT and LayerDrop , DeeBERT does not require further pretraining of the transformer model , which is much more time - consuming than fine - tuning .",
        "aspect": "DistilBERT",
        "sentiment": 0
    },
    {
        "text": "\u2022 Unlike DistilBERT and LayerDrop , DeeBERT does not require further pretraining of the transformer model , which is much more time - consuming than fine - tuning .",
        "aspect": "LayerDrop",
        "sentiment": 0
    },
    {
        "text": "\u2022 Unlike DistilBERT and LayerDrop , DeeBERT does not require further pretraining of the transformer model , which is much more time - consuming than fine - tuning .",
        "aspect": "DeeBERT",
        "sentiment": 0
    },
    {
        "text": "\u2022 Unlike DistilBERT and LayerDrop , DeeBERT does not require further pretraining of the transformer model , which is much more time - consuming than fine - tuning .",
        "aspect": "transformer model",
        "sentiment": 0
    },
    {
        "text": "\u2022 Unlike DistilBERT and LayerDrop , DeeBERT does not require further pretraining of the transformer model , which is much more time - consuming than fine - tuning .",
        "aspect": "fine - tuning",
        "sentiment": 0
    },
    {
        "text": "We propose DeeBERT , an effective method that exploits redundancy in BERT models to achieve better quality - efficiency trade - offs .",
        "aspect": "DeeBERT",
        "sentiment": 2
    },
    {
        "text": "We propose DeeBERT , an effective method that exploits redundancy in BERT models to achieve better quality - efficiency trade - offs .",
        "aspect": "BERT",
        "sentiment": 1
    },
    {
        "text": "We propose DeeBERT , an effective method that exploits redundancy in BERT models to achieve better quality - efficiency trade - offs .",
        "aspect": "redundancy",
        "sentiment": 1
    },
    {
        "text": "Experiments demonstrate its ability to accelerate BERT 's and RoBERTa 's inference by up to \u223c40 % , and also reveal interesting patterns of different transformer layers in BERT models .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Experiments demonstrate its ability to accelerate BERT 's and RoBERTa 's inference by up to \u223c40 % , and also reveal interesting patterns of different transformer layers in BERT models .",
        "aspect": "RoBERTa 's inference",
        "sentiment": 0
    },
    {
        "text": "Experiments demonstrate its ability to accelerate BERT 's and RoBERTa 's inference by up to \u223c40 % , and also reveal interesting patterns of different transformer layers in BERT models .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "There are a few interesting questions left unanswered in this paper , which would provide interesting future research directions : ( 1 ) DeeBERT 's training method , while maintaining good quality in the last off - ramp , reduces model capacity available for intermediate off - ramps ; it would be important to look for a method that achieves a better balance between all off - ramps .",
        "aspect": "DeeBERT 's training method",
        "sentiment": 0
    },
    {
        "text": "( 2 ) The reasons why some transformer layers appear redundant 2 and why Dee - BERT considers some samples easier than others remain unknown ; it would be interesting to further explore relationships between pre - training and layer redundancy , sample complexity and exit layer , and related characteristics .",
        "aspect": "Dee - BERT",
        "sentiment": 0
    },
    {
        "text": "Language Models are Unsupervised Multitask Learners",
        "aspect": "Language Models",
        "sentiment": 0
    },
    {
        "text": "Language Models are Unsupervised Multitask Learners",
        "aspect": "Unsupervised Multitask Learners",
        "sentiment": 0
    },
    {
        "text": "Natural language processing tasks , such as question answering , machine translation , reading comprehension , and summarization , are typically approached with supervised learning on taskspecific datasets .",
        "aspect": "supervised learning",
        "sentiment": 0
    },
    {
        "text": "We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText .",
        "aspect": "language models",
        "sentiment": 1
    },
    {
        "text": "When conditioned on a document plus questions , the answers generated by the language model reach 55 F1 on the CoQA dataset -matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000 + training examples .",
        "aspect": "language model",
        "sentiment": 0
    },
    {
        "text": "The capacity of the language model is essential to the success of zero - shot task transfer and increasing it improves performance in a log - linear fashion across tasks .",
        "aspect": "language model",
        "sentiment": 0
    },
    {
        "text": "Our largest model , GPT-2 , is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero - shot setting but still underfits WebText .",
        "aspect": "GPT-2",
        "sentiment": 2
    },
    {
        "text": "Our largest model , GPT-2 , is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero - shot setting but still underfits WebText .",
        "aspect": "Transformer",
        "sentiment": 1
    },
    {
        "text": "Machine learning systems now excel ( in expectation ) at tasks they are trained for by using a combination of large datasets , high - capacity models , and supervised learning .",
        "aspect": "Machine learning systems",
        "sentiment": 0
    },
    {
        "text": "Machine learning systems now excel ( in expectation ) at tasks they are trained for by using a combination of large datasets , high - capacity models , and supervised learning .",
        "aspect": "high - capacity models",
        "sentiment": 0
    },
    {
        "text": "Machine learning systems now excel ( in expectation ) at tasks they are trained for by using a combination of large datasets , high - capacity models , and supervised learning .",
        "aspect": "supervised learning",
        "sentiment": 0
    },
    {
        "text": "The dominant approach to creating ML systems is to collect a dataset of training examples demonstrating correct behavior for a desired task , train a system to imitate these behaviors , and then test its performance on independent and identically distributed ( IID ) held - out examples .",
        "aspect": "ML systems",
        "sentiment": 0
    },
    {
        "text": "But the often erratic behavior of captioning models , reading comprehension systems , and image classifiers on the diversity and variety of possible inputs highlights some of the shortcomings of this approach .",
        "aspect": "captioning models",
        "sentiment": 0
    },
    {
        "text": "But the often erratic behavior of captioning models , reading comprehension systems , and image classifiers on the diversity and variety of possible inputs highlights some of the shortcomings of this approach .",
        "aspect": "reading comprehension systems",
        "sentiment": 0
    },
    {
        "text": "But the often erratic behavior of captioning models , reading comprehension systems , and image classifiers on the diversity and variety of possible inputs highlights some of the shortcomings of this approach .",
        "aspect": "image classifiers",
        "sentiment": 0
    },
    {
        "text": "Multitask learning ) is a promising framework for improving general performance .",
        "aspect": "Multitask learning",
        "sentiment": 0
    },
    {
        "text": "However , multitask training in NLP is still nascent .",
        "aspect": "multitask training",
        "sentiment": 0
    },
    {
        "text": "Current ML systems need hundreds to thousands of examples to induce functions which generalize well .",
        "aspect": "ML systems",
        "sentiment": 0
    },
    {
        "text": "This suggests that multitask training many need just as many effective training pairs to realize its promise with current approaches .",
        "aspect": "multitask training",
        "sentiment": 0
    },
    {
        "text": "This motivates exploring additional setups for performing multitask learning .",
        "aspect": "multitask learning",
        "sentiment": 0
    },
    {
        "text": "utilize a combination of pre - training and supervised finetuning .",
        "aspect": "pre - training",
        "sentiment": 0
    },
    {
        "text": "utilize a combination of pre - training and supervised finetuning .",
        "aspect": "supervised finetuning",
        "sentiment": 0
    },
    {
        "text": "First , word vectors were learned and used as inputs to task - specific architectures , then the contextual representations of recurrent networks were transferred , and recent work suggests that task - specific architectures are no longer necessary and transferring many self - attention blocks is sufficient .",
        "aspect": "task - specific architectures",
        "sentiment": 0
    },
    {
        "text": "First , word vectors were learned and used as inputs to task - specific architectures , then the contextual representations of recurrent networks were transferred , and recent work suggests that task - specific architectures are no longer necessary and transferring many self - attention blocks is sufficient .",
        "aspect": "contextual representations",
        "sentiment": 0
    },
    {
        "text": "First , word vectors were learned and used as inputs to task - specific architectures , then the contextual representations of recurrent networks were transferred , and recent work suggests that task - specific architectures are no longer necessary and transferring many self - attention blocks is sufficient .",
        "aspect": "recurrent networks",
        "sentiment": 0
    },
    {
        "text": "First , word vectors were learned and used as inputs to task - specific architectures , then the contextual representations of recurrent networks were transferred , and recent work suggests that task - specific architectures are no longer necessary and transferring many self - attention blocks is sufficient .",
        "aspect": "task - specific architectures",
        "sentiment": 0
    },
    {
        "text": "These methods still require supervised training in order to perform a task .",
        "aspect": "supervised training",
        "sentiment": 0
    },
    {
        "text": "When only minimal or no supervised data is available , another line of work has demonstrated the promise of language models to perform specific tasks , such as commonsense reasoning and sentiment analysis .",
        "aspect": "language models",
        "sentiment": 0
    },
    {
        "text": "We demonstrate language models can perform down - stream tasks in a zero - shot setting -without any parameter or architecture modification .",
        "aspect": "language models",
        "sentiment": 0
    },
    {
        "text": "We demonstrate this approach shows potential by highlighting the ability of language models to perform a wide range of tasks in a zero - shot setting .",
        "aspect": "language models",
        "sentiment": 0
    },
    {
        "text": "We trained and benchmarked four LMs with approximately log - uniformly spaced sizes .",
        "aspect": "LMs",
        "sentiment": 0
    },
    {
        "text": "The smallest model is equivalent to the original GPT , and the second smallest equivalent to the largest model from BERT .",
        "aspect": "GPT",
        "sentiment": 0
    },
    {
        "text": "The smallest model is equivalent to the original GPT , and the second smallest equivalent to the largest model from BERT .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Our largest model , which we call GPT-2 , has over an order of magnitude more parameters than GPT .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "Our largest model , which we call GPT-2 , has over an order of magnitude more parameters than GPT .",
        "aspect": "GPT",
        "sentiment": 0
    },
    {
        "text": "A significant portion of this work measured the performance of larger language models trained on larger datasets .",
        "aspect": "language models",
        "sentiment": 0
    },
    {
        "text": "This Interesting learned functionality in generative models has been documented before such as the cells in an RNN language model performing line - width tracking and quote / comment detection .",
        "aspect": "generative models",
        "sentiment": 0
    },
    {
        "text": "This Interesting learned functionality in generative models has been documented before such as the cells in an RNN language model performing line - width tracking and quote / comment detection .",
        "aspect": "RNN language model",
        "sentiment": 0
    },
    {
        "text": "There has been extensive work on pre - training methods for language tasks .",
        "aspect": "pre - training methods",
        "sentiment": 0
    },
    {
        "text": "In addition to those mentioned in the introduction , GloVe scaled word vector representation learning to all of Common Crawl .",
        "aspect": "word vector representation learning",
        "sentiment": 0
    },
    {
        "text": "An influential early work on deep representation learning for text was Skip - thought Vectors .",
        "aspect": "Skip - thought Vectors",
        "sentiment": 0
    },
    {
        "text": "More recent work has shown that LM pre - training is helpful when fine - tuned for difficult generation tasks like chit - chat dialog and dialog based question answering systems as well ) .",
        "aspect": "LM pre - training",
        "sentiment": 0
    },
    {
        "text": "Much research has been dedicated to learning , understanding , and critically evaluating the representations of both supervised and unsupervised pre - training methods .",
        "aspect": "supervised and unsupervised pre - training methods",
        "sentiment": 0
    },
    {
        "text": "These findings potentially help explain the widespread success of pre - training techniques for down - stream NLP tasks as we show that , in the limit , one of these pre - training techniques begins to learn to perform tasks directly without the need for supervised adaption or modification .",
        "aspect": "pre - training techniques",
        "sentiment": 0
    },
    {
        "text": "These findings potentially help explain the widespread success of pre - training techniques for down - stream NLP tasks as we show that , in the limit , one of these pre - training techniques begins to learn to perform tasks directly without the need for supervised adaption or modification .",
        "aspect": "pre - training techniques",
        "sentiment": 0
    },
    {
        "text": "These findings potentially help explain the widespread success of pre - training techniques for down - stream NLP tasks as we show that , in the limit , one of these pre - training techniques begins to learn to perform tasks directly without the need for supervised adaption or modification .",
        "aspect": "supervised adaption",
        "sentiment": 0
    },
    {
        "text": "On reading comprehension the performance of GPT-2 is competitive with supervised baselines in a zero - shot setting .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "On reading comprehension the performance of GPT-2 is competitive with supervised baselines in a zero - shot setting .",
        "aspect": "supervised baselines",
        "sentiment": 0
    },
    {
        "text": "On reading comprehension the performance of GPT-2 is competitive with supervised baselines in a zero - shot setting .",
        "aspect": "zero - shot setting",
        "sentiment": 0
    },
    {
        "text": "While suggestive as a research result , in terms of practical applications , the zero - shot performance of GPT-2 is still far from use - able .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "We have studied the zero - shot performance of WebText LMs on many canonical NLP tasks , but there are many additional tasks that could be evaluated .",
        "aspect": "WebText LMs",
        "sentiment": 0
    },
    {
        "text": "There are undoubtedly many practical tasks where the performance of GPT-2 is still no better than random .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "Even on common tasks that we evaluated on , such as question answering and translation , language models only begin to outperform trivial baselines when they have sufficient capacity .",
        "aspect": "language models",
        "sentiment": 0
    },
    {
        "text": "While zero - shot performance establishes a baseline of the potential performance of GPT-2 on many tasks , it is not clear where the ceiling is with finetuning .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "On some tasks , GPT-2 's fully abstractive output is a significant departure from the extractive pointer network based outputs which are currently state of the art on many question answering and reading comprehension datasets .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "On some tasks , GPT-2 's fully abstractive output is a significant departure from the extractive pointer network based outputs which are currently state of the art on many question answering and reading comprehension datasets .",
        "aspect": "extractive pointer network based outputs",
        "sentiment": 0
    },
    {
        "text": "Given the prior success of fine - tuning GPT , we plan to investigate fine - tuning on benchmarks such as decaNLP and GLUE , especially since it is unclear whether the additional training data and capacity of GPT-2 is sufficient to overcome the inefficiencies of uni - directional representations demonstrated by BERT .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "Given the prior success of fine - tuning GPT , we plan to investigate fine - tuning on benchmarks such as decaNLP and GLUE , especially since it is unclear whether the additional training data and capacity of GPT-2 is sufficient to overcome the inefficiencies of uni - directional representations demonstrated by BERT .",
        "aspect": "uni - directional representations",
        "sentiment": 0
    },
    {
        "text": "Given the prior success of fine - tuning GPT , we plan to investigate fine - tuning on benchmarks such as decaNLP and GLUE , especially since it is unclear whether the additional training data and capacity of GPT-2 is sufficient to overcome the inefficiencies of uni - directional representations demonstrated by BERT .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "When a large language model is trained on a sufficiently large and diverse dataset it is able to perform well across many domains and datasets .",
        "aspect": "language model",
        "sentiment": 0
    },
    {
        "text": "GPT-2 zero - shots to state of the art performance on 7 out of 8 tested language modeling datasets .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "The diversity of tasks the model is able to perform in a zero - shot setting suggests that high - capacity models trained to maximize the likelihood of a sufficiently varied text corpus begin to learn how to perform a surprising amount of tasks without the need for explicit supervision .",
        "aspect": "high - capacity models",
        "sentiment": 0
    },
    {
        "text": "This paper describes Facebook FAIR 's submission to the WMT19 shared news translation task .",
        "aspect": "Facebook FAIR",
        "sentiment": 0
    },
    {
        "text": "Following our submission from last year , our baseline systems are large BPE - based transformer models trained with the FAIRSEQ sequence modeling toolkit which rely on sampled backtranslations .",
        "aspect": "FAIRSEQ sequence modeling toolkit",
        "sentiment": 1
    },
    {
        "text": "Following our submission from last year , our baseline systems are large BPE - based transformer models trained with the FAIRSEQ sequence modeling toolkit which rely on sampled backtranslations .",
        "aspect": "sampled backtranslations",
        "sentiment": 1
    },
    {
        "text": "Following our submission from last year , our baseline systems are large BPE - based transformer models trained with the FAIRSEQ sequence modeling toolkit which rely on sampled backtranslations .",
        "aspect": "BPE - based transformer models",
        "sentiment": 1
    },
    {
        "text": "This year we experiment with different bitext data filtering schemes , as well as with adding filtered back - translated data .",
        "aspect": "bitext data filtering schemes",
        "sentiment": 0
    },
    {
        "text": "We also ensemble and fine - tune our models on domain - specific data , then decode using noisy channel model reranking .",
        "aspect": "noisy channel model reranking",
        "sentiment": 0
    },
    {
        "text": "Our methods are based on techniques and approaches used in our submission from last year , including the use of subword models , , large - scale back - translation , and model ensembling .",
        "aspect": "subword models",
        "sentiment": 0
    },
    {
        "text": "Our methods are based on techniques and approaches used in our submission from last year , including the use of subword models , , large - scale back - translation , and model ensembling .",
        "aspect": "large - scale back - translation",
        "sentiment": 0
    },
    {
        "text": "Our methods are based on techniques and approaches used in our submission from last year , including the use of subword models , , large - scale back - translation , and model ensembling .",
        "aspect": "model ensembling",
        "sentiment": 0
    },
    {
        "text": "Although document level context for En\u2192De is now available , all our systems are pure sentence level systems .",
        "aspect": "sentence level systems",
        "sentiment": 0
    },
    {
        "text": "For all language directions , we back - translate the Newscrawl dataset using a reverse direction bitext system .",
        "aspect": "reverse direction bitext system",
        "sentiment": 0
    },
    {
        "text": "For our final models , we apply a domain - specific finetuning process and decode using noisy channel model reranking .",
        "aspect": "domain - specific finetuning process",
        "sentiment": 0
    },
    {
        "text": "For our final models , we apply a domain - specific finetuning process and decode using noisy channel model reranking .",
        "aspect": "noisy channel model reranking",
        "sentiment": 0
    },
    {
        "text": "Some of these gains can be attributed to differences in dataset quality , but we believe most of the improvement comes from larger models , larger scale back - translation , and noisy channel model reranking with strong channel and language models .",
        "aspect": "scale back - translation",
        "sentiment": 0
    },
    {
        "text": "Some of these gains can be attributed to differences in dataset quality , but we believe most of the improvement comes from larger models , larger scale back - translation , and noisy channel model reranking with strong channel and language models .",
        "aspect": "noisy channel model reranking",
        "sentiment": 0
    },
    {
        "text": "Some of these gains can be attributed to differences in dataset quality , but we believe most of the improvement comes from larger models , larger scale back - translation , and noisy channel model reranking with strong channel and language models .",
        "aspect": "strong channel",
        "sentiment": 0
    },
    {
        "text": "Some of these gains can be attributed to differences in dataset quality , but we believe most of the improvement comes from larger models , larger scale back - translation , and noisy channel model reranking with strong channel and language models .",
        "aspect": "language models",
        "sentiment": 0
    },
    {
        "text": "We report case - sensitive Sa - creBLEU scores using SacreBLEU ( Post , 2018 ) 1 , using international tokenization for En\u2192Ru .",
        "aspect": "SacreBLEU",
        "sentiment": 0
    },
    {
        "text": "We report case - sensitive Sa - creBLEU scores using SacreBLEU ( Post , 2018 ) 1 , using international tokenization for En\u2192Ru .",
        "aspect": "international tokenization",
        "sentiment": 0
    },
    {
        "text": "This paper describes Facebook FAIR 's submission to the WMT19 news translation task .",
        "aspect": "Facebook FAIR",
        "sentiment": 2
    },
    {
        "text": "For all four translation directions , En\u2194De and En\u2194Ru , we use the same strategy of filtering bitext data , performing sampling - based back - translation on monolingual data , then training strong individual mo- dels on a combination of this data .",
        "aspect": "sampling - based back - translation",
        "sentiment": 0
    },
    {
        "text": "For all four translation directions , En\u2194De and En\u2194Ru , we use the same strategy of filtering bitext data , performing sampling - based back - translation on monolingual data , then training strong individual mo- dels on a combination of this data .",
        "aspect": "mo- dels",
        "sentiment": 0
    },
    {
        "text": "Each of these models is fine - tuned and ensembled into a final system that is used for decoding with noisy channel model reranking .",
        "aspect": "noisy channel model reranking",
        "sentiment": 0
    },
    {
        "text": "We demonstrate the effectiveness of our noisy channel - based reranking approach even when applied on top of very strong systems , and rank first in all four directions of the human evaluation campaign .",
        "aspect": "noisy channel - based reranking approach",
        "sentiment": 0
    },
    {
        "text": "Unsupervised pre - training of large neural models has recently revolutionized Natural Language Processing .",
        "aspect": "Unsupervised pre - training of large neural models",
        "sentiment": 0
    },
    {
        "text": "In this paper , we demonstrate the efficacy of pretrained checkpoints for Sequence Generation .",
        "aspect": "pretrained checkpoints",
        "sentiment": 0
    },
    {
        "text": "We developed a Transformer - based sequence - to - sequence model that is compatible with publicly available pre - trained BERT , GPT-2 and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model , both encoder and decoder , with these checkpoints .",
        "aspect": "Transformer - based sequence - to - sequence model",
        "sentiment": 1
    },
    {
        "text": "We developed a Transformer - based sequence - to - sequence model that is compatible with publicly available pre - trained BERT , GPT-2 and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model , both encoder and decoder , with these checkpoints .",
        "aspect": "BERT",
        "sentiment": 3
    },
    {
        "text": "We developed a Transformer - based sequence - to - sequence model that is compatible with publicly available pre - trained BERT , GPT-2 and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model , both encoder and decoder , with these checkpoints .",
        "aspect": "GPT-2",
        "sentiment": 3
    },
    {
        "text": "We developed a Transformer - based sequence - to - sequence model that is compatible with publicly available pre - trained BERT , GPT-2 and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model , both encoder and decoder , with these checkpoints .",
        "aspect": "encoder and decoder",
        "sentiment": 0
    },
    {
        "text": "Unsupervised and self - supervised pre - training methods , such as ELMo , ULMFiT , and more recently BERT , GPT and GPT-2 , XLNet and RoBERTa have established a qualitatively new level of baseline performance for many widely used Natural Language Understanding ( NLU ) benchmarks including some of the most popular , like GLUE and SQuAD .",
        "aspect": "Unsupervised and self - supervised pre - training methods",
        "sentiment": 0
    },
    {
        "text": "Unsupervised and self - supervised pre - training methods , such as ELMo , ULMFiT , and more recently BERT , GPT and GPT-2 , XLNet and RoBERTa have established a qualitatively new level of baseline performance for many widely used Natural Language Understanding ( NLU ) benchmarks including some of the most popular , like GLUE and SQuAD .",
        "aspect": "ELMo",
        "sentiment": 0
    },
    {
        "text": "Unsupervised and self - supervised pre - training methods , such as ELMo , ULMFiT , and more recently BERT , GPT and GPT-2 , XLNet and RoBERTa have established a qualitatively new level of baseline performance for many widely used Natural Language Understanding ( NLU ) benchmarks including some of the most popular , like GLUE and SQuAD .",
        "aspect": "ULMFiT",
        "sentiment": 0
    },
    {
        "text": "Unsupervised and self - supervised pre - training methods , such as ELMo , ULMFiT , and more recently BERT , GPT and GPT-2 , XLNet and RoBERTa have established a qualitatively new level of baseline performance for many widely used Natural Language Understanding ( NLU ) benchmarks including some of the most popular , like GLUE and SQuAD .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Unsupervised and self - supervised pre - training methods , such as ELMo , ULMFiT , and more recently BERT , GPT and GPT-2 , XLNet and RoBERTa have established a qualitatively new level of baseline performance for many widely used Natural Language Understanding ( NLU ) benchmarks including some of the most popular , like GLUE and SQuAD .",
        "aspect": "GPT",
        "sentiment": 0
    },
    {
        "text": "Unsupervised and self - supervised pre - training methods , such as ELMo , ULMFiT , and more recently BERT , GPT and GPT-2 , XLNet and RoBERTa have established a qualitatively new level of baseline performance for many widely used Natural Language Understanding ( NLU ) benchmarks including some of the most popular , like GLUE and SQuAD .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "Unsupervised and self - supervised pre - training methods , such as ELMo , ULMFiT , and more recently BERT , GPT and GPT-2 , XLNet and RoBERTa have established a qualitatively new level of baseline performance for many widely used Natural Language Understanding ( NLU ) benchmarks including some of the most popular , like GLUE and SQuAD .",
        "aspect": "XLNet",
        "sentiment": 0
    },
    {
        "text": "Unsupervised and self - supervised pre - training methods , such as ELMo , ULMFiT , and more recently BERT , GPT and GPT-2 , XLNet and RoBERTa have established a qualitatively new level of baseline performance for many widely used Natural Language Understanding ( NLU ) benchmarks including some of the most popular , like GLUE and SQuAD .",
        "aspect": "RoBERTa",
        "sentiment": 0
    },
    {
        "text": "The most appealing part about this massive shift towards using large architectures pre - trained on large collections of texts is that the pre - trained checkpoints along with the inference code are made freely available .",
        "aspect": "large architectures",
        "sentiment": 0
    },
    {
        "text": "The most appealing part about this massive shift towards using large architectures pre - trained on large collections of texts is that the pre - trained checkpoints along with the inference code are made freely available .",
        "aspect": "inference code",
        "sentiment": 0
    },
    {
        "text": "More importantly , the ability to bootstrap from a state - of - theart performing model such as BERT motivates the community to greatly speed up the progress towards developing better and easily reusable NLU systems .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "While we continue to observe an increasing number of papers building on top of BERT and/or GPT models reporting encouraging improvements on Glue , SQuAD , and other similar benchmarks , very little attention has been paid to using these pre - trained models to warm - start sequence - tosequence ( seq2seq ) models .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "While we continue to observe an increasing number of papers building on top of BERT and/or GPT models reporting encouraging improvements on Glue , SQuAD , and other similar benchmarks , very little attention has been paid to using these pre - trained models to warm - start sequence - tosequence ( seq2seq ) models .",
        "aspect": "GPT models",
        "sentiment": 0
    },
    {
        "text": "While we continue to observe an increasing number of papers building on top of BERT and/or GPT models reporting encouraging improvements on Glue , SQuAD , and other similar benchmarks , very little attention has been paid to using these pre - trained models to warm - start sequence - tosequence ( seq2seq ) models .",
        "aspect": "pre - trained models",
        "sentiment": 0
    },
    {
        "text": "While we continue to observe an increasing number of papers building on top of BERT and/or GPT models reporting encouraging improvements on Glue , SQuAD , and other similar benchmarks , very little attention has been paid to using these pre - trained models to warm - start sequence - tosequence ( seq2seq ) models .",
        "aspect": "warm - start sequence - tosequence ( seq2seq ) models",
        "sentiment": 0
    },
    {
        "text": "It has been argued that the pre - training objective used by BERT is not well suited for tasks that require decoding texts , e.g. , conditional text generation in machine translation and summarization .",
        "aspect": "pre - training objective",
        "sentiment": 0
    },
    {
        "text": "It has been argued that the pre - training objective used by BERT is not well suited for tasks that require decoding texts , e.g. , conditional text generation in machine translation and summarization .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Nevertheless , it remains unclear to what extent employing such large models pre - trained on large collections of text can be beneficial to warm - start sequence - to - sequence generation models .",
        "aspect": "warm - start sequence - to - sequence generation models",
        "sentiment": 0
    },
    {
        "text": "In this paper , we have developed a Transformerbased sequence - to - sequence model that is compatible with publicly available pre - trained BERT , GPT-2 and RoBERTa checkpoints .",
        "aspect": "Transformerbased sequence - to - sequence model",
        "sentiment": 0
    },
    {
        "text": "In this paper , we have developed a Transformerbased sequence - to - sequence model that is compatible with publicly available pre - trained BERT , GPT-2 and RoBERTa checkpoints .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "In this paper , we have developed a Transformerbased sequence - to - sequence model that is compatible with publicly available pre - trained BERT , GPT-2 and RoBERTa checkpoints .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "In this paper , we have developed a Transformerbased sequence - to - sequence model that is compatible with publicly available pre - trained BERT , GPT-2 and RoBERTa checkpoints .",
        "aspect": "RoBERTa checkpoints",
        "sentiment": 0
    },
    {
        "text": "For example , one could imagine using BERT checkpoint to initialize the encoder for better input understanding and choosing GPT-2 model as the decoder for better text generation .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "For example , one could imagine using BERT checkpoint to initialize the encoder for better input understanding and choosing GPT-2 model as the decoder for better text generation .",
        "aspect": "encoder",
        "sentiment": 0
    },
    {
        "text": "For example , one could imagine using BERT checkpoint to initialize the encoder for better input understanding and choosing GPT-2 model as the decoder for better text generation .",
        "aspect": "GPT-2 model",
        "sentiment": 0
    },
    {
        "text": "For example , one could imagine using BERT checkpoint to initialize the encoder for better input understanding and choosing GPT-2 model as the decoder for better text generation .",
        "aspect": "decoder",
        "sentiment": 0
    },
    {
        "text": "One of the main contributions of this paper is that we rigorously experiment with a large number of different settings to combine BERT , GPT and RoBERTa pre - trained checkpoints to initialize our Transformer - based model .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "One of the main contributions of this paper is that we rigorously experiment with a large number of different settings to combine BERT , GPT and RoBERTa pre - trained checkpoints to initialize our Transformer - based model .",
        "aspect": "GPT",
        "sentiment": 0
    },
    {
        "text": "One of the main contributions of this paper is that we rigorously experiment with a large number of different settings to combine BERT , GPT and RoBERTa pre - trained checkpoints to initialize our Transformer - based model .",
        "aspect": "Transformer - based model",
        "sentiment": 0
    },
    {
        "text": "Our models report significant improvements over randomly initialized models demonstrating the benefit of leveraging unsupervised pre - trained models .",
        "aspect": "randomly initialized models",
        "sentiment": 0
    },
    {
        "text": "Our models report significant improvements over randomly initialized models demonstrating the benefit of leveraging unsupervised pre - trained models .",
        "aspect": "unsupervised pre - trained models",
        "sentiment": 0
    },
    {
        "text": "Our results also demonstrate that a pre - trained encoder is an essential component for sequence generation tasks and often these tasks benefit from sharing the weights between the encoder and the decoder .",
        "aspect": "encoder",
        "sentiment": 0
    },
    {
        "text": "Our results also demonstrate that a pre - trained encoder is an essential component for sequence generation tasks and often these tasks benefit from sharing the weights between the encoder and the decoder .",
        "aspect": "encoder",
        "sentiment": 0
    },
    {
        "text": "Our results also demonstrate that a pre - trained encoder is an essential component for sequence generation tasks and often these tasks benefit from sharing the weights between the encoder and the decoder .",
        "aspect": "decoder",
        "sentiment": 0
    },
    {
        "text": "Overall , we have run over 300 experiments spending thousands of TPU v3 hours to better accommodate the language modeling and understanding capabilities of these pre - trained models for text generation .",
        "aspect": "pre - trained models",
        "sentiment": 0
    },
    {
        "text": "search / tree / master / bertseq2seq .",
        "aspect": "bertseq2seq",
        "sentiment": 0
    },
    {
        "text": "2 Models and Pre - trained Checkpoints BERT was primarily developed for encoding text representations for NLU tasks ( encoder - only architecture ) , whereas GPT-2 , as a decoder - only architecture for language modeling .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "2 Models and Pre - trained Checkpoints BERT was primarily developed for encoding text representations for NLU tasks ( encoder - only architecture ) , whereas GPT-2 , as a decoder - only architecture for language modeling .",
        "aspect": "encoder - only architecture",
        "sentiment": 0
    },
    {
        "text": "2 Models and Pre - trained Checkpoints BERT was primarily developed for encoding text representations for NLU tasks ( encoder - only architecture ) , whereas GPT-2 , as a decoder - only architecture for language modeling .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "2 Models and Pre - trained Checkpoints BERT was primarily developed for encoding text representations for NLU tasks ( encoder - only architecture ) , whereas GPT-2 , as a decoder - only architecture for language modeling .",
        "aspect": "decoder - only architecture",
        "sentiment": 0
    },
    {
        "text": "Our model uses a seq2seq architecture with encoder and decoder both composed of Transformer layers .",
        "aspect": "seq2seq architecture",
        "sentiment": 0
    },
    {
        "text": "Our model uses a seq2seq architecture with encoder and decoder both composed of Transformer layers .",
        "aspect": "encoder and decoder",
        "sentiment": 0
    },
    {
        "text": "Our model uses a seq2seq architecture with encoder and decoder both composed of Transformer layers .",
        "aspect": "Transformer layers",
        "sentiment": 0
    },
    {
        "text": "For the encoder , we inherit the BERT Transformer layer implementations , which differs slightly from the canonical Transformer layer ; BERT uses a GELU activation rather than the standard RELU .",
        "aspect": "encoder",
        "sentiment": 0
    },
    {
        "text": "For the encoder , we inherit the BERT Transformer layer implementations , which differs slightly from the canonical Transformer layer ; BERT uses a GELU activation rather than the standard RELU .",
        "aspect": "BERT Transformer layer implementations",
        "sentiment": 0
    },
    {
        "text": "For the encoder , we inherit the BERT Transformer layer implementations , which differs slightly from the canonical Transformer layer ; BERT uses a GELU activation rather than the standard RELU .",
        "aspect": "canonical Transformer layer",
        "sentiment": 0
    },
    {
        "text": "For the encoder , we inherit the BERT Transformer layer implementations , which differs slightly from the canonical Transformer layer ; BERT uses a GELU activation rather than the standard RELU .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "For the encoder , we inherit the BERT Transformer layer implementations , which differs slightly from the canonical Transformer layer ; BERT uses a GELU activation rather than the standard RELU .",
        "aspect": "GELU activation",
        "sentiment": 0
    },
    {
        "text": "For the encoder , we inherit the BERT Transformer layer implementations , which differs slightly from the canonical Transformer layer ; BERT uses a GELU activation rather than the standard RELU .",
        "aspect": "RELU",
        "sentiment": 0
    },
    {
        "text": "If not stated otherwise , the implementation of the decoder layers are also identical to the BERT implementation with two adjustments .",
        "aspect": "decoder layers",
        "sentiment": 0
    },
    {
        "text": "If not stated otherwise , the implementation of the decoder layers are also identical to the BERT implementation with two adjustments .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "First the self - attention mechanism is masked to look only at the left context .",
        "aspect": "self - attention mechanism",
        "sentiment": 0
    },
    {
        "text": "Secondly , we add an encoder - decoder attention mechanism .",
        "aspect": "encoder - decoder attention mechanism",
        "sentiment": 0
    },
    {
        "text": "Note , that if the model was randomly initialized , we found no difference between a BERT compatible decoder and a GPT-2 compatible decoder .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Note , that if the model was randomly initialized , we found no difference between a BERT compatible decoder and a GPT-2 compatible decoder .",
        "aspect": "GPT-2 compatible decoder",
        "sentiment": 0
    },
    {
        "text": "All models were fine - tuned on the target task using Adam with a learning rate of 0.05 .",
        "aspect": "Adam",
        "sentiment": 0
    },
    {
        "text": "We used a linear learning rate warmup with 40k steps , normalization by the square root of the hidden size , and a square root decay .",
        "aspect": "linear learning rate warmup",
        "sentiment": 0
    },
    {
        "text": "We used a linear learning rate warmup with 40k steps , normalization by the square root of the hidden size , and a square root decay .",
        "aspect": "square root decay",
        "sentiment": 0
    },
    {
        "text": "BERT Checkpoints .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "We tokenize our text using the WordPiece to match the BERT pre - trained vocabulary .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "BERT also trains positional embeddings for up to 512 positions , which is the maximum input and output length in all experiments .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "We tokenize our text using the SentencePieces to match the GPT-2 pre - trained vocabulary . 2 Note that , while the available checkpoint is frequently called 117 M , which suggests the same number of parameters , we count 125 M parameters in the checkpoint .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "This is the smallest architecture they trained , and the number of layers , hidden size , and filter size are comparable to BERT - Base .",
        "aspect": "BERT - Base",
        "sentiment": 0
    },
    {
        "text": "While GPT-2 has positional embeddings for up to 1024 position , we only use the first 512 to make the results comparable with BERT .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "While GPT-2 has positional embeddings for up to 1024 position , we only use the first 512 to make the results comparable with BERT .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "RoBERTa is trained using PyTorch , but we found that the learned parameters are fully compatible with the existing TensorFlow BERT architectures with some minor adjustments .",
        "aspect": "RoBERTa",
        "sentiment": 0
    },
    {
        "text": "RoBERTa is trained using PyTorch , but we found that the learned parameters are fully compatible with the existing TensorFlow BERT architectures with some minor adjustments .",
        "aspect": "TensorFlow BERT architectures",
        "sentiment": 0
    },
    {
        "text": "Starting around 2013 , word embeddings like word2vec or GloVe became popular as they were easy to train in an unsupervised fashion on raw text and they improved several downstream tasks when used as features .",
        "aspect": "word embeddings",
        "sentiment": 0
    },
    {
        "text": "Starting around 2013 , word embeddings like word2vec or GloVe became popular as they were easy to train in an unsupervised fashion on raw text and they improved several downstream tasks when used as features .",
        "aspect": "word2vec",
        "sentiment": 0
    },
    {
        "text": "Starting around 2013 , word embeddings like word2vec or GloVe became popular as they were easy to train in an unsupervised fashion on raw text and they improved several downstream tasks when used as features .",
        "aspect": "GloVe",
        "sentiment": 0
    },
    {
        "text": "Starting around 2013 , word embeddings like word2vec or GloVe became popular as they were easy to train in an unsupervised fashion on raw text and they improved several downstream tasks when used as features .",
        "aspect": "unsupervised fashion",
        "sentiment": 0
    },
    {
        "text": "These word embeddings are invariant to the context the word is in .",
        "aspect": "word embeddings",
        "sentiment": 0
    },
    {
        "text": "While ELMo and ULMFiT are based on LSTMs , BERT and GPT are based on the transformer architecture .",
        "aspect": "ELMo",
        "sentiment": 0
    },
    {
        "text": "While ELMo and ULMFiT are based on LSTMs , BERT and GPT are based on the transformer architecture .",
        "aspect": "ULMFiT",
        "sentiment": 0
    },
    {
        "text": "While ELMo and ULMFiT are based on LSTMs , BERT and GPT are based on the transformer architecture .",
        "aspect": "LSTMs",
        "sentiment": 0
    },
    {
        "text": "While ELMo and ULMFiT are based on LSTMs , BERT and GPT are based on the transformer architecture .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "While ELMo and ULMFiT are based on LSTMs , BERT and GPT are based on the transformer architecture .",
        "aspect": "GPT",
        "sentiment": 0
    },
    {
        "text": "While ELMo and ULMFiT are based on LSTMs , BERT and GPT are based on the transformer architecture .",
        "aspect": "transformer architecture",
        "sentiment": 0
    },
    {
        "text": "This architecture outperforms LSTMs on several NLP tasks and we therefore concentrated on these two pre - trained models .",
        "aspect": "LSTMs",
        "sentiment": 0
    },
    {
        "text": "This architecture outperforms LSTMs on several NLP tasks and we therefore concentrated on these two pre - trained models .",
        "aspect": "pre - trained models",
        "sentiment": 0
    },
    {
        "text": "The contextualized embedding for each input token is given by the corresponding output of the last encoder layer .",
        "aspect": "contextualized embedding",
        "sentiment": 0
    },
    {
        "text": "The contextualized embedding for each input token is given by the corresponding output of the last encoder layer .",
        "aspect": "encoder layer",
        "sentiment": 0
    },
    {
        "text": "Pre - training models .",
        "aspect": "Pre - training models",
        "sentiment": 0
    },
    {
        "text": "One can also see these models as pre - trained models , which are then fine - tuned for a downstream task .",
        "aspect": "trained models",
        "sentiment": 0
    },
    {
        "text": "While the unsupervised pre - training strategies are different from those used in our paper , we expect the findings to still hold .",
        "aspect": "unsupervised pre - training strategies",
        "sentiment": 0
    },
    {
        "text": "They show that unsupervised pre - training is not simply a way of getting a good initial marginal distribution , that classical regularization techniques can not achieve the same performance as unsupervised pre - training , and that the effect of unsupervised pre - training does not go away with more training data .",
        "aspect": "regularization techniques",
        "sentiment": 0
    },
    {
        "text": "They show that unsupervised pre - training is not simply a way of getting a good initial marginal distribution , that classical regularization techniques can not achieve the same performance as unsupervised pre - training , and that the effect of unsupervised pre - training does not go away with more training data .",
        "aspect": "unsupervised pre - training",
        "sentiment": 0
    },
    {
        "text": "The primary results support the use of language modeling .",
        "aspect": "language modeling",
        "sentiment": 0
    },
    {
        "text": "Their results suggest that the relative performance of fine - tuning vs. feature extraction depends on the similarity between the pre - training and the target tasks .",
        "aspect": "fine - tuning",
        "sentiment": 0
    },
    {
        "text": "Their results suggest that the relative performance of fine - tuning vs. feature extraction depends on the similarity between the pre - training and the target tasks .",
        "aspect": "feature extraction",
        "sentiment": 0
    },
    {
        "text": "They used a language model to pre - train the encoder and decoder of an RNN seq2seq model .",
        "aspect": "language model",
        "sentiment": 0
    },
    {
        "text": "They used a language model to pre - train the encoder and decoder of an RNN seq2seq model .",
        "aspect": "encoder and decoder",
        "sentiment": 0
    },
    {
        "text": "They used a language model to pre - train the encoder and decoder of an RNN seq2seq model .",
        "aspect": "RNN",
        "sentiment": 0
    },
    {
        "text": "They used a language model to pre - train the encoder and decoder of an RNN seq2seq model .",
        "aspect": "seq2seq",
        "sentiment": 0
    },
    {
        "text": "However their BLEU score of 24.7 on newstest2014 En\u2192De , compared to 30.6 in this work , and 29.4 ROUGE - L on CNN / Dailymail , compared to 36.33 also show the superiority of the transformer model as well as the masked language model objective of BERT .",
        "aspect": "transformer model",
        "sentiment": 0
    },
    {
        "text": "However their BLEU score of 24.7 on newstest2014 En\u2192De , compared to 30.6 in this work , and 29.4 ROUGE - L on CNN / Dailymail , compared to 36.33 also show the superiority of the transformer model as well as the masked language model objective of BERT .",
        "aspect": "masked language model objective",
        "sentiment": 0
    },
    {
        "text": "However their BLEU score of 24.7 on newstest2014 En\u2192De , compared to 30.6 in this work , and 29.4 ROUGE - L on CNN / Dailymail , compared to 36.33 also show the superiority of the transformer model as well as the masked language model objective of BERT .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "MASS is a BERT - inspired method of pre - training sequence to sequence models .",
        "aspect": "MASS",
        "sentiment": 0
    },
    {
        "text": "MASS is a BERT - inspired method of pre - training sequence to sequence models .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "One advantage of this method is that , in contrast to our setups ( except for GPT ) , the encoder - decoder attention mechanism is also pre - trained .",
        "aspect": "GPT",
        "sentiment": 0
    },
    {
        "text": "One advantage of this method is that , in contrast to our setups ( except for GPT ) , the encoder - decoder attention mechanism is also pre - trained .",
        "aspect": "encoder - decoder attention mechanism",
        "sentiment": 0
    },
    {
        "text": "The downside of this approach is that the pre - trained model is task - specific and not as general as BERT or GPT-2 .",
        "aspect": "pre - trained model",
        "sentiment": 0
    },
    {
        "text": "The downside of this approach is that the pre - trained model is task - specific and not as general as BERT or GPT-2 .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "The downside of this approach is that the pre - trained model is task - specific and not as general as BERT or GPT-2 .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "UniLM also unifies bidirectional , unidirectional , and sequence to sequence language modeling .",
        "aspect": "UniLM",
        "sentiment": 0
    },
    {
        "text": "To overcome the issue that the encoder - decoder attention is not pretrained , pre - trained a single transformer language model that encodes the source and generates the target .",
        "aspect": "encoder - decoder attention",
        "sentiment": 0
    },
    {
        "text": "To overcome the issue that the encoder - decoder attention is not pretrained , pre - trained a single transformer language model that encodes the source and generates the target .",
        "aspect": "transformer language model",
        "sentiment": 0
    },
    {
        "text": "This setup matches our GPT setup .",
        "aspect": "GPT setup",
        "sentiment": 0
    },
    {
        "text": "We performed an extensive study on leveraging pre - trained checkpoints for sequence generation .",
        "aspect": "checkpoints",
        "sentiment": 0
    },
    {
        "text": "Our findings show , that a pre - trained encoder is an essential part .",
        "aspect": "encoder",
        "sentiment": 0
    },
    {
        "text": "Most tasks also profit from sharing the weights between the encoder and the decoder , which additionally decreases the memory footprint .",
        "aspect": "encoder",
        "sentiment": 0
    },
    {
        "text": "Most tasks also profit from sharing the weights between the encoder and the decoder , which additionally decreases the memory footprint .",
        "aspect": "decoder",
        "sentiment": 0
    },
    {
        "text": "While combing BERT and GPT-2 into a single model often underperformed a randomly initialized baseline , combining RoBERTa and GPT-2 achieves strong results and shows the importance of sharing the vocabulary .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "While combing BERT and GPT-2 into a single model often underperformed a randomly initialized baseline , combining RoBERTa and GPT-2 achieves strong results and shows the importance of sharing the vocabulary .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "While combing BERT and GPT-2 into a single model often underperformed a randomly initialized baseline , combining RoBERTa and GPT-2 achieves strong results and shows the importance of sharing the vocabulary .",
        "aspect": "randomly initialized baseline",
        "sentiment": 0
    },
    {
        "text": "While combing BERT and GPT-2 into a single model often underperformed a randomly initialized baseline , combining RoBERTa and GPT-2 achieves strong results and shows the importance of sharing the vocabulary .",
        "aspect": "RoBERTa",
        "sentiment": 0
    },
    {
        "text": "While combing BERT and GPT-2 into a single model often underperformed a randomly initialized baseline , combining RoBERTa and GPT-2 achieves strong results and shows the importance of sharing the vocabulary .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "Training a language specific BERT model also improves performance over using the multilingual version .",
        "aspect": "language specific BERT model",
        "sentiment": 0
    },
    {
        "text": "Training a language specific BERT model also improves performance over using the multilingual version .",
        "aspect": "multilingual version",
        "sentiment": 0
    },
    {
        "text": "Transformers : State - of - the - Art Natural Language Processing",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining .",
        "aspect": "model architecture",
        "sentiment": 0
    },
    {
        "text": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining .",
        "aspect": "model pretraining",
        "sentiment": 0
    },
    {
        "text": "Transformer architectures have facilitated building higher - capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks .",
        "aspect": "Transformer architectures",
        "sentiment": 0
    },
    {
        "text": "Transformer architectures have facilitated building higher - capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks .",
        "aspect": "capacity models",
        "sentiment": 0
    },
    {
        "text": "Transformer architectures have facilitated building higher - capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks .",
        "aspect": "pretraining",
        "sentiment": 0
    },
    {
        "text": "Transformers is an open - source library with the goal of opening up these advances to the wider machine learning community .",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "The library consists of carefully engineered stateof - the art Transformer architectures under a unified API .",
        "aspect": "Transformer architectures",
        "sentiment": 0
    },
    {
        "text": "Backing this library is a curated collection of pretrained models made by and available for the community .",
        "aspect": "pretrained models",
        "sentiment": 0
    },
    {
        "text": "Transformers is designed to be extensible by researchers , simple for practitioners , and fast and robust in industrial deployments .",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "The Transformer has rapidly become the dominant architecture for natural language processing , surpassing alternative neural models such as convolutional and recurrent neural networks in performance for tasks in both natural language understanding and natural language generation .",
        "aspect": "Transformer",
        "sentiment": 0
    },
    {
        "text": "The Transformer has rapidly become the dominant architecture for natural language processing , surpassing alternative neural models such as convolutional and recurrent neural networks in performance for tasks in both natural language understanding and natural language generation .",
        "aspect": "neural models",
        "sentiment": 0
    },
    {
        "text": "The Transformer has rapidly become the dominant architecture for natural language processing , surpassing alternative neural models such as convolutional and recurrent neural networks in performance for tasks in both natural language understanding and natural language generation .",
        "aspect": "convolutional and recurrent neural networks",
        "sentiment": 0
    },
    {
        "text": "Model pretraining allows models to be trained on generic corpora and subsequently be easily adapted to specific tasks with strong performance .",
        "aspect": "Model pretraining",
        "sentiment": 0
    },
    {
        "text": "The Transformer architecture is particularly conducive to pretraining on large text corpora , leading to major gains in accuracy on downstream tasks including text classification , language understanding , machine translation , coreference resolution , commonsense inference , and summarization among others .",
        "aspect": "Transformer architecture",
        "sentiment": 0
    },
    {
        "text": "The ubiquitous use of the Transformer calls for systems to train , analyze , scale , and augment the model on a variety of platforms .",
        "aspect": "Transformer",
        "sentiment": 0
    },
    {
        "text": "The pervasive adoption of pretraining methods has led to the need to distribute , fine - tune , deploy , and compress the core pretrained models used by the community .",
        "aspect": "pretraining methods",
        "sentiment": 0
    },
    {
        "text": "The pervasive adoption of pretraining methods has led to the need to distribute , fine - tune , deploy , and compress the core pretrained models used by the community .",
        "aspect": "pretrained models",
        "sentiment": 0
    },
    {
        "text": "Transformers is a library dedicated to supporting Transformer - based architectures and facilitating the distribution of pretrained models .",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "Transformers is a library dedicated to supporting Transformer - based architectures and facilitating the distribution of pretrained models .",
        "aspect": "Transformer - based architectures",
        "sentiment": 0
    },
    {
        "text": "Transformers is a library dedicated to supporting Transformer - based architectures and facilitating the distribution of pretrained models .",
        "aspect": "distribution of pretrained models",
        "sentiment": 0
    },
    {
        "text": "At the core of the libary is an implementation of the Transformer which is designed for both research and production .",
        "aspect": "libary",
        "sentiment": 0
    },
    {
        "text": "At the core of the libary is an implementation of the Transformer which is designed for both research and production .",
        "aspect": "Transformer",
        "sentiment": 0
    },
    {
        "text": "The philosophy is to support industrial - strength implementations of popular model variants that are easy to read , extend , and deploy .",
        "aspect": "model variants",
        "sentiment": 0
    },
    {
        "text": "On this foundation , the library supports the distribution and usage of a wide - variety of pretrained models in a centralized model hub .",
        "aspect": "pretrained models",
        "sentiment": 0
    },
    {
        "text": "On this foundation , the library supports the distribution and usage of a wide - variety of pretrained models in a centralized model hub .",
        "aspect": "centralized model hub",
        "sentiment": 0
    },
    {
        "text": "Transformers is an ongoing effort maintained by the team of engineers and researchers at Hugging Face with support from a vibrant community of over 400 external contributors .",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "The structure of Transformers is inspired by the pioneering tensor2tensor library and the original source code for BERT , both from Google Research .",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "The structure of Transformers is inspired by the pioneering tensor2tensor library and the original source code for BERT , both from Google Research .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "The concept of providing easy caching for pretrained models stemmed from AllenNLP .",
        "aspect": "pretrained models",
        "sentiment": 0
    },
    {
        "text": "The concept of providing easy caching for pretrained models stemmed from AllenNLP .",
        "aspect": "AllenNLP",
        "sentiment": 0
    },
    {
        "text": "The library is also closely related to neural translation and language modeling systems , such as Fairseq , Open - NMT , Texar , Megatron - LM , and Marian NMT .",
        "aspect": "Fairseq",
        "sentiment": 0
    },
    {
        "text": "The library is also closely related to neural translation and language modeling systems , such as Fairseq , Open - NMT , Texar , Megatron - LM , and Marian NMT .",
        "aspect": "Open - NMT",
        "sentiment": 0
    },
    {
        "text": "The library is also closely related to neural translation and language modeling systems , such as Fairseq , Open - NMT , Texar , Megatron - LM , and Marian NMT .",
        "aspect": "Texar",
        "sentiment": 0
    },
    {
        "text": "The library is also closely related to neural translation and language modeling systems , such as Fairseq , Open - NMT , Texar , Megatron - LM , and Marian NMT .",
        "aspect": "Megatron - LM",
        "sentiment": 0
    },
    {
        "text": "The library is also closely related to neural translation and language modeling systems , such as Fairseq , Open - NMT , Texar , Megatron - LM , and Marian NMT .",
        "aspect": "Marian NMT",
        "sentiment": 0
    },
    {
        "text": "Building on these elements , Transformers adds extra user - facing features to allow for easy downloading , caching , and fine - tuning of the models as well as seamless transition to production .",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "Transformers maintains some compatibility with these libraries , most directly including a tool for performing inference using models from Marian NMT and Google 's BERT .",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "Transformers maintains some compatibility with these libraries , most directly including a tool for performing inference using models from Marian NMT and Google 's BERT .",
        "aspect": "Marian NMT",
        "sentiment": 0
    },
    {
        "text": "Transformers maintains some compatibility with these libraries , most directly including a tool for performing inference using models from Marian NMT and Google 's BERT .",
        "aspect": "Google 's BERT",
        "sentiment": 0
    },
    {
        "text": "Transformers provides similar functionality as these libraries .",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "Additionally , each of these libraries now uses the Transformers library and model hub as a low - level framework .",
        "aspect": "Transformers library",
        "sentiment": 0
    },
    {
        "text": "Additionally , each of these libraries now uses the Transformers library and model hub as a low - level framework .",
        "aspect": "model hub",
        "sentiment": 0
    },
    {
        "text": "Additionally , each of these libraries now uses the Transformers library and model hub as a low - level framework .",
        "aspect": "low - level framework",
        "sentiment": 0
    },
    {
        "text": "Since Transformers provides a hub for NLP models , it is also related to popular model hubs including Torch Hub and TensorFlow Hub which collect framework - specific model parameters for easy use .",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "Since Transformers provides a hub for NLP models , it is also related to popular model hubs including Torch Hub and TensorFlow Hub which collect framework - specific model parameters for easy use .",
        "aspect": "NLP models",
        "sentiment": 0
    },
    {
        "text": "Since Transformers provides a hub for NLP models , it is also related to popular model hubs including Torch Hub and TensorFlow Hub which collect framework - specific model parameters for easy use .",
        "aspect": "model hubs",
        "sentiment": 0
    },
    {
        "text": "Since Transformers provides a hub for NLP models , it is also related to popular model hubs including Torch Hub and TensorFlow Hub which collect framework - specific model parameters for easy use .",
        "aspect": "Torch Hub",
        "sentiment": 0
    },
    {
        "text": "Since Transformers provides a hub for NLP models , it is also related to popular model hubs including Torch Hub and TensorFlow Hub which collect framework - specific model parameters for easy use .",
        "aspect": "TensorFlow Hub",
        "sentiment": 0
    },
    {
        "text": "Unlike these hubs , Transformers is domain - specific which allows the system to provide automatic support for model analysis , usage , deployment , benchmarking , and easy replicability .",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "As Transformer and pretraining play larger roles in NLP , it is important for these models to be accessible to researchers and end - users .",
        "aspect": "Transformer",
        "sentiment": 0
    },
    {
        "text": "Transformers is an open - source library and community designed to facilitate users to access large - scale pretrained models , to build and experiment on top of them , and to deploy them in downstream tasks with stateof - the - art performance .",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "Transformers is an open - source library and community designed to facilitate users to access large - scale pretrained models , to build and experiment on top of them , and to deploy them in downstream tasks with stateof - the - art performance .",
        "aspect": "pretrained models",
        "sentiment": 0
    },
    {
        "text": "Transformers has gained significant organic traction since its release and is set up to continue to provide core infrastructure while helping to facilitate access to new models .",
        "aspect": "Transformers",
        "sentiment": 0
    },
    {
        "text": "DistilBERT , a distilled version of BERT : smaller , faster , cheaper and lighter",
        "aspect": "DistilBERT",
        "sentiment": 0
    },
    {
        "text": "DistilBERT , a distilled version of BERT : smaller , faster , cheaper and lighter",
        "aspect": "distilled version",
        "sentiment": 0
    },
    {
        "text": "DistilBERT , a distilled version of BERT : smaller , faster , cheaper and lighter",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "As Transfer Learning from large - scale pre - trained models becomes more prevalent in Natural Language Processing ( NLP ) , operating these large models in on - theedge and/or under constrained computational training or inference budgets remains challenging .",
        "aspect": "Transfer Learning",
        "sentiment": 0
    },
    {
        "text": "As Transfer Learning from large - scale pre - trained models becomes more prevalent in Natural Language Processing ( NLP ) , operating these large models in on - theedge and/or under constrained computational training or inference budgets remains challenging .",
        "aspect": "large - scale pre - trained models",
        "sentiment": 0
    },
    {
        "text": "In this work , we propose a method to pre - train a smaller generalpurpose language representation model , called DistilBERT , which can then be finetuned with good performances on a wide range of tasks like its larger counterparts .",
        "aspect": "generalpurpose language representation model",
        "sentiment": 0
    },
    {
        "text": "In this work , we propose a method to pre - train a smaller generalpurpose language representation model , called DistilBERT , which can then be finetuned with good performances on a wide range of tasks like its larger counterparts .",
        "aspect": "DistilBERT",
        "sentiment": 2
    },
    {
        "text": "While most prior work investigated the use of distillation for building task - specific models , we leverage knowledge distillation during the pre - training phase and show that it is possible to reduce the size of a BERT model by 40 % , while retaining 97 % of its language understanding capabilities and being 60 % faster .",
        "aspect": "distillation",
        "sentiment": 0
    },
    {
        "text": "While most prior work investigated the use of distillation for building task - specific models , we leverage knowledge distillation during the pre - training phase and show that it is possible to reduce the size of a BERT model by 40 % , while retaining 97 % of its language understanding capabilities and being 60 % faster .",
        "aspect": "knowledge distillation",
        "sentiment": 0
    },
    {
        "text": "While most prior work investigated the use of distillation for building task - specific models , we leverage knowledge distillation during the pre - training phase and show that it is possible to reduce the size of a BERT model by 40 % , while retaining 97 % of its language understanding capabilities and being 60 % faster .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "To leverage the inductive biases learned by larger models during pre - training , we introduce a triple loss combining language modeling , distillation and cosine - distance losses .",
        "aspect": "triple loss",
        "sentiment": 0
    },
    {
        "text": "To leverage the inductive biases learned by larger models during pre - training , we introduce a triple loss combining language modeling , distillation and cosine - distance losses .",
        "aspect": "language modeling",
        "sentiment": 0
    },
    {
        "text": "To leverage the inductive biases learned by larger models during pre - training , we introduce a triple loss combining language modeling , distillation and cosine - distance losses .",
        "aspect": "distillation",
        "sentiment": 0
    },
    {
        "text": "To leverage the inductive biases learned by larger models during pre - training , we introduce a triple loss combining language modeling , distillation and cosine - distance losses .",
        "aspect": "cosine - distance losses",
        "sentiment": 0
    },
    {
        "text": "The last two years have seen the rise of Transfer Learning approaches in Natural Language Processing ( NLP ) with large - scale pre - trained language models becoming a basic tool in many NLP tasks .",
        "aspect": "Transfer Learning approaches",
        "sentiment": 0
    },
    {
        "text": "The last two years have seen the rise of Transfer Learning approaches in Natural Language Processing ( NLP ) with large - scale pre - trained language models becoming a basic tool in many NLP tasks .",
        "aspect": "language models",
        "sentiment": 0
    },
    {
        "text": "While these models lead to significant improvement , they often have several hundred million parameters and current research 1 on pre - trained models indicates that training even larger models still leads to better performances on downstream tasks .",
        "aspect": "pre - trained models",
        "sentiment": 0
    },
    {
        "text": "In this paper , we show that it is possible to reach similar performances on many downstream - tasks using much smaller language models pre - trained with knowledge distillation , resulting in models that are lighter and faster at inference time , while also requiring a smaller computational training budget .",
        "aspect": "language models",
        "sentiment": 0
    },
    {
        "text": "In this paper , we show that it is possible to reach similar performances on many downstream - tasks using much smaller language models pre - trained with knowledge distillation , resulting in models that are lighter and faster at inference time , while also requiring a smaller computational training budget .",
        "aspect": "knowledge distillation",
        "sentiment": 0
    },
    {
        "text": "Our general - purpose pre - trained models can be fine - tuned with good performances on several downstream tasks , keeping the flexibility of larger models .",
        "aspect": "general - purpose pre - trained models",
        "sentiment": 0
    },
    {
        "text": "We also show that our compressed models are small enough to run on the edge , e.g. on mobile devices .",
        "aspect": "compressed models",
        "sentiment": 0
    },
    {
        "text": "Using a triple loss , we show that a 40 % smaller Transformer ) pre - trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks , while being 60 % faster at inference time .",
        "aspect": "triple loss",
        "sentiment": 0
    },
    {
        "text": "Using a triple loss , we show that a 40 % smaller Transformer ) pre - trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks , while being 60 % faster at inference time .",
        "aspect": "Transformer",
        "sentiment": 0
    },
    {
        "text": "Using a triple loss , we show that a 40 % smaller Transformer ) pre - trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks , while being 60 % faster at inference time .",
        "aspect": "distillation",
        "sentiment": 0
    },
    {
        "text": "Using a triple loss , we show that a 40 % smaller Transformer ) pre - trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks , while being 60 % faster at inference time .",
        "aspect": "Transformer language model",
        "sentiment": 0
    },
    {
        "text": "We have made the trained weights available along with the training code in the Transformers 2 library from HuggingFace .",
        "aspect": "HuggingFace",
        "sentiment": 0
    },
    {
        "text": "General Language Understanding We assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation ( GLUE ) benchmark , a collection of 9 datasets for evaluating natural language understanding systems .",
        "aspect": "DistilBERT",
        "sentiment": 0
    },
    {
        "text": "We report scores on the development sets for each task by fine - tuning DistilBERT without the use of ensembling or multi - tasking scheme for fine - tuning ( which are mostly orthogonal to the present work ) .",
        "aspect": "DistilBERT",
        "sentiment": 0
    },
    {
        "text": "We report scores on the development sets for each task by fine - tuning DistilBERT without the use of ensembling or multi - tasking scheme for fine - tuning ( which are mostly orthogonal to the present work ) .",
        "aspect": "ensembling",
        "sentiment": 0
    },
    {
        "text": "We report scores on the development sets for each task by fine - tuning DistilBERT without the use of ensembling or multi - tasking scheme for fine - tuning ( which are mostly orthogonal to the present work ) .",
        "aspect": "multi - tasking scheme",
        "sentiment": 0
    },
    {
        "text": "We compare the results to the baseline provided by the authors of GLUE : an ELMo ) encoder followed by two BiLSTMs .",
        "aspect": "GLUE",
        "sentiment": 0
    },
    {
        "text": "We compare the results to the baseline provided by the authors of GLUE : an ELMo ) encoder followed by two BiLSTMs .",
        "aspect": "ELMo ) encoder",
        "sentiment": 0
    },
    {
        "text": "We compare the results to the baseline provided by the authors of GLUE : an ELMo ) encoder followed by two BiLSTMs .",
        "aspect": "BiLSTMs",
        "sentiment": 0
    },
    {
        "text": "Among the 9 tasks , DistilBERT is always on par or improving over the ELMo baseline ( up to 19 points of accuracy on STS - B ) .",
        "aspect": "DistilBERT",
        "sentiment": 0
    },
    {
        "text": "Among the 9 tasks , DistilBERT is always on par or improving over the ELMo baseline ( up to 19 points of accuracy on STS - B ) .",
        "aspect": "ELMo baseline",
        "sentiment": 0
    },
    {
        "text": "DistilBERT also compares surprisingly well to BERT , retaining 97 % of the performance with 40 % fewer parameters .",
        "aspect": "DistilBERT",
        "sentiment": 0
    },
    {
        "text": "DistilBERT also compares surprisingly well to BERT , retaining 97 % of the performance with 40 % fewer parameters .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Task - specific distillation Most of the prior works focus on building task - specific distillation setups .",
        "aspect": "Task - specific distillation",
        "sentiment": 0
    },
    {
        "text": "In the present work , we found it beneficial to use a general - purpose pre - training distillation rather than a task - specific distillation .",
        "aspect": "general - purpose pre - training distillation",
        "sentiment": 0
    },
    {
        "text": "In the present work , we found it beneficial to use a general - purpose pre - training distillation rather than a task - specific distillation .",
        "aspect": "task - specific distillation",
        "sentiment": 0
    },
    {
        "text": "Multi - distillation combine the knowledge of an ensemble of teachers using multi - task learning to regularize the distillation .",
        "aspect": "Multi - distillation",
        "sentiment": 0
    },
    {
        "text": "Multi - distillation combine the knowledge of an ensemble of teachers using multi - task learning to regularize the distillation .",
        "aspect": "ensemble of teachers",
        "sentiment": 0
    },
    {
        "text": "Multi - distillation combine the knowledge of an ensemble of teachers using multi - task learning to regularize the distillation .",
        "aspect": "multi - task learning",
        "sentiment": 0
    },
    {
        "text": "Multi - distillation combine the knowledge of an ensemble of teachers using multi - task learning to regularize the distillation .",
        "aspect": "distillation",
        "sentiment": 0
    },
    {
        "text": "The authors apply Multi - Task Knowledge Distillation to learn a compact question answering model from a set of large question answering models .",
        "aspect": "question answering model",
        "sentiment": 0
    },
    {
        "text": "The authors apply Multi - Task Knowledge Distillation to learn a compact question answering model from a set of large question answering models .",
        "aspect": "large question answering models",
        "sentiment": 0
    },
    {
        "text": "An application of multi - distillation is multi - linguality : adopts a similar approach to us by pre - training a multilingual model from scratch solely through distillation .",
        "aspect": "multi - distillation",
        "sentiment": 0
    },
    {
        "text": "An application of multi - distillation is multi - linguality : adopts a similar approach to us by pre - training a multilingual model from scratch solely through distillation .",
        "aspect": "multilingual model",
        "sentiment": 0
    },
    {
        "text": "An application of multi - distillation is multi - linguality : adopts a similar approach to us by pre - training a multilingual model from scratch solely through distillation .",
        "aspect": "distillation",
        "sentiment": 0
    },
    {
        "text": "Other compression techniques have been studied to compress large models .",
        "aspect": "compression techniques",
        "sentiment": 0
    },
    {
        "text": "Other compression techniques have been studied to compress large models .",
        "aspect": "large models",
        "sentiment": 0
    },
    {
        "text": "DIALOGPT : Large - Scale Generative Pre - training for Conversational Response Generation",
        "aspect": "DIALOGPT",
        "sentiment": 0
    },
    {
        "text": "We present a large , tunable neural conversational response generation model , DIALOGPT ( dialogue generative pre - trained transformer ) .",
        "aspect": "tunable neural conversational response generation model",
        "sentiment": 0
    },
    {
        "text": "We present a large , tunable neural conversational response generation model , DIALOGPT ( dialogue generative pre - trained transformer ) .",
        "aspect": "DIALOGPT",
        "sentiment": 0
    },
    {
        "text": "We present a large , tunable neural conversational response generation model , DIALOGPT ( dialogue generative pre - trained transformer ) .",
        "aspect": "dialogue generative pre - trained transformer",
        "sentiment": 0
    },
    {
        "text": "Trained on 147 M conversation - like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017 , DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single - turn dialogue settings .",
        "aspect": "DialoGPT",
        "sentiment": 0
    },
    {
        "text": "Trained on 147 M conversation - like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017 , DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single - turn dialogue settings .",
        "aspect": "Hugging Face PyTorch transformer",
        "sentiment": 0
    },
    {
        "text": "We show that conversational systems that leverage DialoGPT generate more relevant , contentful and context - consistent responses than strong baseline systems .",
        "aspect": "conversational systems",
        "sentiment": 0
    },
    {
        "text": "We show that conversational systems that leverage DialoGPT generate more relevant , contentful and context - consistent responses than strong baseline systems .",
        "aspect": "DialoGPT",
        "sentiment": 0
    },
    {
        "text": "The pre - trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent opendomain dialogue systems .",
        "aspect": "training pipeline",
        "sentiment": 0
    },
    {
        "text": "We introduce DIALOGPT , a tunable gigawordscale neural network model for generation of conversational reponses , trained on Reddit data .",
        "aspect": "DIALOGPT",
        "sentiment": 2
    },
    {
        "text": "We introduce DIALOGPT , a tunable gigawordscale neural network model for generation of conversational reponses , trained on Reddit data .",
        "aspect": "tunable gigawordscale neural network model",
        "sentiment": 0
    },
    {
        "text": "Recent advances in large - scale pre - training using transformer - based architectures have achieved great empirical success .",
        "aspect": "transformer - based architectures",
        "sentiment": 0
    },
    {
        "text": "OpenAI 's GPT-2 , for example , has demonstrated that transformer models trained on very large datasets can capture long - term dependencies in textual data and generate text that is fluent , lexically diverse , and rich in content .",
        "aspect": "OpenAI 's GPT-2",
        "sentiment": 0
    },
    {
        "text": "OpenAI 's GPT-2 , for example , has demonstrated that transformer models trained on very large datasets can capture long - term dependencies in textual data and generate text that is fluent , lexically diverse , and rich in content .",
        "aspect": "transformer models",
        "sentiment": 0
    },
    {
        "text": "DIALOGPT extends GPT-2 to address the challenges of conversational neural response genera - tion .",
        "aspect": "DIALOGPT",
        "sentiment": 0
    },
    {
        "text": "DIALOGPT extends GPT-2 to address the challenges of conversational neural response genera - tion .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "Most open - domain neural response generation systems suffer from content or style inconsistency , lack of long - term contextual information , and blandness .",
        "aspect": "open - domain neural response generation systems",
        "sentiment": 0
    },
    {
        "text": "While these issues can be alleviated by modelling strategies specifically designed to boost information content , a transformer - based architecture like GPT-2 , which uses a multi - layer self - attentive mechanism to allow fully - connected cross - attention to the full context in a computationally efficient manner , seems like a natural choice for exploring a more general solution .",
        "aspect": "modelling strategies",
        "sentiment": 0
    },
    {
        "text": "While these issues can be alleviated by modelling strategies specifically designed to boost information content , a transformer - based architecture like GPT-2 , which uses a multi - layer self - attentive mechanism to allow fully - connected cross - attention to the full context in a computationally efficient manner , seems like a natural choice for exploring a more general solution .",
        "aspect": "transformer - based architecture",
        "sentiment": 0
    },
    {
        "text": "While these issues can be alleviated by modelling strategies specifically designed to boost information content , a transformer - based architecture like GPT-2 , which uses a multi - layer self - attentive mechanism to allow fully - connected cross - attention to the full context in a computationally efficient manner , seems like a natural choice for exploring a more general solution .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "While these issues can be alleviated by modelling strategies specifically designed to boost information content , a transformer - based architecture like GPT-2 , which uses a multi - layer self - attentive mechanism to allow fully - connected cross - attention to the full context in a computationally efficient manner , seems like a natural choice for exploring a more general solution .",
        "aspect": "multi - layer self - attentive mechanism",
        "sentiment": 0
    },
    {
        "text": "Transformer models , for example , allow long - term dependency information to be better be preserved across time , thereby improving content consistency .",
        "aspect": "Transformer models",
        "sentiment": 0
    },
    {
        "text": "They also have higher model capacity due to their deep structure ( up to 48 layers in GPT-2 ) and are more effective in leveraging large - scale datasets ( more than 100 million training instances ) than RNN - based approaches .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "They also have higher model capacity due to their deep structure ( up to 48 layers in GPT-2 ) and are more effective in leveraging large - scale datasets ( more than 100 million training instances ) than RNN - based approaches .",
        "aspect": "RNN - based approaches",
        "sentiment": 0
    },
    {
        "text": "DialoGPT ; Blog : https://aka.ms/dialogpt 2 Our model is also available over Hugging face Transformers .",
        "aspect": "DialoGPT",
        "sentiment": 0
    },
    {
        "text": "DialoGPT ; Blog : https://aka.ms/dialogpt 2 Our model is also available over Hugging face Transformers .",
        "aspect": "Hugging face Transformers",
        "sentiment": 0
    },
    {
        "text": "https://huggingface.co/microsoft/ DialoGPT - medium language , ( 5 ) where source and target sequences together are longer than 200 words , ( 6 ) where the target contains offensive language , identified by phrase matching against a large blocklist .",
        "aspect": "phrase matching",
        "sentiment": 0
    },
    {
        "text": "There are several open - sourced toolkits for largescale pre - trained transformer models .",
        "aspect": "transformer models",
        "sentiment": 0
    },
    {
        "text": "Huggingface Conv - AI transfer learning repository contains the code for training conversational AI systems with transfer learning based on the GPT-2 transformer language model , which achieves the state - of - the - art performance on ConvAI-2 dialogue competition .",
        "aspect": "transfer learning",
        "sentiment": 0
    },
    {
        "text": "Huggingface Conv - AI transfer learning repository contains the code for training conversational AI systems with transfer learning based on the GPT-2 transformer language model , which achieves the state - of - the - art performance on ConvAI-2 dialogue competition .",
        "aspect": "GPT-2 transformer language model",
        "sentiment": 0
    },
    {
        "text": "DLGnet is a large transformer model trained on dialogue dataset and achieves good performance in multi - turn dialogue generation .",
        "aspect": "DLGnet",
        "sentiment": 0
    },
    {
        "text": "DLGnet is a large transformer model trained on dialogue dataset and achieves good performance in multi - turn dialogue generation .",
        "aspect": "transformer model",
        "sentiment": 0
    },
    {
        "text": "AllenNLP ) is developed as a toolkit for many natural language processing tasks , including the large - scale pre - trained bi - LSTM sentence representation learning framework ELMo .",
        "aspect": "AllenNLP",
        "sentiment": 0
    },
    {
        "text": "AllenNLP ) is developed as a toolkit for many natural language processing tasks , including the large - scale pre - trained bi - LSTM sentence representation learning framework ELMo .",
        "aspect": "large - scale pre - trained bi - LSTM sentence representation learning framework ELMo",
        "sentiment": 0
    },
    {
        "text": "Texar focuses on text generation including style transferring and controllable generation .",
        "aspect": "Texar",
        "sentiment": 0
    },
    {
        "text": "It includes reinforcement learning capabilities along with its sequence modelling tools .",
        "aspect": "reinforcement learning capabilities",
        "sentiment": 0
    },
    {
        "text": "It includes reinforcement learning capabilities along with its sequence modelling tools .",
        "aspect": "sequence modelling tools",
        "sentiment": 0
    },
    {
        "text": "DeepPavlov is a popular framework focusing on task - oriented dialogue .",
        "aspect": "DeepPavlov",
        "sentiment": 0
    },
    {
        "text": "This public repository contains several demos and pre - trained models for question answering and sentiment classification .",
        "aspect": "pre - trained models",
        "sentiment": 0
    },
    {
        "text": "Icecaps ( Shiv et al . , 2019 ) is a response generation toolkit with techniques such as grounding on personalities or external knowledge and multi - task training .",
        "aspect": "Icecaps",
        "sentiment": 0
    },
    {
        "text": "Icecaps ( Shiv et al . , 2019 ) is a response generation toolkit with techniques such as grounding on personalities or external knowledge and multi - task training .",
        "aspect": "response generation toolkit",
        "sentiment": 0
    },
    {
        "text": "ParlAI is another library for developing task - oriented dialogue systems .",
        "aspect": "ParlAI",
        "sentiment": 0
    },
    {
        "text": "These are known issues in current stateof - the - art end - to - end conversation models trained on large naturally - occurring datasets .",
        "aspect": "end - to - end conversation models",
        "sentiment": 0
    },
    {
        "text": "A major motive for releasing DIALOGPT is to enable researchers to investigate these issues and develop mitigation strategies .",
        "aspect": "mitigation strategies",
        "sentiment": 0
    },
    {
        "text": "We have released an open - domain pre - trained model , DIALOGPT , trained on massive real - world Reddit dataset .",
        "aspect": "pre - trained model",
        "sentiment": 0
    },
    {
        "text": "We have released an open - domain pre - trained model , DIALOGPT , trained on massive real - world Reddit dataset .",
        "aspect": "DIALOGPT",
        "sentiment": 2
    },
    {
        "text": "The package consists of a distributed training pipeline and several pre - trained models that can be fine - tuned to obtain a conversation model on a moderately - sized customized dataset in few hours .",
        "aspect": "distributed training pipeline",
        "sentiment": 0
    },
    {
        "text": "The package consists of a distributed training pipeline and several pre - trained models that can be fine - tuned to obtain a conversation model on a moderately - sized customized dataset in few hours .",
        "aspect": "conversation model",
        "sentiment": 0
    },
    {
        "text": "DIALOGPT is fully opensourced and easy to deploy , allowing users to ex - tend the pre - trained conversational system to bootstrap training using various datasets .",
        "aspect": "DIALOGPT",
        "sentiment": 0
    },
    {
        "text": "DIALOGPT is fully opensourced and easy to deploy , allowing users to ex - tend the pre - trained conversational system to bootstrap training using various datasets .",
        "aspect": "conversational system",
        "sentiment": 0
    },
    {
        "text": "We will investigate leveraging reinforcement learning to further improve the relevance of the generated responses and prevent the model from generating egregious responses .",
        "aspect": "reinforcement learning",
        "sentiment": 0
    },
    {
        "text": "We present BART , a denoising autoencoder for pretraining sequence - to - sequence models .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "We present BART , a denoising autoencoder for pretraining sequence - to - sequence models .",
        "aspect": "denoising autoencoder",
        "sentiment": 0
    },
    {
        "text": "BART is trained by (",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "It uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes .",
        "aspect": "Tranformer - based neural machine translation architecture",
        "sentiment": 0
    },
    {
        "text": "It uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "It uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes .",
        "aspect": "bidirectional encoder",
        "sentiment": 0
    },
    {
        "text": "It uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes .",
        "aspect": "GPT",
        "sentiment": 0
    },
    {
        "text": "It uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes .",
        "aspect": "left - to - right decoder",
        "sentiment": 0
    },
    {
        "text": "It uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes .",
        "aspect": "pretraining schemes",
        "sentiment": 0
    },
    {
        "text": "We evaluate a number of noising approaches , finding the best performance by both randomly shuffling the order of the original sentences and using a novel in - filling scheme , where spans of text are replaced with a single mask token .",
        "aspect": "noising approaches",
        "sentiment": 0
    },
    {
        "text": "We evaluate a number of noising approaches , finding the best performance by both randomly shuffling the order of the original sentences and using a novel in - filling scheme , where spans of text are replaced with a single mask token .",
        "aspect": "in - filling scheme",
        "sentiment": 0
    },
    {
        "text": "BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD , achieves new stateof - the - art results on a range of abstractive dialogue , question answering , and summarization tasks , with gains of up to 6 ROUGE .",
        "aspect": "RoBERTa",
        "sentiment": 0
    },
    {
        "text": "BART also provides a 1.1 BLEU increase over a back - translation system for machine translation , with only target language pretraining .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "BART also provides a 1.1 BLEU increase over a back - translation system for machine translation , with only target language pretraining .",
        "aspect": "back - translation system",
        "sentiment": 0
    },
    {
        "text": "BART also provides a 1.1 BLEU increase over a back - translation system for machine translation , with only target language pretraining .",
        "aspect": "target language pretraining",
        "sentiment": 0
    },
    {
        "text": "We also report ablation experiments that replicate other pretraining schemes within the BART framework , to better measure which factors most influence end - task performance .",
        "aspect": "pretraining schemes",
        "sentiment": 0
    },
    {
        "text": "We also report ablation experiments that replicate other pretraining schemes within the BART framework , to better measure which factors most influence end - task performance .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "Self - supervised methods have achieved remarkable success in a wide range of NLP tasks .",
        "aspect": "Self - supervised methods",
        "sentiment": 0
    },
    {
        "text": "The most successful approaches have been variants of masked language models , which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out .",
        "aspect": "masked language models",
        "sentiment": 0
    },
    {
        "text": "The most successful approaches have been variants of masked language models , which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out .",
        "aspect": "denoising autoencoders",
        "sentiment": 0
    },
    {
        "text": "In this paper , we present BART , which pre - trains a model combining Bidirectional and Auto - Regressive Transformers .",
        "aspect": "BART",
        "sentiment": 2
    },
    {
        "text": "In this paper , we present BART , which pre - trains a model combining Bidirectional and Auto - Regressive Transformers .",
        "aspect": "Bidirectional and Auto - Regressive Transformers",
        "sentiment": 0
    },
    {
        "text": "BART is a denoising autoencoder built with a sequence - to - sequence model that is applicable to a very wide range of end tasks .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "BART is a denoising autoencoder built with a sequence - to - sequence model that is applicable to a very wide range of end tasks .",
        "aspect": "denoising autoencoder",
        "sentiment": 0
    },
    {
        "text": "BART is a denoising autoencoder built with a sequence - to - sequence model that is applicable to a very wide range of end tasks .",
        "aspect": "sequence - to - sequence model",
        "sentiment": 0
    },
    {
        "text": "Pretraining has two stages ( 1 ) text is corrupted with an arbitrary noising function , and ( 2 ) a sequence - to - sequence model is learned to reconstruct the original text .",
        "aspect": "Pretraining",
        "sentiment": 0
    },
    {
        "text": "Pretraining has two stages ( 1 ) text is corrupted with an arbitrary noising function , and ( 2 ) a sequence - to - sequence model is learned to reconstruct the original text .",
        "aspect": "noising function",
        "sentiment": 0
    },
    {
        "text": "Pretraining has two stages ( 1 ) text is corrupted with an arbitrary noising function , and ( 2 ) a sequence - to - sequence model is learned to reconstruct the original text .",
        "aspect": "sequence - to - sequence model",
        "sentiment": 0
    },
    {
        "text": "BART uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes ( see Figure ) .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "BART uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes ( see Figure ) .",
        "aspect": "Tranformer - based neural machine translation architecture",
        "sentiment": 0
    },
    {
        "text": "BART uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes ( see Figure ) .",
        "aspect": "generalizing BERT",
        "sentiment": 0
    },
    {
        "text": "BART uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes ( see Figure ) .",
        "aspect": "bidirectional encoder",
        "sentiment": 0
    },
    {
        "text": "BART uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes ( see Figure ) .",
        "aspect": "GPT",
        "sentiment": 0
    },
    {
        "text": "BART uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes ( see Figure ) .",
        "aspect": "left - to - right decoder",
        "sentiment": 0
    },
    {
        "text": "BART uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes ( see Figure ) .",
        "aspect": "pretraining schemes",
        "sentiment": 0
    },
    {
        "text": "We evaluate a number of noising approaches , finding the best performance by both randomly shuffling the order of the original sentences and using a novel in - filling scheme , where arbitrary length spans of text ( including zero length ) are replaced with a single mask token .",
        "aspect": "noising approaches",
        "sentiment": 0
    },
    {
        "text": "We evaluate a number of noising approaches , finding the best performance by both randomly shuffling the order of the original sentences and using a novel in - filling scheme , where arbitrary length spans of text ( including zero length ) are replaced with a single mask token .",
        "aspect": "- filling scheme",
        "sentiment": 0
    },
    {
        "text": "BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "It matches the performance of RoBERTa with comparable training resources on GLUE and , and achieves new state - of - the - art results on a range of abstractive dialogue , question answering , and summarization tasks .",
        "aspect": "RoBERTa",
        "sentiment": 0
    },
    {
        "text": "For example , it improves performance by 6 ROUGE over previous work on XSum .",
        "aspect": "XSum",
        "sentiment": 0
    },
    {
        "text": "BART also opens up new ways of thinking about fine tuning .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers .",
        "aspect": "transformer layers",
        "sentiment": 0
    },
    {
        "text": "These layers are trained to essentially translate the foreign language to noised For fine - tuning , an uncorrupted document is input to both the encoder and decoder , and we use representations from the final hidden state of the decoder .",
        "aspect": "encoder",
        "sentiment": 0
    },
    {
        "text": "These layers are trained to essentially translate the foreign language to noised For fine - tuning , an uncorrupted document is input to both the encoder and decoder , and we use representations from the final hidden state of the decoder .",
        "aspect": "decoder",
        "sentiment": 0
    },
    {
        "text": "These layers are trained to essentially translate the foreign language to noised For fine - tuning , an uncorrupted document is input to both the encoder and decoder , and we use representations from the final hidden state of the decoder .",
        "aspect": "decoder",
        "sentiment": 0
    },
    {
        "text": "Figure : A schematic comparison of BART with BERT and GPT .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "Figure : A schematic comparison of BART with BERT and GPT .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "Figure : A schematic comparison of BART with BERT and GPT .",
        "aspect": "GPT",
        "sentiment": 0
    },
    {
        "text": "English , by propagation through BART , thereby using BART as a pre - trained target - side language model .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "English , by propagation through BART , thereby using BART as a pre - trained target - side language model .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "English , by propagation through BART , thereby using BART as a pre - trained target - side language model .",
        "aspect": "target - side language model",
        "sentiment": 0
    },
    {
        "text": "This approach improves performance over a strong back - translation MT baseline by 1.1 BLEU on the WMT Romanian - English benchmark .",
        "aspect": "back - translation MT baseline",
        "sentiment": 0
    },
    {
        "text": "To better understand these effects , we also report an ablation analysis that replicates other recently proposed training objectives .",
        "aspect": "ablation analysis",
        "sentiment": 0
    },
    {
        "text": "We find that BART exhibits the most consistently strong performance across the full range of tasks we consider .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "BART shows large improvements on summarization metrics , of up to 6 points over the prior state - of - the - art .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "To understand BART 's performance beyond automated metrics , we analyse its generations qualitatively .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "Table shows example summaries generated by BART .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "Early methods for pretraining were based on language models .",
        "aspect": "language models",
        "sentiment": 0
    },
    {
        "text": "GPT only models leftward context , which is problematic for some tasks .",
        "aspect": "GPT",
        "sentiment": 0
    },
    {
        "text": "ELMo concatenates left - only and right - only representations , but does not pre - train interactions between these features .",
        "aspect": "ELMo",
        "sentiment": 0
    },
    {
        "text": "BERT introduced masked language modelling , which allows pre - training to learn interactions between left and right context words .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "BERT introduced masked language modelling , which allows pre - training to learn interactions between left and right context words .",
        "aspect": "masked language modelling",
        "sentiment": 0
    },
    {
        "text": "Predictions are not made auto - regressively , reducing the effectiveness of BERT for generation tasks .",
        "aspect": "auto - regressively",
        "sentiment": 0
    },
    {
        "text": "Predictions are not made auto - regressively , reducing the effectiveness of BERT for generation tasks .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "UniLM fine - tunes BERT with an ensemble of masks , some of which allow only leftward context .",
        "aspect": "UniLM",
        "sentiment": 0
    },
    {
        "text": "UniLM fine - tunes BERT with an ensemble of masks , some of which allow only leftward context .",
        "aspect": "BERT",
        "sentiment": 0
    },
    {
        "text": "UniLM fine - tunes BERT with an ensemble of masks , some of which allow only leftward context .",
        "aspect": "ensemble of masks",
        "sentiment": 0
    },
    {
        "text": "Like BART , this allows UniLM to be used for both generative and discriminative tasks .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "Like BART , this allows UniLM to be used for both generative and discriminative tasks .",
        "aspect": "UniLM",
        "sentiment": 0
    },
    {
        "text": "A difference is that UniLM predictions are conditionally independent , whereas BART 's are autoregressive .",
        "aspect": "BART 's",
        "sentiment": 0
    },
    {
        "text": "A difference is that UniLM predictions are conditionally independent , whereas BART 's are autoregressive .",
        "aspect": "autoregressive",
        "sentiment": 0
    },
    {
        "text": "A difference is that UniLM predictions are conditionally independent , whereas BART 's are autoregressive .",
        "aspect": "UniLM",
        "sentiment": 0
    },
    {
        "text": "BART reduces the mismatch between pre - training and generation tasks , because the decoder is always trained on uncorrupted context .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "BART reduces the mismatch between pre - training and generation tasks , because the decoder is always trained on uncorrupted context .",
        "aspect": "decoder",
        "sentiment": 0
    },
    {
        "text": "MASS is perhaps the most similar model to BART .",
        "aspect": "MASS",
        "sentiment": 0
    },
    {
        "text": "MASS is less effective for discriminative tasks , because disjoint sets of tokens are fed into the encoder and decoder .",
        "aspect": "MASS",
        "sentiment": 0
    },
    {
        "text": "MASS is less effective for discriminative tasks , because disjoint sets of tokens are fed into the encoder and decoder .",
        "aspect": "encoder and decoder",
        "sentiment": 0
    },
    {
        "text": "Large pre - trained language models have been shown to store factual knowledge in their parameters , and achieve state - of - the - art results when fine - tuned on downstream NLP tasks .",
        "aspect": "language models",
        "sentiment": 0
    },
    {
        "text": "However , their ability to access and precisely manipulate knowledge is still limited , and hence on knowledge - intensive tasks , their performance lags behind task - specific architectures .",
        "aspect": "task - specific architectures",
        "sentiment": 0
    },
    {
        "text": "Pretrained models with a differentiable access mechanism to explicit non - parametric memory have so far been only investigated for extractive downstream tasks .",
        "aspect": "Pretrained models",
        "sentiment": 0
    },
    {
        "text": "Pretrained models with a differentiable access mechanism to explicit non - parametric memory have so far been only investigated for extractive downstream tasks .",
        "aspect": "differentiable access mechanism",
        "sentiment": 0
    },
    {
        "text": "We explore a general - purpose fine - tuning recipe for retrieval - augmented generation ( RAG ) -models which combine pre - trained parametric and non - parametric memory for language generation .",
        "aspect": "fine - tuning recipe",
        "sentiment": 0
    },
    {
        "text": "We explore a general - purpose fine - tuning recipe for retrieval - augmented generation ( RAG ) -models which combine pre - trained parametric and non - parametric memory for language generation .",
        "aspect": "RAG",
        "sentiment": 0
    },
    {
        "text": "We explore a general - purpose fine - tuning recipe for retrieval - augmented generation ( RAG ) -models which combine pre - trained parametric and non - parametric memory for language generation .",
        "aspect": "parametric and non - parametric memory",
        "sentiment": 0
    },
    {
        "text": "We introduce RAG models where the parametric memory is a pre - trained seq2seq model and the non - parametric memory is a dense vector index of Wikipedia , accessed with a pre - trained neural retriever .",
        "aspect": "RAG models",
        "sentiment": 0
    },
    {
        "text": "We introduce RAG models where the parametric memory is a pre - trained seq2seq model and the non - parametric memory is a dense vector index of Wikipedia , accessed with a pre - trained neural retriever .",
        "aspect": "parametric memory",
        "sentiment": 0
    },
    {
        "text": "We introduce RAG models where the parametric memory is a pre - trained seq2seq model and the non - parametric memory is a dense vector index of Wikipedia , accessed with a pre - trained neural retriever .",
        "aspect": "seq2seq model",
        "sentiment": 0
    },
    {
        "text": "We introduce RAG models where the parametric memory is a pre - trained seq2seq model and the non - parametric memory is a dense vector index of Wikipedia , accessed with a pre - trained neural retriever .",
        "aspect": "non - parametric memory",
        "sentiment": 0
    },
    {
        "text": "We introduce RAG models where the parametric memory is a pre - trained seq2seq model and the non - parametric memory is a dense vector index of Wikipedia , accessed with a pre - trained neural retriever .",
        "aspect": "neural retriever",
        "sentiment": 0
    },
    {
        "text": "We compare two RAG formulations , one which conditions on the same retrieved passages across the whole generated sequence , and another which can use different passages per token .",
        "aspect": "RAG formulations",
        "sentiment": 0
    },
    {
        "text": "We fine - tune and evaluate our models on a wide range of knowledgeintensive NLP tasks and set the state of the art on three open domain QA tasks , outperforming parametric seq2seq models and task - specific retrieve - and - extract architectures .",
        "aspect": "parametric seq2seq models",
        "sentiment": 0
    },
    {
        "text": "We fine - tune and evaluate our models on a wide range of knowledgeintensive NLP tasks and set the state of the art on three open domain QA tasks , outperforming parametric seq2seq models and task - specific retrieve - and - extract architectures .",
        "aspect": "task - specific retrieve - and - extract architectures",
        "sentiment": 0
    },
    {
        "text": "For language generation tasks , we find that RAG models generate more specific , diverse and factual language than a state - of - the - art parametric - only seq2seq baseline .",
        "aspect": "RAG models",
        "sentiment": 0
    },
    {
        "text": "For language generation tasks , we find that RAG models generate more specific , diverse and factual language than a state - of - the - art parametric - only seq2seq baseline .",
        "aspect": "parametric - only seq2seq baseline",
        "sentiment": 0
    },
    {
        "text": "Pre - trained neural language models have been shown to learn a substantial amount of in - depth knowledge from data .",
        "aspect": "neural language models",
        "sentiment": 0
    },
    {
        "text": "Hybrid models that combine parametric memory with non - parametric ( i.e. , retrieval - based ) memories can address some of these issues because knowledge can be directly revised and expanded , and accessed knowledge can be inspected and interpreted .",
        "aspect": "Hybrid models",
        "sentiment": 0
    },
    {
        "text": "Hybrid models that combine parametric memory with non - parametric ( i.e. , retrieval - based ) memories can address some of these issues because knowledge can be directly revised and expanded , and accessed knowledge can be inspected and interpreted .",
        "aspect": "parametric memory",
        "sentiment": 0
    },
    {
        "text": "Hybrid models that combine parametric memory with non - parametric ( i.e. , retrieval - based ) memories can address some of these issues because knowledge can be directly revised and expanded , and accessed knowledge can be inspected and interpreted .",
        "aspect": "non - parametric",
        "sentiment": 0
    },
    {
        "text": "Hybrid models that combine parametric memory with non - parametric ( i.e. , retrieval - based ) memories can address some of these issues because knowledge can be directly revised and expanded , and accessed knowledge can be inspected and interpreted .",
        "aspect": "retrieval - based ) memories",
        "sentiment": 0
    },
    {
        "text": "REALM and ORQA , two recently introduced models that combine masked language models with a differentiable retriever , have shown promising results , Figure : Overview of our approach .",
        "aspect": "REALM",
        "sentiment": 0
    },
    {
        "text": "REALM and ORQA , two recently introduced models that combine masked language models with a differentiable retriever , have shown promising results , Figure : Overview of our approach .",
        "aspect": "ORQA",
        "sentiment": 0
    },
    {
        "text": "REALM and ORQA , two recently introduced models that combine masked language models with a differentiable retriever , have shown promising results , Figure : Overview of our approach .",
        "aspect": "masked language models",
        "sentiment": 0
    },
    {
        "text": "REALM and ORQA , two recently introduced models that combine masked language models with a differentiable retriever , have shown promising results , Figure : Overview of our approach .",
        "aspect": "differentiable retriever",
        "sentiment": 0
    },
    {
        "text": "We combine a pre - trained retriever ( Query Encoder + Document Index ) with a pre - trained seq2seq model ( Generator ) and fine - tune end - to - end .",
        "aspect": "retriever",
        "sentiment": 0
    },
    {
        "text": "We combine a pre - trained retriever ( Query Encoder + Document Index ) with a pre - trained seq2seq model ( Generator ) and fine - tune end - to - end .",
        "aspect": "Query Encoder + Document Index",
        "sentiment": 0
    },
    {
        "text": "We combine a pre - trained retriever ( Query Encoder + Document Index ) with a pre - trained seq2seq model ( Generator ) and fine - tune end - to - end .",
        "aspect": "Generator",
        "sentiment": 0
    },
    {
        "text": "For query x , we use Maximum Inner Product Search ( MIPS ) to find the top - K documents z i .",
        "aspect": "Maximum Inner Product Search",
        "sentiment": 0
    },
    {
        "text": "For query x , we use Maximum Inner Product Search ( MIPS ) to find the top - K documents z i .",
        "aspect": "MIPS",
        "sentiment": 0
    },
    {
        "text": "Here , we bring hybrid parametric and non - parametric memory to the \" workhorse of NLP , \" i.e. sequence - to - sequence ( seq2seq ) models .",
        "aspect": "hybrid parametric and non - parametric memory",
        "sentiment": 0
    },
    {
        "text": "Here , we bring hybrid parametric and non - parametric memory to the \" workhorse of NLP , \" i.e. sequence - to - sequence ( seq2seq ) models .",
        "aspect": "sequence - to - sequence",
        "sentiment": 0
    },
    {
        "text": "Here , we bring hybrid parametric and non - parametric memory to the \" workhorse of NLP , \" i.e. sequence - to - sequence ( seq2seq ) models .",
        "aspect": "seq2seq",
        "sentiment": 0
    },
    {
        "text": "We endow pre - trained , parametric - memory generation models with a non - parametric memory through a general - purpose fine - tuning approach which we refer to as retrieval - augmented generation ( RAG ) .",
        "aspect": "parametric - memory generation models",
        "sentiment": 0
    },
    {
        "text": "We endow pre - trained , parametric - memory generation models with a non - parametric memory through a general - purpose fine - tuning approach which we refer to as retrieval - augmented generation ( RAG ) .",
        "aspect": "non - parametric memory",
        "sentiment": 0
    },
    {
        "text": "We endow pre - trained , parametric - memory generation models with a non - parametric memory through a general - purpose fine - tuning approach which we refer to as retrieval - augmented generation ( RAG ) .",
        "aspect": "fine - tuning approach",
        "sentiment": 0
    },
    {
        "text": "We endow pre - trained , parametric - memory generation models with a non - parametric memory through a general - purpose fine - tuning approach which we refer to as retrieval - augmented generation ( RAG ) .",
        "aspect": "RAG",
        "sentiment": 0
    },
    {
        "text": "We build RAG models where the parametric memory is a pre - trained seq2seq transformer , and the non - parametric memory is a dense vector index of Wikipedia , accessed with a pre - trained neural retriever .",
        "aspect": "RAG models",
        "sentiment": 0
    },
    {
        "text": "We build RAG models where the parametric memory is a pre - trained seq2seq transformer , and the non - parametric memory is a dense vector index of Wikipedia , accessed with a pre - trained neural retriever .",
        "aspect": "parametric memory",
        "sentiment": 0
    },
    {
        "text": "We build RAG models where the parametric memory is a pre - trained seq2seq transformer , and the non - parametric memory is a dense vector index of Wikipedia , accessed with a pre - trained neural retriever .",
        "aspect": "non - parametric memory",
        "sentiment": 0
    },
    {
        "text": "We build RAG models where the parametric memory is a pre - trained seq2seq transformer , and the non - parametric memory is a dense vector index of Wikipedia , accessed with a pre - trained neural retriever .",
        "aspect": "neural retriever",
        "sentiment": 0
    },
    {
        "text": "We combine these components in a probabilistic model trained end - to - end ( Fig . ) .",
        "aspect": "probabilistic model",
        "sentiment": 0
    },
    {
        "text": "The retriever ( Dense Passage Retriever , henceforth DPR ) provides latent documents conditioned on the input , and the seq2seq model ( BART ) then conditions on these latent documents together with the input to generate the output .",
        "aspect": "retriever",
        "sentiment": 0
    },
    {
        "text": "The retriever ( Dense Passage Retriever , henceforth DPR ) provides latent documents conditioned on the input , and the seq2seq model ( BART ) then conditions on these latent documents together with the input to generate the output .",
        "aspect": "Dense Passage Retriever",
        "sentiment": 0
    },
    {
        "text": "The retriever ( Dense Passage Retriever , henceforth DPR ) provides latent documents conditioned on the input , and the seq2seq model ( BART ) then conditions on these latent documents together with the input to generate the output .",
        "aspect": "DPR",
        "sentiment": 0
    },
    {
        "text": "The retriever ( Dense Passage Retriever , henceforth DPR ) provides latent documents conditioned on the input , and the seq2seq model ( BART ) then conditions on these latent documents together with the input to generate the output .",
        "aspect": "seq2seq model",
        "sentiment": 0
    },
    {
        "text": "The retriever ( Dense Passage Retriever , henceforth DPR ) provides latent documents conditioned on the input , and the seq2seq model ( BART ) then conditions on these latent documents together with the input to generate the output .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "We marginalize the latent documents with a top - K approximation , either on a per - output basis ( assuming the same document is responsible for all tokens ) or a per - token basis ( where different documents are responsible for different tokens ) .",
        "aspect": "top - K approximation",
        "sentiment": 0
    },
    {
        "text": "Like T5 or BART , RAG can be fine - tuned on any seq2seq task , whereby both the generator and retriever are jointly learned .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "Like T5 or BART , RAG can be fine - tuned on any seq2seq task , whereby both the generator and retriever are jointly learned .",
        "aspect": "RAG",
        "sentiment": 0
    },
    {
        "text": "Like T5 or BART , RAG can be fine - tuned on any seq2seq task , whereby both the generator and retriever are jointly learned .",
        "aspect": "generator",
        "sentiment": 0
    },
    {
        "text": "Like T5 or BART , RAG can be fine - tuned on any seq2seq task , whereby both the generator and retriever are jointly learned .",
        "aspect": "retriever",
        "sentiment": 0
    },
    {
        "text": "There has been extensive previous work proposing architectures to enrich systems with non - parametric memory which are trained from scratch for specific tasks , e.g.",
        "aspect": "non - parametric memory",
        "sentiment": 0
    },
    {
        "text": "memory networks , stackaugmented networks and memory layers .",
        "aspect": "memory networks",
        "sentiment": 0
    },
    {
        "text": "memory networks , stackaugmented networks and memory layers .",
        "aspect": "stackaugmented networks",
        "sentiment": 0
    },
    {
        "text": "memory networks , stackaugmented networks and memory layers .",
        "aspect": "memory layers",
        "sentiment": 0
    },
    {
        "text": "In contrast , we explore a setting where both parametric and non - parametric memory components are pre - trained and pre - loaded with extensive knowledge .",
        "aspect": "parametric and non - parametric memory components",
        "sentiment": 0
    },
    {
        "text": "Crucially , by using pre - trained access mechanisms , the ability to access knowledge is present without additional training .",
        "aspect": "access mechanisms",
        "sentiment": 0
    },
    {
        "text": "Our results highlight the benefits of combining parametric and non - parametric memory with generation for knowledge - intensive tasks - tasks that humans could not reasonably be expected to perform without access to an external knowledge source .",
        "aspect": "parametric and non - parametric memory",
        "sentiment": 0
    },
    {
        "text": "Our RAG models achieve state - of - the - art results on open Natural Questions , WebQuestions and CuratedTrec and strongly outperform recent approaches that use specialised pre - training objectives on TriviaQA .",
        "aspect": "RAG models",
        "sentiment": 0
    },
    {
        "text": "Our RAG models achieve state - of - the - art results on open Natural Questions , WebQuestions and CuratedTrec and strongly outperform recent approaches that use specialised pre - training objectives on TriviaQA .",
        "aspect": "pre - training objectives",
        "sentiment": 0
    },
    {
        "text": "Despite these being extractive tasks , we find that unconstrained generation outperforms previous extractive approaches .",
        "aspect": "extractive approaches",
        "sentiment": 0
    },
    {
        "text": "For FEVER fact verification , we achieve results within 4.3 % of state - of - the - art pipeline models which use strong retrieval supervision .",
        "aspect": "pipeline models",
        "sentiment": 0
    },
    {
        "text": "Finally , we demonstrate that the non - parametric memory can be replaced to update the models ' knowledge as the world changes .",
        "aspect": "non - parametric memory",
        "sentiment": 0
    },
    {
        "text": "We experiment with RAG in a wide range of knowledge - intensive tasks .",
        "aspect": "RAG",
        "sentiment": 0
    },
    {
        "text": "We use the document encoder to compute an embedding for each document , and build a single MIPS index using FAISS with a Hierarchical Navigable Small World approximation for fast retrieval .",
        "aspect": "document encoder",
        "sentiment": 0
    },
    {
        "text": "We use the document encoder to compute an embedding for each document , and build a single MIPS index using FAISS with a Hierarchical Navigable Small World approximation for fast retrieval .",
        "aspect": "MIPS index",
        "sentiment": 0
    },
    {
        "text": "We use the document encoder to compute an embedding for each document , and build a single MIPS index using FAISS with a Hierarchical Navigable Small World approximation for fast retrieval .",
        "aspect": "FAISS",
        "sentiment": 0
    },
    {
        "text": "We use the document encoder to compute an embedding for each document , and build a single MIPS index using FAISS with a Hierarchical Navigable Small World approximation for fast retrieval .",
        "aspect": "Hierarchical Navigable Small World approximation",
        "sentiment": 0
    },
    {
        "text": "Generation Diversity Section 4.3 shows that RAG models are more factual and specific than BART for Jeopardy question generation .",
        "aspect": "RAG models",
        "sentiment": 0
    },
    {
        "text": "Generation Diversity Section 4.3 shows that RAG models are more factual and specific than BART for Jeopardy question generation .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "Table shows that RAG - Sequence 's generations are more diverse than RAG - Token 's , and both are significantly more diverse than BART without needing any diversity - promoting decoding .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "Table shows that RAG - Sequence 's generations are more diverse than RAG - Token 's , and both are significantly more diverse than BART without needing any diversity - promoting decoding .",
        "aspect": "diversity - promoting decoding",
        "sentiment": 0
    },
    {
        "text": "Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task .",
        "aspect": "RAG",
        "sentiment": 0
    },
    {
        "text": "To assess the effectiveness of the retrieval mechanism , we run ablations where we freeze the retriever during training .",
        "aspect": "retrieval mechanism",
        "sentiment": 0
    },
    {
        "text": "We compare RAG 's dense retriever to a word overlap - based BM25 retriever .",
        "aspect": "RAG 's dense retriever",
        "sentiment": 0
    },
    {
        "text": "We compare RAG 's dense retriever to a word overlap - based BM25 retriever .",
        "aspect": "word overlap - based BM25 retriever",
        "sentiment": 0
    },
    {
        "text": "Here , we replace RAG 's retriever with a fixed BM25 system , and use BM25 retrieval scores as logits when calculating p(z|x ) .",
        "aspect": "RAG 's retriever",
        "sentiment": 0
    },
    {
        "text": "Here , we replace RAG 's retriever with a fixed BM25 system , and use BM25 retrieval scores as logits when calculating p(z|x ) .",
        "aspect": "BM25 system",
        "sentiment": 0
    },
    {
        "text": "For FEVER , BM25 performs best , perhaps since FEVER claims are heavily entity - centric and thus well - suited for word overlap - based retrieval .",
        "aspect": "BM25",
        "sentiment": 0
    },
    {
        "text": "Index hot - swapping An advantage of non - parametric memory models like RAG is that knowledge can be easily updated at test time .",
        "aspect": "non - parametric memory models",
        "sentiment": 0
    },
    {
        "text": "Index hot - swapping An advantage of non - parametric memory models like RAG is that knowledge can be easily updated at test time .",
        "aspect": "RAG",
        "sentiment": 0
    },
    {
        "text": "Parametric - only models like T5 or BART need further training to update their behavior as the world changes .",
        "aspect": "Parametric - only models",
        "sentiment": 0
    },
    {
        "text": "Parametric - only models like T5 or BART need further training to update their behavior as the world changes .",
        "aspect": "T5",
        "sentiment": 0
    },
    {
        "text": "Parametric - only models like T5 or BART need further training to update their behavior as the world changes .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "To demonstrate , we build an index using the DrQA Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results ( December 2018 ) .",
        "aspect": "RAG",
        "sentiment": 0
    },
    {
        "text": "Our work unifies previous successes in incorporating retrieval into individual tasks , showing that a single retrieval - based architecture is capable of achieving strong performance across several tasks .",
        "aspect": "retrieval - based architecture",
        "sentiment": 0
    },
    {
        "text": "General - Purpose Architectures for NLP Prior work on general - purpose architectures for NLP tasks has shown great success without the use of retrieval .",
        "aspect": "General - Purpose Architectures",
        "sentiment": 0
    },
    {
        "text": "General - Purpose Architectures for NLP Prior work on general - purpose architectures for NLP tasks has shown great success without the use of retrieval .",
        "aspect": "general - purpose architectures",
        "sentiment": 0
    },
    {
        "text": "A single , pre - trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks after fine - tuning .",
        "aspect": "pre - trained language model",
        "sentiment": 0
    },
    {
        "text": "A single , pre - trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks after fine - tuning .",
        "aspect": "fine - tuning",
        "sentiment": 0
    },
    {
        "text": "GPT-2 later showed that a single , left - to - right , pre - trained language model could achieve strong performance across both discriminative and generative tasks .",
        "aspect": "GPT-2",
        "sentiment": 0
    },
    {
        "text": "GPT-2 later showed that a single , left - to - right , pre - trained language model could achieve strong performance across both discriminative and generative tasks .",
        "aspect": "pre - trained language model",
        "sentiment": 0
    },
    {
        "text": "For further improvement , BART and T5 propose a single , pre - trained encoder - decoder model that leverages bi - directional attention to achieve stronger performance on discriminative and generative tasks .",
        "aspect": "pre - trained encoder - decoder model",
        "sentiment": 0
    },
    {
        "text": "For further improvement , BART and T5 propose a single , pre - trained encoder - decoder model that leverages bi - directional attention to achieve stronger performance on discriminative and generative tasks .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "For further improvement , BART and T5 propose a single , pre - trained encoder - decoder model that leverages bi - directional attention to achieve stronger performance on discriminative and generative tasks .",
        "aspect": "T5",
        "sentiment": 0
    },
    {
        "text": "Our work aims to expand the space of possible tasks with a single , unified architecture , by learning a retrieval module to augment pre - trained , generative language models .",
        "aspect": "unified architecture",
        "sentiment": 0
    },
    {
        "text": "Our work aims to expand the space of possible tasks with a single , unified architecture , by learning a retrieval module to augment pre - trained , generative language models .",
        "aspect": "retrieval module",
        "sentiment": 0
    },
    {
        "text": "Our work aims to expand the space of possible tasks with a single , unified architecture , by learning a retrieval module to augment pre - trained , generative language models .",
        "aspect": "generative language models",
        "sentiment": 0
    },
    {
        "text": "Learned Retrieval There is significant work on learning to retrieve documents in information retrieval , more recently with pre - trained , neural language models similar to ours .",
        "aspect": "neural language models",
        "sentiment": 0
    },
    {
        "text": "Some work optimizes the retrieval module to aid in a specific , downstream task such as question answering , using search , reinforcement learning , or a latent variable approach as in our work .",
        "aspect": "retrieval module",
        "sentiment": 0
    },
    {
        "text": "Some work optimizes the retrieval module to aid in a specific , downstream task such as question answering , using search , reinforcement learning , or a latent variable approach as in our work .",
        "aspect": "reinforcement learning",
        "sentiment": 0
    },
    {
        "text": "Some work optimizes the retrieval module to aid in a specific , downstream task such as question answering , using search , reinforcement learning , or a latent variable approach as in our work .",
        "aspect": "latent variable approach",
        "sentiment": 0
    },
    {
        "text": "These successes leverage different retrieval - based architectures and optimization techniques to achieve strong performance on a single task , while we show that a single retrieval - based architecture can be fine - tuned for strong performance on a variety of tasks .",
        "aspect": "retrieval - based architectures",
        "sentiment": 0
    },
    {
        "text": "These successes leverage different retrieval - based architectures and optimization techniques to achieve strong performance on a single task , while we show that a single retrieval - based architecture can be fine - tuned for strong performance on a variety of tasks .",
        "aspect": "optimization techniques",
        "sentiment": 0
    },
    {
        "text": "These successes leverage different retrieval - based architectures and optimization techniques to achieve strong performance on a single task , while we show that a single retrieval - based architecture can be fine - tuned for strong performance on a variety of tasks .",
        "aspect": "retrieval - based architecture",
        "sentiment": 0
    },
    {
        "text": "Memory - based Architectures Our document index can be seen as a large external memory for neural networks to attend to , analogous to memory networks .",
        "aspect": "Memory - based Architectures",
        "sentiment": 0
    },
    {
        "text": "Memory - based Architectures Our document index can be seen as a large external memory for neural networks to attend to , analogous to memory networks .",
        "aspect": "neural networks",
        "sentiment": 0
    },
    {
        "text": "Memory - based Architectures Our document index can be seen as a large external memory for neural networks to attend to , analogous to memory networks .",
        "aspect": "memory networks",
        "sentiment": 0
    },
    {
        "text": "Other work improves the ability of dialog models to generate factual text by attending over fact embeddings .",
        "aspect": "dialog models",
        "sentiment": 0
    },
    {
        "text": "This approach has also been used in knowledge - intensive dialog , where generators have been conditioned on retrieved text directly , albeit obtained via TF - IDF rather than end - to - end learnt retrieval .",
        "aspect": "TF - IDF",
        "sentiment": 0
    },
    {
        "text": "This approach has also been used in knowledge - intensive dialog , where generators have been conditioned on retrieved text directly , albeit obtained via TF - IDF rather than end - to - end learnt retrieval .",
        "aspect": "end - to - end learnt retrieval",
        "sentiment": 0
    },
    {
        "text": "Retrieve - and - Edit approaches Our method shares some similarities with retrieve - and - edit style approaches , where a similar training input - output pair is retrieved for a given input , and then edited to provide a final output .",
        "aspect": "Retrieve - and - Edit approaches",
        "sentiment": 0
    },
    {
        "text": "Retrieve - and - Edit approaches Our method shares some similarities with retrieve - and - edit style approaches , where a similar training input - output pair is retrieved for a given input , and then edited to provide a final output .",
        "aspect": "retrieve - and - edit style approaches",
        "sentiment": 0
    },
    {
        "text": "This said , RAG techniques may work well in these settings , and could represent promising future work .",
        "aspect": "RAG techniques",
        "sentiment": 0
    },
    {
        "text": "In this work , we presented hybrid generation models with access to parametric and non - parametric memory .",
        "aspect": "hybrid generation models",
        "sentiment": 2
    },
    {
        "text": "We showed that our RAG models obtain state of the art results on open - domain QA .",
        "aspect": "RAG models",
        "sentiment": 0
    },
    {
        "text": "We found that people prefer RAG 's generation over purely parametric BART , finding RAG more factual and specific .",
        "aspect": "RAG 's generation",
        "sentiment": 0
    },
    {
        "text": "We found that people prefer RAG 's generation over purely parametric BART , finding RAG more factual and specific .",
        "aspect": "parametric BART",
        "sentiment": 0
    },
    {
        "text": "We found that people prefer RAG 's generation over purely parametric BART , finding RAG more factual and specific .",
        "aspect": "RAG",
        "sentiment": 0
    },
    {
        "text": "We conducted an thorough investigation of the learned retrieval component , validating its effectiveness , and we illustrated how the retrieval index can be hot - swapped to update the model without requiring any retraining .",
        "aspect": "retrieval component",
        "sentiment": 0
    },
    {
        "text": "We conducted an thorough investigation of the learned retrieval component , validating its effectiveness , and we illustrated how the retrieval index can be hot - swapped to update the model without requiring any retraining .",
        "aspect": "retraining",
        "sentiment": 0
    },
    {
        "text": "In future work , it may be fruitful to investigate if the two components can be jointly pre - trained from scratch , either with a denoising objective similar to BART or some another objective .",
        "aspect": "BART",
        "sentiment": 0
    },
    {
        "text": "Cross - lingual Language Model Pretraining",
        "aspect": "Cross - lingual Language Model Pretraining",
        "sentiment": 0
    },
    {
        "text": "Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding .",
        "aspect": "generative pretraining",
        "sentiment": 0
    },
    {
        "text": "In this work , we extend this approach to multiple languages and show the effectiveness of cross - lingual pretraining .",
        "aspect": "cross - lingual pretraining",
        "sentiment": 0
    },
    {
        "text": "We propose two methods to learn cross - lingual language models ( XLMs ): one unsupervised that only relies on monolingual data , and one supervised that leverages parallel data with a new cross - lingual language model objective .",
        "aspect": "cross - lingual language models",
        "sentiment": 0
    },
    {
        "text": "We propose two methods to learn cross - lingual language models ( XLMs ): one unsupervised that only relies on monolingual data , and one supervised that leverages parallel data with a new cross - lingual language model objective .",
        "aspect": "XLMs",
        "sentiment": 0
    },
    {
        "text": "We propose two methods to learn cross - lingual language models ( XLMs ): one unsupervised that only relies on monolingual data , and one supervised that leverages parallel data with a new cross - lingual language model objective .",
        "aspect": "unsupervised",
        "sentiment": 0
    },
    {
        "text": "We propose two methods to learn cross - lingual language models ( XLMs ): one unsupervised that only relies on monolingual data , and one supervised that leverages parallel data with a new cross - lingual language model objective .",
        "aspect": "supervised",
        "sentiment": 0
    },
    {
        "text": "We propose two methods to learn cross - lingual language models ( XLMs ): one unsupervised that only relies on monolingual data , and one supervised that leverages parallel data with a new cross - lingual language model objective .",
        "aspect": "cross - lingual language model objective",
        "sentiment": 0
    },
    {
        "text": "Our code and pretrained models will be made publicly available .",
        "aspect": "pretrained models",
        "sentiment": 0
    },
    {
        "text": "Generative pretraining of sentence encoders has led to strong improvements on numerous natural language understanding benchmarks .",
        "aspect": "Generative pretraining of sentence encoders",
        "sentiment": 0
    },
    {
        "text": "In this context , a Transformer language model is learned on a large unsupervised text corpus , and then fine - tuned on natural language understanding ( NLU ) tasks such as classification or natural language inference .",
        "aspect": "Transformer language model",
        "sentiment": 0
    },
    {
        "text": "Recent developments in learning and evaluating cross - lingual sentence representations in many languages aim at mitigating the English - centric bias and suggest that it is possible to build universal cross - lingual encoders that can encode any sentence into a shared embedding space .",
        "aspect": "cross - lingual encoders",
        "sentiment": 0
    },
    {
        "text": "In this work , we demonstrate the effectiveness of cross - lingual language model pretraining on multiple cross - lingual understanding ( XLU ) benchmarks .",
        "aspect": "cross - lingual language model pretraining",
        "sentiment": 0
    },
    {
        "text": "1 . We introduce a new unsupervised method for learning cross - lingual representations using cross - lingual language modeling and investigate two monolingual pretraining objectives .",
        "aspect": "unsupervised method",
        "sentiment": 0
    },
    {
        "text": "1 . We introduce a new unsupervised method for learning cross - lingual representations using cross - lingual language modeling and investigate two monolingual pretraining objectives .",
        "aspect": "cross - lingual language modeling",
        "sentiment": 0
    },
    {
        "text": "4 . We show that cross - lingual language models can provide significant improvements on the perplexity of low - resource languages .",
        "aspect": "cross - lingual language models",
        "sentiment": 0
    },
    {
        "text": "5 . We will make our code and pretrained models publicly available .",
        "aspect": "pretrained models",
        "sentiment": 0
    },
    {
        "text": "Our work builds on top of ; ; who investigate language modeling for pretraining Transformer encoders .",
        "aspect": "language modeling",
        "sentiment": 0
    },
    {
        "text": "Concurrent to our work , results on cross - lingual classification using a cross - lingual language modeling approach were showcased on the BERT repository 1 .",
        "aspect": "cross - lingual language modeling approach",
        "sentiment": 0
    },
    {
        "text": "Aligning distributions of text representations has a long tradition , starting from word embeddings alignment and the work of that leverages small dictionaries to align word representations from different languages .",
        "aspect": "word representations",
        "sentiment": 0
    },
    {
        "text": "A series of follow - up studies show that cross - lingual representations can be used to improve the quality of monolingual representations , that orthogonal transformations are sufficient to align these word distributions , and that all these techniques can be applied to an arbitrary number of languages .",
        "aspect": "monolingual representations",
        "sentiment": 0
    },
    {
        "text": "A series of follow - up studies show that cross - lingual representations can be used to improve the quality of monolingual representations , that orthogonal transformations are sufficient to align these word distributions , and that all these techniques can be applied to an arbitrary number of languages .",
        "aspect": "orthogonal transformations",
        "sentiment": 0
    },
    {
        "text": "But the most successful recent approach of cross - lingual encoders is probably the one of for multilingual machine translation .",
        "aspect": "cross - lingual encoders",
        "sentiment": 0
    },
    {
        "text": "They show that a single sequence - tosequence model can be used to perform machine translation for many language pairs , by using a single shared LSTM encoder and decoder .",
        "aspect": "sequence - tosequence model",
        "sentiment": 0
    },
    {
        "text": "They show that a single sequence - tosequence model can be used to perform machine translation for many language pairs , by using a single shared LSTM encoder and decoder .",
        "aspect": "shared LSTM encoder and decoder",
        "sentiment": 0
    },
    {
        "text": "Their multilingual model outperformed the state of the art on low - resource language pairs , and enabled zero - shot translation .",
        "aspect": "multilingual model",
        "sentiment": 0
    },
    {
        "text": "Following this approach , show that the resulting encoder can be used to produce cross - lingual sentence embeddings .",
        "aspect": "encoder",
        "sentiment": 0
    },
    {
        "text": "They obtained a new state of the art on the XNLI crosslingual classification benchmark by learning a classifier on top of the fixed sentence representations .",
        "aspect": "classifier",
        "sentiment": 0
    },
    {
        "text": "They obtained a new state of the art on the XNLI crosslingual classification benchmark by learning a classifier on top of the fixed sentence representations .",
        "aspect": "fixed sentence representations",
        "sentiment": 0
    },
    {
        "text": "While these methods require a significant amount of parallel data , recent work in unsupervised machine translation show that sentence representations can be aligned in a completely unsupervised way .",
        "aspect": "sentence representations",
        "sentiment": 0
    },
    {
        "text": "Similar to this work , we show that we can align distributions of sentences in a completely unsupervised way , and that our cross - lingual models can be used for a broad set of natural language understanding tasks , including machine translation .",
        "aspect": "cross - lingual models",
        "sentiment": 0
    },
    {
        "text": "The most similar work to ours is probably the one of , where the authors train a LSTM language model with sentences from different languages .",
        "aspect": "LSTM language model",
        "sentiment": 0
    },
    {
        "text": "They share the LSTM parameters , but use different lookup tables to represent the words in each language .",
        "aspect": "LSTM",
        "sentiment": 0
    },
    {
        "text": "In this work , we show for the first time the strong impact of cross - lingual language model ( XLM ) pretraining .",
        "aspect": "cross - lingual language model",
        "sentiment": 0
    },
    {
        "text": "In this work , we show for the first time the strong impact of cross - lingual language model ( XLM ) pretraining .",
        "aspect": "XLM",
        "sentiment": 0
    },
    {
        "text": "We investigate two unsupervised training objectives that require only monolingual corpora : Causal Language Modeling ( CLM ) and Masked Language Modeling ( MLM ) .",
        "aspect": "Causal Language Modeling",
        "sentiment": 0
    },
    {
        "text": "We investigate two unsupervised training objectives that require only monolingual corpora : Causal Language Modeling ( CLM ) and Masked Language Modeling ( MLM ) .",
        "aspect": "CLM",
        "sentiment": 0
    },
    {
        "text": "We investigate two unsupervised training objectives that require only monolingual corpora : Causal Language Modeling ( CLM ) and Masked Language Modeling ( MLM ) .",
        "aspect": "Masked Language Modeling",
        "sentiment": 0
    },
    {
        "text": "We investigate two unsupervised training objectives that require only monolingual corpora : Causal Language Modeling ( CLM ) and Masked Language Modeling ( MLM ) .",
        "aspect": "MLM",
        "sentiment": 0
    },
    {
        "text": "We show that both the CLM and MLM approaches provide strong cross - lingual features that can be used for pretraining models .",
        "aspect": "CLM",
        "sentiment": 0
    },
    {
        "text": "We show that both the CLM and MLM approaches provide strong cross - lingual features that can be used for pretraining models .",
        "aspect": "MLM approaches",
        "sentiment": 0
    },
    {
        "text": "We show that both the CLM and MLM approaches provide strong cross - lingual features that can be used for pretraining models .",
        "aspect": "pretraining models",
        "sentiment": 0
    },
    {
        "text": "On unsupervised machine translation , we show that MLM pretraining is extremely effective .",
        "aspect": "MLM pretraining",
        "sentiment": 0
    },
    {
        "text": "We also demonstrate that cross - lingual language model can be used to improve the perplexity of a Nepali language model , and that it provides unsupervised cross - lingual word embeddings .",
        "aspect": "cross - lingual language model",
        "sentiment": 0
    },
    {
        "text": "We also demonstrate that cross - lingual language model can be used to improve the perplexity of a Nepali language model , and that it provides unsupervised cross - lingual word embeddings .",
        "aspect": "Nepali language model",
        "sentiment": 0
    },
    {
        "text": "Without using a single parallel sentence , a cross - lingual language model fine - tuned on the XNLI cross - lingual classification benchmark already outperforms the previous supervised state of the art by 1.3 % accuracy on average .",
        "aspect": "cross - lingual language model",
        "sentiment": 0
    },
    {
        "text": "A key contribution of our work is the translation language modeling ( TLM ) objective which improves crosslingual language model pretraining by leveraging parallel data .",
        "aspect": "translation language modeling",
        "sentiment": 0
    },
    {
        "text": "A key contribution of our work is the translation language modeling ( TLM ) objective which improves crosslingual language model pretraining by leveraging parallel data .",
        "aspect": "TLM",
        "sentiment": 0
    },
    {
        "text": "A key contribution of our work is the translation language modeling ( TLM ) objective which improves crosslingual language model pretraining by leveraging parallel data .",
        "aspect": "crosslingual language model pretraining",
        "sentiment": 0
    },
    {
        "text": "TLM naturally extends the BERT MLM approach by using batches of parallel sentences instead of consecutive sentences .",
        "aspect": "TLM",
        "sentiment": 0
    },
    {
        "text": "TLM naturally extends the BERT MLM approach by using batches of parallel sentences instead of consecutive sentences .",
        "aspect": "MLM approach",
        "sentiment": 0
    },
    {
        "text": "We obtain a significant gain by using TLM in addition to MLM , and we show that this supervised approach beats the previous state of the art on XNLI by 4.9 % accuracy on average .",
        "aspect": "TLM",
        "sentiment": 0
    },
    {
        "text": "We obtain a significant gain by using TLM in addition to MLM , and we show that this supervised approach beats the previous state of the art on XNLI by 4.9 % accuracy on average .",
        "aspect": "MLM",
        "sentiment": 0
    },
    {
        "text": "We obtain a significant gain by using TLM in addition to MLM , and we show that this supervised approach beats the previous state of the art on XNLI by 4.9 % accuracy on average .",
        "aspect": "supervised approach",
        "sentiment": 0
    },
    {
        "text": "Our code and pretrained models will be made publicly available .",
        "aspect": "pretrained models",
        "sentiment": 0
    }
]